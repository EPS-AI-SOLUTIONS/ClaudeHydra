---
description: "Load and display available AI models from all providers"
---

# ðŸ¤– AI MODELS - Lazy Loader

## âš¡ AUTO-LOAD PROTOCOL

When this command is invoked, **IMMEDIATELY** load models from all providers:

```bash
# Load all models with lazy caching
node .claude/scripts/model-loader.js status
```

## ðŸ“Š Available Commands

| Command | Description |
|---------|-------------|
| `/ai-models` | Show all available models |
| `/ai-models best code` | Get best model for coding |
| `/ai-models best analysis` | Get best model for analysis |
| `/ai-models provider google` | Show Google/Gemini models |
| `/ai-models provider ollama` | Show local Ollama models |

## ðŸŽ¯ Task Types

| Task | Best Providers |
|------|----------------|
| `code` | DeepSeek â†’ OpenAI â†’ Anthropic â†’ Google |
| `analysis` | Google â†’ Anthropic â†’ OpenAI â†’ DeepSeek |
| `reasoning` | Anthropic â†’ DeepSeek â†’ OpenAI â†’ Google |
| `multimodal` | Google â†’ OpenAI â†’ Anthropic |
| `realtime` | xAI â†’ Google â†’ OpenAI |
| `local` | Ollama |

## ðŸ”§ Configuration

Models are cached for 5 minutes. Configuration in:
- `.claude/config/multi-cli.json` â†’ `model_loader` section
- `.claude/scripts/model-loader.js` â†’ `MODEL_RANKINGS`

---

ARGUMENTS: $ARGUMENTS
