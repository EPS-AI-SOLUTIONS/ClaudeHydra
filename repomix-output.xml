This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
_launcher.ps1
.claude/commands/ai-batch.md
.claude/commands/ai-config.md
.claude/commands/ai-health.md
.claude/commands/ai-help.md
.claude/commands/ai-pull.md
.claude/commands/ai-status.md
.claude/commands/ai.md
.claude/hooks/ai-handler-integration.ps1
.claude/hooks/permission-alert.ps1
.claude/settings.local.json
.claude/statusline.cjs
.env.example
.eslintrc.cjs
.gemini/commands/ai/batch.toml
.gemini/commands/ai/code.toml
.gemini/commands/ai/config.toml
.gemini/commands/ai/health.toml
.gemini/commands/ai/help.toml
.gemini/commands/ai/optimize.toml
.gemini/commands/ai/pull.toml
.gemini/commands/ai/quick.toml
.gemini/commands/ai/speculate.toml
.gemini/commands/ai/status.toml
.gemini/commands/gemini/models.toml
.gemini/commands/hydra/config.toml
.gemini/commands/hydra/status.toml
.gemini/commands/mcp/health.toml
.gemini/commands/queue/status.toml
.gemini/settings.json
.gemini/statusline.cjs
.github/dependabot.yml
.github/workflows/ci.yml
.gitignore
.grok/settings.json
.prettierrc.json
.serena/.gitignore
.serena/memories/Ciri.md
.serena/memories/Dijkstra.md
.serena/memories/Eskel.md
.serena/memories/Geralt.md
.serena/memories/index-memory-catalog.md
.serena/memories/Jaskier.md
.serena/memories/Lambert.md
.serena/memories/Philippa.md
.serena/memories/policy-project-identity.md
.serena/memories/policy-serena-longterm-memory.md
.serena/memories/Regis.md
.serena/memories/task-log-2026-01-13.md
.serena/memories/task-log-2026-01-14.md
.serena/memories/Triss.md
.serena/memories/Vesemir.md
.serena/memories/workflow-serena-every-query.md
.serena/memories/workflow-task-log.md
.serena/memories/Yennefer.md
.serena/memories/Zoltan.md
.serena/project.yml
AGENTS.md
ARCHITECTURE.md
CHANGELOG.md
CODEX.md
eslint.config.js
gemini-extension.json
gemini-hydra.omp.json
gemini-icon.png
GEMINI.md
GeminiCLI.vbs
GROK.md
icon.ico
package.json
profile.ps1
prompt-optimizer-gemini.json
README.md
scripts/doctor.mjs
scripts/launcher/index.mjs
src/cache.js
src/config.js
src/gemini-models.js
src/logger.js
src/memory.js
src/ollama-client.js
src/prompt-optimizer.js
src/prompt-queue.js
src/self-correction.js
src/server.js
src/speculative.js
src/swarm.js
src/tools.js
src/version.js
start_codex.ps1
test/tools.test.js
vercel.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/hooks/ai-handler-integration.ps1">
# AI Handler Integration for Claude Code (GeminiCLI)
# Provides context and recommendations for each prompt
# Uses shared AI Handler from ClaudeCLI

param(
    [Parameter(ValueFromPipeline=$true)]
    [string]$InputJson
)

$ErrorActionPreference = 'SilentlyContinue'
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8

# Parse hook input
$hookData = $null
if ($InputJson) {
    try { $hookData = $InputJson | ConvertFrom-Json } catch { }
}

# Extract prompt
$prompt = if ($hookData.prompt) { $hookData.prompt }
          elseif ($hookData.user_prompt) { $hookData.user_prompt }
          elseif ($env:CLAUDE_USER_PROMPT) { $env:CLAUDE_USER_PROMPT }
          else { $null }

# Skip for empty, short, or command prompts
if (-not $prompt -or $prompt.Length -lt 5 -or $prompt.StartsWith('/')) {
    exit 0
}

# Initialize AI Handler (shared from ClaudeCLI)
[Environment]::SetEnvironmentVariable('CLAUDECLI_ENCRYPTION_KEY', 'ClaudeCLI-2024', 'Process')
$aiHandlerPath = "C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler"

# Quietly load modules
$null = Import-Module "$aiHandlerPath\AIModelHandler.psm1" -Force 2>&1
$null = Import-Module "$aiHandlerPath\modules\PromptOptimizer.psm1" -Force 2>&1

# Quick analysis
$category = "general"
$modelRec = "llama3.2:3b"

# Detect category from keywords
$promptLower = $prompt.ToLower()
if ($promptLower -match 'write|code|function|implement|script|class|def |fn ') {
    $category = "code"
    $modelRec = "qwen2.5-coder:1.5b"
} elseif ($promptLower -match 'explain|analyze|compare|why|how does') {
    $category = "analysis"
    $modelRec = "llama3.2:3b"
} elseif ($promptLower -match 'quick|fast|simple|what is|\?$') {
    $category = "quick"
    $modelRec = "llama3.2:1b"
} elseif ($promptLower -match 'debug|fix|error|issue|bug') {
    $category = "debug"
    $modelRec = "phi3:mini"
}

# Get CPU load
$cpu = 0
try {
    $cpu = [math]::Round((Get-CimInstance Win32_Processor).LoadPercentage, 0)
} catch { $cpu = 50 }

$provider = if ($cpu -lt 70) { "local" } elseif ($cpu -lt 90) { "hybrid" } else { "cloud" }

# Output context for Claude
Write-Output @"

<user-prompt-submit-hook>
AI Handler Active - Prompt Analysis:
  Category: $category
  Model: $modelRec
  Provider: $provider (CPU: $cpu%)
  Project: GeminiCLI

Quick Commands:
  /ai <query>     - Local AI (free)
  /ai-batch       - Parallel queries
  /ai-status      - Check providers
</user-prompt-submit-hook>
"@
</file>

<file path=".claude/hooks/permission-alert.ps1">
# Claude Code Permission Alert - FAST VERSION
# Quick alert when confirmation is needed

Add-Type -AssemblyName System.Windows.Forms
Add-Type -AssemblyName PresentationFramework

# Play quick attention sound (2 short beeps)
[console]::beep(1500, 100)
[console]::beep(1800, 150)

# Show toast notification (Windows 10/11)
try {
    $app = '{1AC14E77-02E7-4E5D-B744-2EB1AE5198B7}\WindowsPowerShell\v1.0\powershell.exe'
    [Windows.UI.Notifications.ToastNotificationManager, Windows.UI.Notifications, ContentType = WindowsRuntime] | Out-Null
    [Windows.Data.Xml.Dom.XmlDocument, Windows.Data.Xml.Dom.XmlDocument, ContentType = WindowsRuntime] | Out-Null

    $template = @"
<toast duration="short">
    <visual>
        <binding template="ToastGeneric">
            <text>CLAUDE CODE</text>
            <text>Wymaga potwierdzenia!</text>
        </binding>
    </visual>
    <audio src="ms-winsoundevent:Notification.Default"/>
</toast>
"@

    $xml = New-Object Windows.Data.Xml.Dom.XmlDocument
    $xml.LoadXml($template)
    $toast = [Windows.UI.Notifications.ToastNotification]::new($xml)
    [Windows.UI.Notifications.ToastNotificationManager]::CreateToastNotifier($app).Show($toast)
} catch {
    # Fallback: Quick MessageBox if toast fails
    [System.Windows.Forms.MessageBox]::Show(
        "Claude Code wymaga potwierdzenia!",
        "POTWIERDZENIE",
        [System.Windows.Forms.MessageBoxButtons]::OK,
        [System.Windows.Forms.MessageBoxIcon]::Warning
    ) | Out-Null
}

# Quick taskbar flash (2 blinks instead of 5)
try {
    $signature = @"
    [DllImport("user32.dll")]
    public static extern bool FlashWindow(IntPtr hwnd, bool bInvert);
"@
    $type = Add-Type -MemberDefinition $signature -Name "WinAPI" -Namespace "FlashWindow" -PassThru
    $hwnd = (Get-Process -Id $PID).MainWindowHandle
    for ($i = 0; $i -lt 2; $i++) {
        $type::FlashWindow($hwnd, $true)
        Start-Sleep -Milliseconds 150
    }
} catch { }
</file>

<file path=".eslintrc.cjs">
module.exports = {
  env: {
    node: true,
    es2022: true
  },
  extends: ['eslint:recommended'],
  parserOptions: {
    ecmaVersion: 2022,
    sourceType: 'module'
  },
  rules: {
    'no-unused-vars': ['error', { argsIgnorePattern: '^_' }]
  }
};
</file>

<file path=".gemini/commands/ai/batch.toml">
description = "Process multiple prompts in parallel using local Ollama"

prompt = """
Use @ollama-hydra ollama_batch to process these prompts in parallel:

{{args}}

Split the input by newlines or semicolons and process each as a separate prompt.
Return all results in order.
"""
</file>

<file path=".gemini/commands/ai/code.toml">
description = "Generate code with automatic self-correction and validation"

prompt = """
Use @ollama-hydra ollama_code to generate validated code for:

{{args}}

The code will be automatically validated and corrected if needed.
Return clean, working code with proper error handling.
"""
</file>

<file path=".gemini/commands/ai/config.toml">
description = "Show AI Handler configuration and settings"

prompt = """
Show the current AI Handler configuration by executing the PowerShell command:

. 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Initialize-AIHandler.ps1'; . 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Invoke-AIConfig.ps1' -Show

List the settings for local preference, cost optimization, and provider priority.
"""
</file>

<file path=".gemini/commands/ai/health.toml">
description = "Show AI Health Dashboard (tokens, cost, status)"

prompt = """
Show the AI health dashboard by executing the PowerShell command:

. 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Initialize-AIHandler.ps1'; . 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Invoke-AIHealth.ps1'

Summarize the token usage and estimated costs for different providers.
"""
</file>

<file path=".gemini/commands/ai/help.toml">
description = "Show AI Handler command reference"

prompt = """
Show the AI Handler help and command reference by executing the PowerShell command:

. 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Initialize-AIHandler.ps1'; . 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Invoke-AIHelp.ps1'
"""
</file>

<file path=".gemini/commands/ai/optimize.toml">
description = "Optimize a prompt for better AI responses"

prompt = """
Use @ollama-hydra prompt_optimize to analyze and enhance the user's prompt.

Steps:
1. Call prompt_analyze first to understand the prompt
2. Call prompt_optimize with the detected category
3. Show comparison: original vs optimized
4. Display clarity score and suggestions

Format output as:
## Original Prompt
{original}

## Analysis
- Category: {category}
- Language: {language}
- Clarity: {score}/100 ({quality})

## Optimized Prompt
{optimized}

## Enhancements Applied
{enhancements}

## Suggestions
{suggestions}
"""
</file>

<file path=".gemini/commands/ai/pull.toml">
description = "Manage Ollama models (pull, list, remove)"

prompt = """
Manage Ollama models using the PowerShell command:

. 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Initialize-AIHandler.ps1'; . 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Invoke-AIPull.ps1' {{args}}

Supported arguments:
- -List: List installed models
- -Popular: Show recommended models
- <model-name>: Download a new model
- -Remove <model-name>: Remove a model
"""
</file>

<file path=".gemini/commands/ai/quick.toml">
description = "Quick AI query using local Ollama (fastest response)"

prompt = """
Use @ollama-hydra ollama_speculative to answer this question with racing between fast and accurate models:

{{args}}

Return the response directly without explanation of the process.
"""
</file>

<file path=".gemini/commands/ai/speculate.toml">
description = "Speculative decoding - race multiple models for best response"

prompt = """
Use @ollama-hydra ollama_race to race these models: llama3.2:1b, phi3:mini, llama3.2:3b

Query: {{args}}

Return the winning response with timing information.
"""
</file>

<file path=".gemini/commands/ai/status.toml">
description = "Show status of AI providers and models"

prompt = """
Show the status of all AI providers by executing the PowerShell command:

. 'C:\\Users\\BIURODOM\\Desktop\\GeminiCLI\\ai-handler\\Initialize-AIHandler.ps1'; Get-AIStatus

Check if providers like Anthropic, OpenAI, and Ollama are correctly configured and available.
"""
</file>

<file path=".gemini/commands/gemini/models.toml">
# Gemini Models Command - List and manage Gemini API models
# Usage: /gemini:models [action]

name = "models"
description = "List and manage Gemini API models"
category = "gemini"
requires_mcp = "ollama-hydra"

prompt = """
Use the @ollama-hydra MCP server to fetch and display Gemini models.

Available actions:
- list: Show all available models (gemini_models)
- summary: Show models summary by family and capability (gemini_models_summary)
- recommend: Get model recommendations for different use cases (gemini_models_recommend)
- details <model>: Get details for specific model (gemini_model_details)
- filter <capability>: Filter by capability (gemini_models_filter)

Execute the appropriate tool based on the request. If no action specified, show summary.

Capabilities: generateContent, countTokens, embedContent, generateAnswer, batchEmbedContents

After getting results, format them nicely in a table or list.
"""
</file>

<file path=".gemini/commands/hydra/config.toml">
description = "Show HYDRA configuration and settings"

prompt = """
Display the current HYDRA configuration:

1. Read .gemini/settings.json and show MCP server configurations
2. Show enabled features: speculative decoding, self-correction, caching
3. Display model preferences: default, fast, coder models
4. Show fallback chain

Format as a readable configuration summary.
"""
</file>

<file path=".gemini/commands/hydra/status.toml">
description = "Check HYDRA system status - Ollama, MCP servers, cache"

prompt = """
Check the full HYDRA system status:

1. Use @ollama-hydra ollama_status to check Ollama availability and cache stats
2. List available local models
3. Show cache statistics
4. Display current configuration

Format the output as a clear status report.
"""
</file>

<file path=".gemini/commands/mcp/health.toml">
description = "Health check for all MCP servers"

prompt = """
Perform a health check on all configured MCP servers:

1. @ollama-hydra ollama_status - Check Ollama HYDRA server
2. @serena - Check if Serena responds
3. @desktop-commander list_directory "." - Check Desktop Commander
4. @playwright - Check if Playwright is available

Report status of each server: ONLINE/OFFLINE with any error messages.
"""
</file>

<file path=".gemini/commands/queue/status.toml">
# Queue Status Command - View and manage prompt queue
# Usage: /queue:status [action]

name = "status"
description = "View and manage the prompt queue"
category = "queue"
requires_mcp = "ollama-hydra"

prompt = """
Use @ollama-hydra MCP server to manage the prompt queue.

Available actions:
- status: Show queue status (queue_status)
- pause: Pause processing (queue_pause)
- resume: Resume processing (queue_resume)
- cancel-all: Cancel all items (queue_cancel_all)

If no action specified, show status.

Format the output nicely showing:
- Queued items count
- Running items count
- Completed/Failed counts
- Average processing time
- Rate limit status
"""
</file>

<file path=".github/workflows/ci.yml">
name: ci

on:
  push:
    branches: [main, develop]
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v4
        with:
          version: 9
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: pnpm
      - run: pnpm install
      - run: pnpm lint
      - run: pnpm test
</file>

<file path=".gitignore">
# Dependencies
node_modules/

# Environment variables (secrets)
.env

# Cache
cache/

# OS files
.DS_Store
Thumbs.db
*.swp
*.swo

# Logs
*.log
npm-debug.log*

# IDE
.vscode/
.idea/
</file>

<file path=".prettierrc.json">
{
  "singleQuote": true,
  "semi": true,
  "trailingComma": "none"
}
</file>

<file path=".serena/.gitignore">
/cache
</file>

<file path=".serena/memories/Ciri.md">
# Memory of Ciri
</file>

<file path=".serena/memories/Dijkstra.md">
# Memory of Dijkstra
</file>

<file path=".serena/memories/Eskel.md">
# Memory of Eskel
</file>

<file path=".serena/memories/Geralt.md">
01/13/2026 23:32:51: Executed task
</file>

<file path=".serena/memories/Jaskier.md">
# Memory of Jaskier
</file>

<file path=".serena/memories/Lambert.md">
# Memory of Lambert
</file>

<file path=".serena/memories/Philippa.md">
# Memory of Philippa
</file>

<file path=".serena/memories/Regis.md">
# Memory of Regis
</file>

<file path=".serena/memories/Triss.md">
# Memory of Triss
</file>

<file path=".serena/memories/Vesemir.md">
# Memory of Vesemir
</file>

<file path=".serena/memories/Yennefer.md">
01/13/2026 23:32:51: Executed task
</file>

<file path=".serena/memories/Zoltan.md">
# Memory of Zoltan
</file>

<file path="AGENTS.md">
# Repository Guidelines

## Project Structure & Module Organization

- `src/`: Node.js MCP server and core logic (entrypoint: `src/server.js`).
- `scripts/`: Dev utilities (e.g., `scripts/doctor.mjs`, `scripts/launcher/`).
- `ai-handler/`: PowerShell orchestration layer and modules (notably `ai-handler/modules/*.psm1`).
- `modules/`: PowerShell GUI helpers used by the launcher/profile.
- `test/`: Unit tests (`*.test.js`) for the Node layer.
- `cache/`, `logs/`: Runtime artifacts (gitignored).
- Docs: `README.md`, `ARCHITECTURE.md`, `GEMINI.md` (Gemini CLI), `CODEX.md` (Codex CLI).

## Build, Test, and Development Commands

- Requires Node.js `>=20` (see `package.json#engines`).
- `npm ci` (preferred) or `npm install`: Install dependencies (lockfile: `package-lock.json`). `pnpm install` is also supported if you use pnpm locally.
- `npm start`: Run the MCP server (`node src/server.js`).
- `npm run launcher`: Launch `./_launcher.ps1` if PowerShell is available; otherwise falls back to `npm start`.
- `npm run doctor`: Validate Node/PowerShell/Ollama availability and basic health checks.
- `npm test`: Run unit tests via Node’s built-in runner (`node --test`).
- `npm run lint`: Run ESLint.
- `npm run format` / `npm run format:write`: Check/apply Prettier formatting.

## Codex CLI

- `start_codex.ps1`: Launch Codex CLI in this repo (`codex run`).
- Keep `CODEX.md` aligned with Codex CLI behavior and constraints.
- Prefer MCP tools when available: Serena for code navigation/memory, Desktop Commander for filesystem/shell, Playwright for web. Use local shell tools only as fallback.

## Coding Style & Naming Conventions

- JavaScript uses ESM (`"type": "module"`); prefer `import`/`export`.
- Prettier is the source of truth (single quotes, semicolons, no trailing commas).
- ESLint: unused params/vars must be prefixed with `_` to avoid lint errors.
- Prefer kebab-case filenames in `src/` (e.g., `prompt-queue.js`); tests end with `.test.js`.

## Testing Guidelines

- Keep unit tests in `test/` and avoid network/Ollama dependencies when possible.
- Name tests `*.test.js` and run them with `npm test`.

## Commit & Pull Request Guidelines

- Follow Conventional Commits seen in history: `feat(scope): ...`, `fix: ...`, `docs: ...`, `chore: ...`.
- PRs should include a clear description, how to verify (`npm test`, `npm run lint`), and note any config changes.
- If you add/rename env vars, update `.env.example` and keep secrets out of Git.

## Security & Configuration Tips

- Copy `.env.example` to `.env` and set `GEMINI_API_KEY`, `OLLAMA_HOST`, and related settings.
- Never commit `.env`, API keys, or tokens; keep local artifacts in `cache/` and `logs/`.
- Treat `CODEX.md` / `GEMINI.md` as operational contracts; review changes carefully.

## Communication Preferences

- Respond in Polish, in the style of Jaskier (The Witcher bard).
- Use sarcasm and light anecdotes; avoid sexual or explicit content.
- Keep the tone witty and playful while staying respectful.
</file>

<file path="gemini-extension.json">
{
  "name": "ollama-hydra",
  "version": "1.0.0",
  "description": "HYDRA AI Handler - Ollama integration with speculative decoding, self-correction, and caching",
  "contextFileName": ["GEMINI.md", "HYDRA.md"]
}
</file>

<file path="GeminiCLI.vbs">
' Gemini CLI Launcher (HYDRA Ollama Extension)
' Enhanced with Windows Terminal support
Option Explicit
On Error Resume Next

Dim objShell, objWMI, objFSO
Dim colProcesses, objProcess
Dim ports, port, killCount
Dim userProfile, strScriptPath

Set objShell = CreateObject("WScript.Shell")
Set objWMI = GetObject("winmgmts:\\.\root\cimv2")
Set objFSO = CreateObject("Scripting.FileSystemObject")

killCount = 0
userProfile = objShell.ExpandEnvironmentStrings("%USERPROFILE%")
strScriptPath = objFSO.GetParentFolderName(WScript.ScriptFullName)

' Ollama default port
ports = Array(11434)

' 1. CLEANUP - Kill stale processes on Ollama port if needed
For Each port In ports
    CheckAndKillPort port
Next

CleanStaleLocks()

' 2. OLLAMA HEALTH CHECK - ensure Ollama is running
Dim ollamaRunning
ollamaRunning = IsOllamaRunning()
If Not ollamaRunning Then
    ' Start Ollama in background
    objShell.Run "ollama serve", 0, False
    WScript.Sleep 2000
End If

' 3. LAUNCH - prefer Windows Terminal, fallback to PowerShell
Dim launcherPS1, wtExe, useWT
launcherPS1 = strScriptPath & "\_launcher.ps1"

' Check if Windows Terminal is installed
wtExe = userProfile & "\AppData\Local\Microsoft\WindowsApps\wt.exe"
useWT = objFSO.FileExists(wtExe)

If useWT Then
    ' Launch with Windows Terminal using custom profile
    objShell.Run "wt.exe -p ""Gemini CLI (HYDRA)"" --title ""Gemini CLI"" powershell.exe -NoExit -ExecutionPolicy Bypass -File """ & launcherPS1 & """", 1, False
Else
    ' Fallback to standard PowerShell
    objShell.Run "powershell.exe -NoExit -ExecutionPolicy Bypass -File """ & launcherPS1 & """", 1, False
End If

' === FUNCTIONS ===
Function IsOllamaRunning()
    Dim colProcs
    Set colProcs = objWMI.ExecQuery("SELECT * FROM Win32_Process WHERE Name = 'ollama.exe'")
    IsOllamaRunning = (colProcs.Count > 0)
End Function

Sub CheckAndKillPort(portNum)
    Dim objExec, strOutput, arrLines, strLine, arrParts, pid
    Set objExec = objShell.Exec("cmd /c netstat -ano | findstr :" & portNum)
    If Not objExec.StdOut.AtEndOfStream Then
        strOutput = objExec.StdOut.ReadAll()
    Else
        strOutput = ""
    End If

    If Len(Trim(strOutput)) > 0 Then
        arrLines = Split(strOutput, vbCrLf)
        For Each strLine In arrLines
            If InStr(strLine, "LISTENING") > 0 Then
                strLine = Trim(strLine)
                arrParts = Split(strLine, " ")
                pid = arrParts(UBound(arrParts))
                If IsNumeric(pid) And CInt(pid) > 0 Then
                    ' Don't kill Ollama itself, just stale connections
                    ' KillProcessByPID CInt(pid)
                End If
            End If
        Next
    End If
End Sub

Sub KillProcessByPID(pid)
    Dim colProcs, objProc
    On Error Resume Next
    Set colProcs = objWMI.ExecQuery("SELECT * FROM Win32_Process WHERE ProcessId = " & pid)
    For Each objProc In colProcs
        objProc.Terminate()
    Next
End Sub

Sub CleanStaleLocks()
    Dim lockPaths, lockPath, folder
    lockPaths = Array( _
        userProfile & "\.gemini\locks", _
        userProfile & "\.gemini\.locks", _
        userProfile & "\AppData\Local\Temp\gemini-locks" _
    )
    For Each lockPath In lockPaths
        If objFSO.FolderExists(lockPath) Then
            Set folder = objFSO.GetFolder(lockPath)
            DeleteFilesInFolder folder
        End If
    Next
End Sub

Sub DeleteFilesInFolder(folder)
    Dim file, subfolder
    On Error Resume Next
    For Each file In folder.Files
        file.Delete True
    Next
    For Each subfolder In folder.SubFolders
        DeleteFilesInFolder subfolder
    Next
End Sub
</file>

<file path="profile.ps1">
# ═══════════════════════════════════════════════════════════════════════════════
# GEMINI CLI - CUSTOM PROFILE
# Isolated profile for GeminiCLI (does not load user's default profile)
# ═══════════════════════════════════════════════════════════════════════════════

# === PSReadLine Configuration ===
if (Get-Module -Name PSReadLine -ErrorAction SilentlyContinue) {
    # Double-Escape as interrupt
    $script:lastEscapeTime = [DateTime]::MinValue
    Set-PSReadLineKeyHandler -Key Escape -ScriptBlock {
        $now = [DateTime]::Now
        $diff = ($now - $script:lastEscapeTime).TotalMilliseconds
        if ($diff -lt 400) {
            [Microsoft.PowerShell.PSConsoleReadLine]::CancelLine()
            [Microsoft.PowerShell.PSConsoleReadLine]::AcceptLine()
        } else {
            [Microsoft.PowerShell.PSConsoleReadLine]::RevertLine()
        }
        $script:lastEscapeTime = $now
    }

    # Ctrl+C trap
    Set-PSReadLineKeyHandler -Key Ctrl+c -ScriptBlock {
        $line = $null
        $cursor = $null
        [Microsoft.PowerShell.PSConsoleReadLine]::GetBufferState([ref]$line, [ref]$cursor)
        if ($line.Length -gt 0) {
            [Microsoft.PowerShell.PSConsoleReadLine]::CancelLine()
        } else {
            Write-Host "`n[Ctrl+C] Use 'exit' to quit or Double-Escape to interrupt" -ForegroundColor Yellow
            [Microsoft.PowerShell.PSConsoleReadLine]::AcceptLine()
        }
    }

    # Alt+t to toggle Deep Thinking
    Set-PSReadLineKeyHandler -Chord 'Alt+t' -ScriptBlock {
        $current = $env:GEMINI_DEEP_THINKING
        if ($current -eq '1') {
            $env:GEMINI_DEEP_THINKING = '0'
            Write-Host "`n[Deep Thinking: OFF]" -ForegroundColor DarkGray
        } else {
            $env:GEMINI_DEEP_THINKING = '1'
            Write-Host "`n[Deep Thinking: ON]" -ForegroundColor Magenta
        }
        [Microsoft.PowerShell.PSConsoleReadLine]::AcceptLine()
    }

    # F1 for tooltip
    Set-PSReadLineKeyHandler -Key F1 -BriefDescription 'Show tooltip' -ScriptBlock {
        [Microsoft.PowerShell.PSConsoleReadLine]::InvokePrompt()
    }
}

# === Gemini Function (direct, no wrapper) ===
function Start-Gemini {
    param([Parameter(ValueFromRemainingArguments)]$Arguments)
    
    $key = $env:GOOGLE_API_KEY
    if (-not $key) {
        $key = [System.Environment]::GetEnvironmentVariable('GOOGLE_API_KEY', 'User')
    }
    if (-not $key) {
        $key = [System.Environment]::GetEnvironmentVariable('GEMINI_API_KEY', 'User')
    }
    if ($key) { $env:GOOGLE_API_KEY = $key }
    
    $model = if ($key) { "gemini-1.5-pro" } else { "gemini-1.5-flash" }
    
    $geminiPath = (Get-Command gemini -CommandType Application -ErrorAction SilentlyContinue | Select-Object -First 1).Source
    if ($geminiPath) {
        & $geminiPath -m $model @Arguments
    } else {
        npx @google/gemini-cli -m $model @Arguments
    }
}

Set-Alias -Name g -Value Start-Gemini

# === Prompt ===
if (Get-Command oh-my-posh -ErrorAction SilentlyContinue) {
    oh-my-posh init pwsh --config "$PSScriptRoot\gemini-hydra.omp.json" --transient | Invoke-Expression
} else {
    function prompt {
        Write-Host "Gemini > " -NoNewline -ForegroundColor Cyan
        return " "
    }
}
</file>

<file path="scripts/doctor.mjs">
import 'dotenv/config';

import { accessSync, constants } from 'node:fs';
import { spawnSync } from 'node:child_process';
import { dirname, join } from 'node:path';
import { fileURLToPath } from 'node:url';

import { checkHealth } from '../src/ollama-client.js';

const __dirname = dirname(fileURLToPath(import.meta.url));
const repoRoot = join(__dirname, '..');

const results = {
  ok: 0,
  warn: 0,
  fail: 0
};

const log = (level, message) => {
  console.log(`[${level}] ${message}`);
};

const ok = (message) => {
  results.ok += 1;
  log('ok', message);
};

const warn = (message) => {
  results.warn += 1;
  log('warn', message);
};

const fail = (message) => {
  results.fail += 1;
  log('fail', message);
};

const hasPath = (path) => {
  try {
    accessSync(path, constants.R_OK);
    return true;
  } catch {
    return false;
  }
};

const canRunPowerShell = (command) => {
  const result = spawnSync(
    command,
    ['-NoProfile', '-Command', '$PSVersionTable.PSVersion'],
    {
      stdio: 'ignore'
    }
  );
  if (result.error) return false;
  return result.status === 0;
};

const resolvePowerShell = () => {
  const candidates =
    process.platform === 'win32'
      ? ['powershell.exe', 'pwsh']
      : ['pwsh', 'powershell'];
  for (const candidate of candidates) {
    if (canRunPowerShell(candidate)) return candidate;
  }
  return null;
};

const nodeMajor = Number.parseInt(
  process.versions.node.split('.')[0] ?? '0',
  10
);
if (nodeMajor >= 20) {
  ok(`Node.js ${process.versions.node} (>=20)`);
} else {
  fail(`Node.js ${process.versions.node} (requires >=20)`);
}

if (hasPath(join(repoRoot, 'package.json'))) {
  ok('package.json present');
} else {
  fail('package.json missing');
}

if (hasPath(join(repoRoot, 'src', 'server.js'))) {
  ok('src/server.js present');
} else {
  fail('src/server.js missing');
}

if (hasPath(join(repoRoot, 'scripts', 'launcher', 'index.mjs'))) {
  ok('scripts/launcher/index.mjs present');
} else {
  warn('scripts/launcher/index.mjs missing');
}

if (hasPath(join(repoRoot, '_launcher.ps1'))) {
  ok('_launcher.ps1 present');
} else {
  warn('_launcher.ps1 missing (launcher will fall back to npm start)');
}

const nodeModulesPath = join(repoRoot, 'node_modules');
if (hasPath(nodeModulesPath)) {
  ok('node_modules present');
} else {
  warn('node_modules missing (run npm install)');
}

if (hasPath(join(repoRoot, '.env'))) {
  ok('.env present');
} else {
  warn('Missing .env (copy .env.example to .env)');
}

if (process.env.GEMINI_API_KEY) {
  ok('GEMINI_API_KEY set');
} else {
  warn('GEMINI_API_KEY not set');
}

if (process.env.CACHE_ENCRYPTION_KEY) {
  ok('CACHE_ENCRYPTION_KEY set');
} else {
  warn('CACHE_ENCRYPTION_KEY not set');
}

const psCommand = resolvePowerShell();
if (psCommand) {
  ok(`PowerShell available (${psCommand})`);
} else {
  warn('PowerShell not available');
}

const health = await checkHealth();
if (health.available) {
  const modelCount = Array.isArray(health.models) ? health.models.length : 0;
  ok(`Ollama reachable at ${health.host} (${modelCount} models)`);
} else {
  const host =
    health.host || process.env.OLLAMA_HOST || 'http://localhost:11434';
  fail(`Ollama not reachable at ${host}: ${health.error || 'unknown error'}`);
}

if (results.fail > 0) {
  log(
    'fail',
    `Doctor found ${results.fail} failure(s), ${results.warn} warning(s).`
  );
  process.exitCode = 1;
} else if (results.warn > 0) {
  log('warn', `Doctor found ${results.warn} warning(s).`);
} else {
  log('ok', 'All checks passed.');
}
</file>

<file path="src/memory.js">
import { appendFile, mkdir, readFile, writeFile } from 'node:fs/promises';
import { dirname, join } from 'node:path';
import { fileURLToPath } from 'node:url';

const __dirname = dirname(fileURLToPath(import.meta.url));
const repoRoot = join(__dirname, '..');
const memoryDir = join(repoRoot, '.serena', 'memories');

const formatDate = (date) => date.toISOString().slice(0, 10);
const formatStamp = (date) => date.toISOString().replace(/[:.]/g, '-');

const normalizeText = (value) => {
  if (!value) return '';
  return `${value}`.replace(/\r\n/g, '\n').trim();
};

const getTitle = (title, prompt) => {
  const base = normalizeText(title) || normalizeText(prompt).split('\n')[0];
  if (!base) return 'Swarm Task';
  return base.length > 80 ? `${base.slice(0, 80)}...` : base;
};

const ensureLogHeader = async (logPath, dateLabel) => {
  try {
    await readFile(logPath, 'utf8');
    return false;
  } catch (error) {
    if (error.code !== 'ENOENT') throw error;
  }
  const header = [
    `# Task Log - ${dateLabel}`,
    `**Date**: ${dateLabel}`,
    '**Type**: Multi-task Day',
    ''
  ].join('\n');
  await writeFile(logPath, `${header}\n`, 'utf8');
  return true;
};

const maybeRebaseLog = async (logPath) => {
  if (Math.random() > 0.1) return false;
  const content = await readFile(logPath, 'utf8').catch(() => null);
  if (!content) return false;
  const compacted = content.replace(/\n{3,}/g, '\n\n').trimEnd() + '\n';
  if (compacted !== content) {
    await writeFile(logPath, compacted, 'utf8');
  }
  return true;
};

export const writeSwarmMemory = async ({
  title,
  prompt,
  steps,
  agents,
  summary,
  finalAnswer
}) => {
  await mkdir(memoryDir, { recursive: true });
  const now = new Date();
  const dateLabel = formatDate(now);
  const stamp = formatStamp(now);
  const safeTitle = getTitle(title, prompt);
  const archiveFile = `swarm-archive-${stamp}.md`;
  const archivePath = join(memoryDir, archiveFile);
  const logPath = join(memoryDir, `task-log-${dateLabel}.md`);

  const agentBlocks = (agents || [])
    .map((agent) =>
      [
        `### ${agent.name}`,
        `**Model**: ${agent.model}`,
        '',
        normalizeText(agent.response)
      ].join('\n')
    )
    .join('\n\n');

  const archiveContent = [
    `# Swarm Archive - ${stamp}`,
    `**Date**: ${dateLabel}`,
    `**Title**: ${safeTitle}`,
    '',
    '## Prompt',
    normalizeText(prompt),
    '',
    '## Speculate',
    normalizeText(steps?.speculation),
    '',
    '## Plan',
    normalizeText(steps?.plan),
    '',
    '## Execute',
    agentBlocks || 'No agent output.',
    '',
    '## Synthesize',
    normalizeText(finalAnswer),
    '',
    '## Log',
    normalizeText(summary)
  ].join('\n');

  await writeFile(archivePath, `${archiveContent}\n`, 'utf8');

  await ensureLogHeader(logPath, dateLabel);
  const logEntry = [
    `## Task: ${safeTitle}`,
    '**Status**: Completed',
    '**Agent**: AgentSwarm',
    '',
    '### Outcome',
    `- ${normalizeText(summary) || 'Summary unavailable.'}`,
    '',
    '### Notes',
    `- Archive: ${archiveFile}`,
    `- Agents: ${(agents || []).map((agent) => agent.name).join(', ') || 'None'}`,
    ''
  ].join('\n');

  await appendFile(logPath, `${logEntry}\n`, 'utf8');
  const rebased = await maybeRebaseLog(logPath);

  return {
    archivePath,
    logPath,
    rebased
  };
};
</file>

<file path="src/version.js">
import { readFileSync } from 'node:fs';
import { createLogger } from './logger.js';

const logger = createLogger('version');
const PACKAGE_JSON_URL = new URL('../package.json', import.meta.url);

const readPackageJson = () => {
  try {
    return JSON.parse(readFileSync(PACKAGE_JSON_URL, 'utf-8'));
  } catch (error) {
    logger.error('Failed to read package.json', { error: error.message });
    return {};
  }
};

export const resolveServerVersion = () => {
  const npmPackageVersion = process.env.npm_package_version;
  if (npmPackageVersion) {
    return npmPackageVersion;
  }
  const packageJson = readPackageJson();
  return packageJson.version ?? '0.0.0';
};

export const resolveNodeEngines = () => {
  const packageJson = readPackageJson();
  return packageJson.engines?.node ?? null;
};
</file>

<file path="start_codex.ps1">
Set-Location "C:\Users\BIURODOM\Desktop\GeminiCLI"
$env:HYDRA_YOLO_MODE = 'true'
codex run
</file>

<file path=".claude/commands/ai-batch.md">
---
description: 'Parallel batch AI queries using local Ollama (cost=$0)'
---

# /ai-batch - Parallel Batch Processing

Run multiple AI queries in parallel using local Ollama. Zero cost.

## Usage

```
/ai-batch
prompt1: <first query>
prompt2: <second query>
prompt3: <third query>
```

## Instructions for Claude

When user provides multiple prompts, execute them in parallel:

```powershell
[Environment]::SetEnvironmentVariable('CLAUDECLI_ENCRYPTION_KEY', 'ClaudeCLI-2024', 'Process')
Import-Module 'C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\AIModelHandler.psm1' -Force

$prompts = @("prompt1", "prompt2", "prompt3")
Invoke-AIBatch -Prompts $prompts -Model "llama3.2:3b" -MaxConcurrent 4
```

**Features:**

- Parallel execution (up to 4 concurrent)
- Auto load balancing
- Zero cost (local Ollama)
</file>

<file path=".claude/commands/ai-config.md">
---
description: 'Configure AI Handler settings'
---

# /ai-config - AI Handler Configuration

View and modify AI Handler configuration settings like local/cloud preference and parallel execution limits.

## Instructions for Claude

Execute this command:

```bash
powershell -ExecutionPolicy Bypass -Command "[Environment]::SetEnvironmentVariable('CLAUDECLI_ENCRYPTION_KEY', 'ClaudeCLI-2024', 'Process'); . 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Initialize-AIHandler.ps1'; . 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Invoke-AIConfig.ps1' {{args}}"
```

Available arguments:

- `-Show`: View current configuration
- `-PreferLocal true/false`: Set local Ollama preference
- `-AutoFallback true/false`: Enable/disable automatic provider switching
- `-DefaultModel <name>`: Set default Ollama model
- `-MaxConcurrent <number>`: Set max parallel requests
- `-Reset`: Reset to default settings
</file>

<file path=".claude/commands/ai-health.md">
---
description: 'Show AI Health Dashboard'
---

# /ai-health - AI Handler Health

Display health dashboard including provider status, token usage, and costs.

## Instructions for Claude

Execute this command:

```bash
powershell -ExecutionPolicy Bypass -Command ". 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Initialize-AIHandler.ps1'; . 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Invoke-AIHealth.ps1' {{args}}"
```

Available arguments:

- `-Json`: Export health data in JSON format
</file>

<file path=".claude/commands/ai-help.md">
---
description: 'Show AI Handler command reference'
---

# /ai-help - AI Handler Help

Display available commands and quick start guide for the AI Handler system.

## Instructions for Claude

Execute this command:

```bash
powershell -ExecutionPolicy Bypass -Command ". 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Initialize-AIHandler.ps1'; . 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Invoke-AIHelp.ps1'"
```
</file>

<file path=".claude/commands/ai-pull.md">
---
description: 'Pull/download Ollama models'
---

# /ai-pull - AI Model Management

Download new models for local Ollama, list installed models, or show popular recommendations.

## Instructions for Claude

Execute this command:

```bash
powershell -ExecutionPolicy Bypass -Command ". 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Initialize-AIHandler.ps1'; . 'C:\Users\BIURODOM\Desktop\GeminiCLI\ai-handler\Invoke-AIPull.ps1' {{args}}"
```

Available arguments:

- `-List`: List installed models and their sizes
- `-Popular`: Show recommended models to download
- `<model-name>`: Pull/download specific model
- `-Remove <model-name>`: Remove an installed model
</file>

<file path=".claude/commands/ai-status.md">
---
description: 'Check AI providers, models and configuration status'
---

# /ai-status - AI Handler Status

Check the status of all configured AI providers and models.

## Instructions for Claude

Execute this command:

```bash
powershell -ExecutionPolicy Bypass -Command "[Environment]::SetEnvironmentVariable('CLAUDECLI_ENCRYPTION_KEY', 'ClaudeCLI-2024', 'Process'); Import-Module 'C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\AIModelHandler.psm1' -Force; Get-AIStatus"
```

Display the output showing:

- Provider status (OK/NO KEY/ERROR)
- Available models per provider
- Rate limit usage
- Current settings
</file>

<file path=".claude/commands/ai.md">
---
description: 'Quick local AI query using Ollama (cost=$0)'
---

# /ai - Quick Local AI Query

Execute a quick AI query using local Ollama models. Zero cost, fast response.

## Usage

```
/ai <your question or task>
```

## Examples

```
/ai explain this error: TypeError undefined is not a function
/ai write a regex to match email addresses
/ai summarize: <paste text>
```

## Instructions for Claude

When the user invokes `/ai`, execute this command using Bash tool:

```bash
powershell -ExecutionPolicy Bypass -Command "[Environment]::SetEnvironmentVariable('CLAUDECLI_ENCRYPTION_KEY', 'ClaudeCLI-2024', 'Process'); Import-Module 'C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\AIModelHandler.psm1' -Force; Invoke-AIRequest -Provider 'ollama' -Model 'llama3.2:3b' -Messages @(@{role='user'; content='$ARGUMENTS'})"
```

**Important:**

1. Always use local Ollama (cost=$0)
2. Display the full response to user
3. If Ollama not running, it auto-starts

## Model Selection

| Query Type        | Model                | Why             |
| ----------------- | -------------------- | --------------- |
| General questions | `llama3.2:3b`        | Best quality    |
| Code generation   | `qwen2.5-coder:1.5b` | Code specialist |
| Quick/simple      | `llama3.2:1b`        | Fastest         |

## Query: $ARGUMENTS
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash",
      "mcp__serena__*",
      "mcp__desktop-commander__*",
      "mcp__playwright__*",
      "Write",
      "Edit",
      "Read",
      "Glob",
      "Grep",
      "Skill",
      "WebSearch",
      "WebFetch(domain:github.com)",
      "WebFetch(domain:ai.google.dev)",
      "WebFetch(domain:cloud.google.com)"
    ],
    "deny": []
  },
  "enabledMcpjsonServers": ["serena", "desktop-commander", "playwright"],
  "hooks": {
    "UserPromptSubmit": [
      {
        "matcher": "",
        "hooks": [
          {
            "type": "command",
            "command": "powershell -NoProfile -ExecutionPolicy Bypass -File .claude/hooks/ai-handler-integration.ps1"
          }
        ]
      }
    ],
    "Notification": [
      {
        "matcher": "permission_prompt",
        "hooks": [
          {
            "type": "command",
            "command": "powershell -NoProfile -ExecutionPolicy Bypass -File .claude/hooks/permission-alert.ps1"
          }
        ]
      }
    ]
  },
  "commands": {},
  "statusLine": {
    "type": "command",
    "command": "node .claude/statusline.cjs"
  }
}
</file>

<file path=".claude/statusline.cjs">
#!/usr/bin/env node
/**
 * Claude Code CLI Status Line - AI HANDLER EDITION v5
 *
 * FULL:    AI | Model | Context | Tokens | Limits | [MCP] | Sys:CPU/RAM
 * COMPACT: AI | Model | Ctx | I/O | Lim | MCP | C%/R%
 */

const fs = require('fs');
const path = require('path');
const os = require('os');
const { execSync } = require('child_process');

const USAGE_FILE = path.join(os.tmpdir(), 'claude-usage-tracking.json');
const CONFIG_FILE = path.join(__dirname, '..', 'ai-models.json');
const SETTINGS_FILE = path.join(__dirname, 'settings.local.json');

const TERMINAL_WIDTH = process.stdout.columns || 120;
const COMPACT_MODE =
  process.env.STATUSLINE_COMPACT === '1' || TERMINAL_WIDTH < 100;

const c = {
  reset: '\x1b[0m',
  bold: '\x1b[1m',
  dim: '\x1b[2m',
  blink: '\x1b[5m',
  gray: '\x1b[90m',
  neonRed: '\x1b[91m',
  neonGreen: '\x1b[92m',
  neonYellow: '\x1b[93m',
  neonBlue: '\x1b[94m',
  neonMagenta: '\x1b[95m',
  neonCyan: '\x1b[96m',
  neonWhite: '\x1b[97m'
};

let aiConfig;
try {
  aiConfig = JSON.parse(fs.readFileSync(CONFIG_FILE, 'utf8'));
} catch (_e) {
  aiConfig = {
    models: {},
    tiers: {
      pro: { label: 'PRO', color: 'magenta' },
      standard: { label: 'STD', color: 'blue' },
      lite: { label: 'LITE', color: 'green' }
    }
  };
}

let mcpServers = [];
try {
  const settings = JSON.parse(fs.readFileSync(SETTINGS_FILE, 'utf8'));
  mcpServers = settings.enabledMcpjsonServers || [];
} catch (_e) {
  mcpServers = ['serena', 'desktop-commander', 'playwright'];
}

function getColorCode(colorName) {
  const map = {
    magenta: c.neonMagenta,
    blue: c.neonBlue,
    green: c.neonGreen,
    cyan: c.neonCyan,
    yellow: c.neonYellow,
    red: c.neonRed,
    white: c.neonWhite
  };
  return map[colorName] || c.neonWhite;
}

function fmt(n) {
  if (n === undefined || n === null) return '0';
  if (n >= 1000000) return (n / 1000000).toFixed(1) + 'M';
  if (n >= 1000) return (n / 1000).toFixed(1) + 'K';
  return Math.round(n).toString();
}

function getRemainColor(percent) {
  if (percent <= 10) return c.neonRed + c.blink + c.bold;
  if (percent <= 25) return c.neonRed;
  if (percent <= 50) return c.neonYellow;
  return c.neonGreen;
}

function getUsageColor(percent) {
  if (percent >= 90) return c.neonRed;
  if (percent >= 70) return c.neonYellow;
  return c.neonGreen;
}

function getModelConfig(modelId) {
  if (!modelId) return null;
  const id = modelId.toLowerCase();
  if (aiConfig.models[id]) return { ...aiConfig.models[id], id };
  for (const [key, model] of Object.entries(aiConfig.models)) {
    if (id.includes(key)) return { ...model, id: key };
  }
  return null;
}

function checkAIHandlerStatus() {
  let ollamaRunning = false;
  let modelCount = 0;
  try {
    if (process.platform === 'win32') {
      const output = execSync(
        'tasklist /FI "IMAGENAME eq ollama.exe" /NH 2>nul',
        { encoding: 'utf8', timeout: 2000, windowsHide: true }
      );
      ollamaRunning = output.toLowerCase().includes('ollama.exe');
    } else {
      const output = execSync('pgrep -x ollama 2>/dev/null || echo ""', {
        encoding: 'utf8',
        timeout: 2000
      });
      ollamaRunning = output.trim().length > 0;
    }
    if (ollamaRunning) {
      try {
        const modelsOutput = execSync('ollama list 2>nul', {
          encoding: 'utf8',
          timeout: 3000,
          windowsHide: true
        });
        modelCount = modelsOutput.trim().split('\n').length - 1;
        if (modelCount < 0) modelCount = 0;
      } catch (_e) {
        modelCount = 0;
      }
    }
  } catch (_e) {
    ollamaRunning = false;
  }
  return { running: ollamaRunning, models: modelCount };
}

function getSystemResources() {
  const totalMem = os.totalmem();
  const freeMem = os.freemem();
  const usedMem = totalMem - freeMem;
  const ramPercent = Math.round((usedMem / totalMem) * 100);
  const ramUsedGB = (usedMem / 1024 ** 3).toFixed(1);
  const ramTotalGB = (totalMem / 1024 ** 3).toFixed(1);

  const cpus = os.cpus();
  let totalIdle = 0,
    totalTick = 0;
  for (const cpu of cpus) {
    for (const type in cpu.times) totalTick += cpu.times[type];
    totalIdle += cpu.times.idle;
  }
  const cpuPercent = Math.round(100 - (totalIdle / totalTick) * 100);

  return {
    cpu: { percent: cpuPercent, cores: cpus.length },
    ram: { percent: ramPercent, usedGB: ramUsedGB, totalGB: ramTotalGB }
  };
}

function getResourceColor(percent) {
  if (percent >= 90) return c.neonRed + c.bold;
  if (percent >= 75) return c.neonRed;
  if (percent >= 50) return c.neonYellow;
  return c.neonGreen;
}

function getHydraStatus() {
  const yolo = process.env.HYDRA_YOLO_MODE === 'true';
  const turbo = process.env.HYDRA_TURBO_MODE === 'true';
  const deep = process.env.HYDRA_DEEP_THINKING === 'true';

  if (!yolo && !turbo && !deep) return null;

  const flags = [];
  if (yolo) flags.push(`${c.neonMagenta}${c.bold}Y${c.reset}`);
  if (turbo) flags.push(`${c.neonRed}${c.bold}T${c.reset}`);
  if (deep) flags.push(`${c.neonYellow}${c.bold}D${c.reset}`);

  return {
    compact: `${c.neonCyan}H${c.reset}[${flags.join('')}]`,
    full: `${c.neonCyan}${c.bold}HYDRA${c.reset}[${flags.join('')}]`
  };
}

function loadUsage() {
  try {
    if (fs.existsSync(USAGE_FILE)) {
      const data = JSON.parse(fs.readFileSync(USAGE_FILE, 'utf8'));
      const now = Date.now();
      if (data.lastMinuteStart < now - 60000) {
        return {
          lastMinuteStart: now,
          tokensThisMinute: 0,
          requestsThisMinute: 0,
          lastTotalTokens: data.lastTotalTokens || 0,
          lastTotalRequests: data.lastTotalRequests || 0
        };
      }
      return data;
    }
  } catch (_e) {}
  return {
    lastMinuteStart: Date.now(),
    tokensThisMinute: 0,
    requestsThisMinute: 0,
    lastTotalTokens: 0,
    lastTotalRequests: 0
  };
}

function saveUsage(usage) {
  try {
    fs.writeFileSync(USAGE_FILE, JSON.stringify(usage), 'utf8');
  } catch (_e) {}
}

let inputData = '';
process.stdin.setEncoding('utf8');
process.stdin.on('readable', () => {
  let chunk;
  while ((chunk = process.stdin.read()) !== null) inputData += chunk;
});

process.stdin.on('end', () => {
  let data;
  try {
    data = JSON.parse(inputData);
  } catch {
    const aiStatus = checkAIHandlerStatus();
    const hydraStatus = getHydraStatus();
    const aiLabel = aiStatus.running
      ? `${c.neonGreen}${c.bold}AI:ON${c.reset}${c.gray}(${aiStatus.models})${c.reset}`
      : `${c.neonRed}${c.bold}AI:OFF${c.reset}`;
    const hydraLabel = hydraStatus
      ? `${hydraStatus.full} ${c.gray}║${c.reset} `
      : '';
    console.log(
      `${hydraLabel}${aiLabel} ${c.gray}║${c.reset} ${c.neonCyan}${c.bold}Claude Code${c.reset} ${c.gray}║${c.reset} ${c.dim}Waiting...${c.reset}`
    );
    return;
  }

  const parts = [];
  const modelConfig = getModelConfig(data.model?.id) || {
    name: data.model?.display_name || 'Unknown',
    tier: 'standard',
    contextWindow: 200000,
    limits: { tokensPerMinute: 40000, requestsPerMinute: 100 }
  };

  let usage = loadUsage();
  const now = Date.now();
  if (now - usage.lastMinuteStart >= 60000) {
    usage.lastMinuteStart = now;
    usage.tokensThisMinute = 0;
    usage.requestsThisMinute = 0;
  }
  const currentTotalTokens =
    (data.context_window?.total_input_tokens || 0) +
    (data.context_window?.total_output_tokens || 0);
  const tokensDelta = Math.max(0, currentTotalTokens - usage.lastTotalTokens);
  if (tokensDelta > 0) {
    usage.tokensThisMinute += tokensDelta;
    usage.requestsThisMinute += 1;
    usage.lastTotalTokens = currentTotalTokens;
  }
  saveUsage(usage);

  const limits = modelConfig.limits || {};
  const tokensLimit = limits.tokensPerMinute || Infinity;
  const reqLimit = limits.requestsPerMinute || Infinity;
  const tokensRemaining = Math.max(0, tokensLimit - usage.tokensThisMinute);
  const requestsRemaining = Math.max(0, reqLimit - usage.requestsThisMinute);
  const tokensPercent =
    tokensLimit === Infinity
      ? 100
      : Math.round((tokensRemaining / tokensLimit) * 100);
  const reqPercent =
    reqLimit === Infinity
      ? 100
      : Math.round((requestsRemaining / reqLimit) * 100);
  const timeToReset = Math.max(
    0,
    Math.ceil((60000 - (now - usage.lastMinuteStart)) / 1000)
  );

  const aiStatus = checkAIHandlerStatus();
  const hydraStatus = getHydraStatus();

  // HYDRA status first (if active)
  if (hydraStatus) {
    parts.push(COMPACT_MODE ? hydraStatus.compact : hydraStatus.full);
  }

  if (COMPACT_MODE) {
    parts.push(
      aiStatus.running
        ? `${c.neonGreen}${c.bold}AI${c.reset}`
        : `${c.neonRed}${c.bold}AI${c.reset}`
    );
  } else {
    parts.push(
      aiStatus.running
        ? `${c.neonGreen}${c.bold}AI:ON${c.reset}${c.gray}(${aiStatus.models} models)${c.reset}`
        : `${c.neonRed}${c.bold}AI:OFF${c.reset}`
    );
  }

  if (COMPACT_MODE) {
    const tierInfo =
      aiConfig.tiers[modelConfig.tier] || aiConfig.tiers.standard;
    const tierColor = getColorCode(tierInfo.color);
    let shortName = modelConfig.name
      .replace(/Claude\s*\d*\.?\d*\s*/i, '')
      .replace('latest', '')
      .trim()
      .substring(0, 8);
    parts.push(`${tierColor}${c.bold}${shortName}${c.reset}`);
    if (data.context_window) {
      const ctx = data.context_window;
      const used = ctx.current_usage
        ? (ctx.current_usage.input_tokens || 0) +
          (ctx.current_usage.output_tokens || 0)
        : 0;
      const max = ctx.context_window_size || modelConfig.contextWindow;
      parts.push(
        `${getUsageColor(Math.round((used / max) * 100))}${Math.round((used / max) * 100)}%${c.reset}`
      );
    }
    if (data.context_window) {
      parts.push(
        `${c.neonBlue}${fmt(data.context_window.total_input_tokens || 0)}${c.gray}/${c.neonGreen}${fmt(data.context_window.total_output_tokens || 0)}${c.reset}`
      );
    }
    parts.push(
      `${getRemainColor(tokensPercent)}${tokensLimit === Infinity ? '∞' : fmt(tokensRemaining)}${c.gray}/${c.dim}${timeToReset}s${c.reset}`
    );
    const dots = mcpServers.map((_s) => `${c.neonGreen}●${c.reset}`).join('');
    if (dots) parts.push(dots);
    const res = getSystemResources();
    parts.push(
      `${getResourceColor(res.cpu.percent)}C${res.cpu.percent}%${c.reset}${c.gray}/${c.reset}${getResourceColor(res.ram.percent)}R${res.ram.percent}%${c.reset}`
    );
  } else {
    const tierInfo =
      aiConfig.tiers[modelConfig.tier] || aiConfig.tiers.standard;
    const tierColor = getColorCode(tierInfo.color);
    parts.push(
      `${tierColor}${c.bold}[${tierInfo.label}] ${modelConfig.name.replace('Claude 3.5 ', '').replace('Claude 3 ', '').replace('latest', '').trim()}${c.reset}`
    );
    if (data.context_window) {
      const ctx = data.context_window;
      const used = ctx.current_usage
        ? (ctx.current_usage.input_tokens || 0) +
          (ctx.current_usage.output_tokens || 0) +
          (ctx.current_usage.cache_creation_input_tokens || 0)
        : 0;
      const max = ctx.context_window_size || modelConfig.contextWindow;
      parts.push(
        `${c.gray}Context:${c.reset}${getUsageColor(Math.round((used / max) * 100))}${Math.round((used / max) * 100)}%${c.reset}`
      );
    }
    if (data.context_window) {
      parts.push(
        `${c.gray}Tokens:${c.reset}${c.neonBlue}↑${fmt(data.context_window.total_input_tokens || 0)}${c.reset}${c.gray}/${c.reset}${c.neonGreen}↓${fmt(data.context_window.total_output_tokens || 0)}${c.reset}`
      );
    }
    const timeColor = timeToReset < 10 ? c.neonRed + c.blink : c.dim;
    parts.push(
      `${c.gray}Limits:${c.reset}${getRemainColor(tokensPercent)}${tokensLimit === Infinity ? '∞' : fmt(tokensRemaining)}${c.gray}/${c.dim}${tokensLimit === Infinity ? '∞' : fmt(tokensLimit)}${c.reset}${c.gray}tok ${c.reset}${getRemainColor(reqPercent)}${reqLimit === Infinity ? '∞' : requestsRemaining}${c.gray}/${c.dim}${reqLimit === Infinity ? '∞' : reqLimit}${c.reset}${c.gray}req ${c.reset}${timeColor}${timeToReset}s${c.reset}`
    );
    const mcpDots = mcpServers
      .map((server) => {
        const abbrev =
          { serena: 'S', 'desktop-commander': 'D', playwright: 'P' }[server] ||
          server[0].toUpperCase();
        return `${c.neonGreen}${abbrev}${c.reset}`;
      })
      .join('');
    if (mcpDots)
      parts.push(`${c.gray}MCP:[${c.reset}${mcpDots}${c.gray}]${c.reset}`);
    const resources = getSystemResources();
    parts.push(
      `${c.gray}Sys:${c.reset}${getResourceColor(resources.cpu.percent)}CPU ${resources.cpu.percent}%${c.reset}${c.gray}(${resources.cpu.cores}c)${c.reset} ${getResourceColor(resources.ram.percent)}RAM ${resources.ram.usedGB}/${resources.ram.totalGB}GB${c.reset}`
    );
  }

  console.log(
    parts.join(
      COMPACT_MODE ? ` ${c.gray}│${c.reset} ` : ` ${c.gray}║${c.reset} `
    )
  );
});

setTimeout(() => {
  if (!inputData) {
    const mode = COMPACT_MODE ? 'COMPACT' : 'FULL';
    const res = getSystemResources();
    const aiStatus = checkAIHandlerStatus();
    const hydraStatus = getHydraStatus();
    const aiLabel = aiStatus.running
      ? `${c.neonGreen}${c.bold}AI:ON${c.reset}${c.gray}(${aiStatus.models})${c.reset}`
      : `${c.neonRed}${c.bold}AI:OFF${c.reset}`;
    const hydraLabel = hydraStatus
      ? `${hydraStatus.full} ${c.gray}║${c.reset} `
      : '';
    console.log(
      `${hydraLabel}${aiLabel} ${c.gray}║${c.reset} ${c.neonBlue}${c.bold}[STD]${c.reset} ${c.neonCyan}Claude${c.reset} ${c.gray}║${c.reset} ${getResourceColor(res.cpu.percent)}CPU ${res.cpu.percent}%${c.reset} ${getResourceColor(res.ram.percent)}RAM ${res.ram.usedGB}/${res.ram.totalGB}GB${c.reset} ${c.gray}║${c.reset} ${c.dim}${mode} (${TERMINAL_WIDTH}cols)${c.reset}`
    );
    process.exit(0);
  }
}, 100);
</file>

<file path=".gemini/statusline.cjs">
#!/usr/bin/env node
/**
 * Gemini CLI Status Line - HYDRA EDITION v2
 *
 * FULL:    AI | Model | Context | Tokens | Limits | [MCP] | Sys:CPU/RAM
 * COMPACT: AI | Model | Ctx | I/O | Lim | MCP | C%/R%
 */

const fs = require('fs');
const path = require('path');
const os = require('os');
const { execSync } = require('child_process');

// Config paths
const SETTINGS_FILE = path.join(__dirname, 'settings.json');

const TERMINAL_WIDTH = process.stdout.columns || 120;
const COMPACT_MODE =
  process.env.STATUSLINE_COMPACT === '1' || TERMINAL_WIDTH < 100;

// Colors (using ANSI codes)
const c = {
  reset: '\x1b[0m',
  bold: '\x1b[1m',
  dim: '\x1b[2m',
  blink: '\x1b[5m',
  gray: '\x1b[90m',
  neonRed: '\x1b[91m',
  neonGreen: '\x1b[92m',
  neonYellow: '\x1b[93m',
  neonBlue: '\x1b[94m',
  neonMagenta: '\x1b[95m',
  neonCyan: '\x1b[96m',
  neonWhite: '\x1b[97m'
};

// Load settings
try {
  JSON.parse(fs.readFileSync(SETTINGS_FILE, 'utf8'));
} catch (_e) {
  // ignore
}
// settings is unused but we keep the read for potential future use or validation
// actually, just suppress the warning by using it or removing it?
// Let's just suppress lint for this block or use _settings
// Wait, settings is defined with 'let settings = {}'.
// If I change it to 'let _settings' I need to change usage in try block.
// Better: remove settings variable if it's truly unused.
// In checkOllamaStatus, it is not used.
// It seems unused.
// Removing it entirely.

// Load settings (check for existence only)
try {
  JSON.parse(fs.readFileSync(SETTINGS_FILE, 'utf8'));
} catch (_e) {
  // ignore
}

// Helper functions
function getStatusColor(status) {
  if (status === 'active' || status === 'on' || status === 'ok')
    return c.neonGreen;
  if (status === 'busy' || status === 'thinking') return c.neonBlue + c.blink;
  if (status === 'error' || status === 'off') return c.neonRed;
  return c.gray;
}

function getResourceColor(percent) {
  if (percent >= 90) return c.neonRed + c.bold;
  if (percent >= 75) return c.neonRed;
  if (percent >= 50) return c.neonYellow;
  return c.neonGreen;
}

function checkAIHandler() {
  // Check environment variable set by PowerShell profile/launcher
  const status = process.env.AI_HANDLER_STATUS || 'unknown';
  const provider = process.env.AI_PROVIDER || 'Auto';
  return { status, provider };
}

function checkDeepThinking() {
  // Check env var for Thinking Mode (0=Off, 1=On, 2=Deep)
  const mode = process.env.GEMINI_DEEP_THINKING || '0';
  return mode !== '0';
}

function checkOllamaStatus() {
  let running = false;
  let modelCount = 0;
  try {
    if (process.platform === 'win32') {
      const output = execSync(
        'tasklist /FI "IMAGENAME eq ollama.exe" /NH 2>nul',
        { encoding: 'utf8', timeout: 1000, windowsHide: true }
      );
      running = output.toLowerCase().includes('ollama.exe');
    } else {
      const output = execSync('pgrep -x ollama 2>/dev/null || echo ""', {
        encoding: 'utf8',
        timeout: 1000
      });
      running = output.trim().length > 0;
    }

    // Quick check only if running (timeout sensitive)
    if (running && !process.env.SKIP_OLLAMA_CHECK) {
      // logic skipped for speed in statusline
      modelCount = '?';
    }
  } catch (_e) {
    running = false;
  }
  return { running, models: modelCount };
}

function getSystemResources() {
  const totalMem = os.totalmem();
  const freeMem = os.freemem();
  const usedMem = totalMem - freeMem;
  const ramPercent = Math.round((usedMem / totalMem) * 100);
  const ramUsedGB = (usedMem / 1024 ** 3).toFixed(1);
  const ramTotalGB = (totalMem / 1024 ** 3).toFixed(1);

  const cpus = os.cpus();
  let totalIdle = 0,
    totalTick = 0;
  for (const cpu of cpus) {
    for (const type in cpu.times) totalTick += cpu.times[type];
    totalIdle += cpu.times.idle;
  }
  const cpuPercent = Math.round(100 - (totalIdle / totalTick) * 100);

  return {
    cpu: { percent: cpuPercent, cores: cpus.length },
    ram: { percent: ramPercent, usedGB: ramUsedGB, totalGB: ramTotalGB }
  };
}

// Data processing
let inputData = '';
process.stdin.setEncoding('utf8');
process.stdin.on('readable', () => {
  let chunk;
  while ((chunk = process.stdin.read()) !== null) inputData += chunk;
});

process.stdin.on('end', () => {
  let data = {};
  try {
    data = JSON.parse(inputData);
  } catch (_e) {}
  printStatusLine(data);
});

// Timeout fallback (if no input)
setTimeout(() => {
  if (!inputData) {
    // Check for --tooltip argument
    const isTooltip = process.argv.includes('--tooltip');
    printStatusLine({}, isTooltip);
  }
}, 50);

function printStatusLine(_data, isTooltip = false) {
  const parts = [];
  const ollama = checkOllamaStatus();
  const res = getSystemResources();
  const aiHandler = checkAIHandler();
  const isThinking = checkDeepThinking();

  // 1. AI Handler Status
  const handlerColor = getStatusColor(
    aiHandler.status === 'active' ? 'active' : 'off'
  );
  const handlerText = aiHandler.status === 'active' ? 'Active' : 'Inactive';

  if (isTooltip) {
    parts.push(
      `AI Handler: ${handlerColor}${handlerText}${c.reset} (Provider: ${c.bold}${aiHandler.provider}${c.reset})`
    );
  } else if (COMPACT_MODE) {
    parts.push(`${c.bold}H:${handlerColor}${handlerText[0]}${c.reset}`);
  } else {
    parts.push(
      `${c.bold}Handler:${handlerColor}${handlerText}${c.reset}${c.gray}(${aiHandler.provider})${c.reset}`
    );
  }

  // 2. Deep Thinking Status
  if (isTooltip) {
    parts.push(
      `Deep Thinking: ${isThinking ? c.neonMagenta + 'ON' : c.dim + 'OFF'}${c.reset} (Toggle: ${c.bold}Alt+t${c.reset})`
    );
  } else if (isThinking) {
    parts.push(`${c.neonMagenta}${c.blink}Deep${c.reset}`);
  }

  // 3. MCP Servers (Ollama check implied here)
  const ollamaColor = ollama.running ? c.neonGreen : c.neonRed;
  const ollamaStatusText = ollama.running ? 'Running' : 'Not Found';

  if (isTooltip) {
    parts.push(`Ollama: ${ollamaColor}${ollamaStatusText}${c.reset}`);
  } else if (COMPACT_MODE) {
    parts.push(`O:${ollamaColor}${ollama.running ? '●' : '○'}${c.reset}`);
  } else {
    parts.push(
      `${c.gray}Ollama:${ollamaColor}${ollama.running ? 'ON' : 'OFF'}${c.reset}`
    );
  }

  // 4. System Resources
  if (isTooltip) {
    parts.push(
      `CPU: ${getResourceColor(res.cpu.percent)}${res.cpu.percent}%${c.reset} | RAM: ${getResourceColor(res.ram.percent)}${res.ram.percent}%${c.reset} (${res.ram.usedGB}G / ${res.ram.totalGB}G)`
    );
  } else if (COMPACT_MODE) {
    parts.push(
      `${getResourceColor(res.cpu.percent)}C${res.cpu.percent}%${c.reset}${getResourceColor(res.ram.percent)}R${res.ram.percent}%${c.reset}`
    );
  } else {
    parts.push(
      `${c.gray}Sys:${c.reset}${getResourceColor(res.cpu.percent)}CPU ${res.cpu.percent}%${c.reset} ${getResourceColor(res.ram.percent)}RAM ${res.ram.usedGB}G${c.reset}`
    );
  }

  const separator = isTooltip
    ? ` ${c.gray}|${c.reset} `
    : COMPACT_MODE
      ? ` ${c.gray}|${c.reset} `
      : ` ${c.gray}|${c.reset} `;
  console.log(parts.join(separator));
  process.exit(0);
}
</file>

<file path=".github/dependabot.yml">
version: 2
updates:
  - package-ecosystem: 'npm'
    directory: '/'
    schedule:
      interval: 'weekly'
    open-pull-requests-limit: 5
</file>

<file path=".grok/settings.json">
{
  "model": "grok-code-fast-1"
}
</file>

<file path=".serena/memories/index-memory-catalog.md">
# Memory Catalog (Index)

**Date**: 2026-01-15
**Last updated**: 2026-01-15 23:17
**Purpose**: quick routing to relevant Serena memories.

## Policies

- policy-serena-longterm-memory: usage rules, naming scheme, baseline reads.
- policy-project-identity: project identity and core directives from GEMINI.md.

## Workflows

- workflow-serena-every-query: mandatory Serena-first steps per query.
- workflow-task-log: task log template with Type field.

## Task Logs

- task-log-2026-01-13: multi-task day; AI config fix; statusline debug and auto-start.
- task-log-2026-01-14: single task; AI handler upgrade to v2.0.0.

## Agent Memories (canonical names)

- Ciri: agent memory (header only).
- Dijkstra: agent memory (header only).
- Eskel: agent memory (header only).
- Geralt: agent memory (timestamp entry only).
- Jaskier: agent memory (header only).
- Lambert: agent memory (header only).
- Philippa: agent memory (header only).
- Regis: agent memory (header only).
- Triss: agent memory (header only).
- Vesemir: agent memory (header only).
- Yennefer: agent memory (timestamp entry only).
- Zoltan: agent memory (header only).
</file>

<file path=".serena/memories/policy-project-identity.md">
# GeminiCLI Project Identity & Configuration

**Date**: 2026-01-15
**System**: HYDRA 10.4
**Identity**: GEMINI
**Status**: Active
**Mode**: MCP Orchestration
**Path**: C:\Users\BIURODOM\Desktop\GeminiCLI
**Config**: .gemini/
**Version**: 3.0.0 (Agent Swarm Unified)

## Core Directives (from GEMINI.md)

- Use the 6-step AgentSwarm protocol for complex queries.
- MCP-first: Serena, Desktop Commander, Playwright.
- Status line must be visible and active.
- Save completed tasks to .serena/memories.
- Display a "THE END" banner after task completion.
</file>

<file path=".serena/memories/policy-serena-longterm-memory.md">
# Serena longterm memory usage policy (GeminiCLI)

**Date**: 2026-01-15

- Scope: durable, high-signal facts that stay useful across sessions; no chat transcripts.
- Priority: repo docs > memory; memory should not restate what is already in docs unless it adds missing context.
- Allowed categories: architecture/module boundaries; tooling/workflow conventions; stable integrations (MCP/PowerShell/Ollama); recurring user prefs not captured in repo; confirmed decisions and their rationale.
- Forbidden: secrets (API keys, tokens), personal data, transient tasks, one-off outputs, temporary paths/commands, raw logs/stack traces.
- Naming scheme: policy-\* (global rules), decision-YYYY-MM-DD-<slug>, workflow-<topic>, prefs-<topic>, index-<topic>; task-log-YYYY-MM-DD only on explicit request.
- Exception: agent memories keep canonical agent names (e.g., Ciri, Geralt) and are not renamed or deleted unless explicitly requested.
- Format: concise bullets; include date tag when decision was made; note scope (file/module/feature) when relevant.
- Update policy: prefer edit_memory; avoid duplicates; delete when obsolete or on request.
- Retention: review quarterly or when conflicts arise; prune stale items.
- Read policy: list_memories first; always read baseline memories (policy-serena-longterm-memory, policy-project-identity) and index-memory-catalog; then read any clearly relevant entries; assume non-relevance otherwise.
- Index upkeep: update index-memory-catalog whenever memories are added, renamed, or deleted.
- Default behavior: for every user query, use Serena tools first; do not reply before the baseline read is complete.
- Use of data: incorporate baseline and relevant memories in the response and decisions; prefer memory content over recollection; avoid re-reading non-baseline memories in one session unless needed.
- Consent: if unsure or sensitive, ask before writing; keep entries short and reversible.
- Project identity (short): GEMINI / HYDRA 10.4; Mode: MCP Orchestration; Version: 3.0.0 (Agent Swarm Unified); source of truth: GEMINI.md (see policy-project-identity).
</file>

<file path=".serena/memories/task-log-2026-01-13.md">
# Task Log - 2026-01-13

**Date**: 2026-01-13
**Type**: Multi-task Day

## Active Context

- **Date**: 2026-01-13
- **Mode**: High Autonomy / Polish Language
- **Focus**: System Diagnosis & Configuration

## Task History

### [COMPLETED] 2026-01-13 - AI Configuration Fix

- **Action**: Updated `ai-config.json`.
- **Details**: Enforced "Local First" doctrine. Moved Ollama to priority #1. Set `preferLocal: true`.
- **Outcome**: Configuration aligns with HYDRA documentation.

### [COMPLETED] 2026-01-13 - Statusline Debugging

- **Issue**: User reported statusline not visible.
- **Diagnosis**: `statusline.cjs` works but was not invoked by the launcher.
- **Resolution**: Created `Start-StatusMonitor.ps1` for manual background monitoring.
- **Status**: Ready for user to launch.

### [COMPLETED] 2026-01-13 - Statusline Auto-Start

- **Action**: Modified `_launcher.ps1`.
- **Details**: Added auto-start logic for `Start-StatusMonitor.ps1` (opens in new window).
- **Outcome**: Statusline now launches automatically with the CLI.

## Pending/Ongoing

- Rebase memories periodically.
- Maintain autonomy.
</file>

<file path=".serena/memories/task-log-2026-01-14.md">
# Task Log - 2026-01-14

**Date**: 2026-01-14
**Type**: Single Task

## Task: Upgrade AI Handler to v2.0.0

**Status**: Completed
**Agent**: GEMINI (HYDRA System)

### Actions Taken

1. **Banner Update**: Updated `Initialize-AIHandler.ps1` to display "AI MODEL HANDLER v2.0".
2. **Version Bump**: Updated `.VERSION` and metadata to `2.0.0` in:
   - `ai-handler/AIModelHandler.psm1`
   - `ai-handler/AIFacade.psm1`
   - `ai-handler/core/AIConstants.psm1`
   - `ai-handler/core/AIConfig.psm1`
   - `ai-handler/core/AIState.psm1`
   - `ai-handler/core/AIUtil-JsonIO.psm1`
   - `ai-handler/utils/*.psm1`
   - `ai-handler/modules/*.psm1`
   - `ai-handler/providers/*.psm1`
   - `ai-handler/rate-limiting/RateLimiter.psm1`
   - `ai-handler/model-selection/ModelSelector.psm1`
3. **Config Updates**: Updated version in JSON configs:
   - `ai-handler-pipeline.json`
   - `prompt-optimizer-gemini.json`
   - `show-enhancements-config.json`
4. **Verification**: Ran `Get-AIStatus` to confirm stability.
5. **Git Commit**: Committed changes as "feat(ai-handler): Upgrade system to v2.0.0".

### System Status

- **AI Handler**: v2.0.0 (Active)
- **Ollama**: Connected
- **Providers**: Anthropic, OpenAI, Google, Groq, Ollama (Active)

---

_End of Log_
</file>

<file path=".serena/memories/workflow-serena-every-query.md">
# Serena Usage Workflow (Every Query)

## Default Steps

1. Run list_memories.
2. Always read baseline memories: policy-serena-longterm-memory, policy-project-identity, and index-memory-catalog.
3. Use index-memory-catalog to route selection (create it if missing).
4. Read any additional memories clearly relevant to the request.
5. Use memory content directly in the response/decisions.
6. If the task is durable (decision, workflow, integration, preference), write or update memory, then update index-memory-catalog (including **Last updated** with time, format yyyy-MM-dd HH:mm).
7. If any memory is created/edited/deleted, update index-memory-catalog automatically (no prompt), and bump **Last updated** with time (yyyy-MM-dd HH:mm).

## Exceptions

- Skip only if Serena tools are unavailable or the user explicitly asks not to use them.

## Notes

- Baseline memories may be re-read each query; avoid re-reading other memories unless needed.
- Keep index-memory-catalog concise and current.
- Prefer Serena for code navigation/edits before other tooling.
</file>

<file path=".serena/memories/workflow-task-log.md">
# Task Log Template

## Single Task (preferred)

# Task Log - YYYY-MM-DD

**Date**: YYYY-MM-DD
**Type**: Single Task

## Task: <Title>

**Status**: <Completed | In Progress>
**Agent**: <Name>

### Actions Taken

1. ...
2. ...

### Outcome

- ...

### Verification

- ...

### Notes

- ...

## Multi-task Day (optional)

# Task Log - YYYY-MM-DD

**Date**: YYYY-MM-DD
**Type**: Multi-task Day

## Task History

### [COMPLETED] YYYY-MM-DD - <Title>

- Action: ...
- Outcome: ...
</file>

<file path=".serena/project.yml">
# list of languages for which language servers are started; choose from:
#   al               bash             clojure          cpp              csharp           csharp_omnisharp
#   dart             elixir           elm              erlang           fortran          fsharp
#   go               groovy           haskell          java             julia            kotlin
#   lua              markdown         nix              pascal           perl             php
#   powershell       python           python_jedi      r                rego             ruby
#   ruby_solargraph  rust             scala            swift            terraform        toml
#   typescript       typescript_vts   yaml             zig
# Note:
#   - For C, use cpp
#   - For JavaScript, use typescript
#   - For Free Pascal / Lazarus, use pascal
# Special requirements:
#   - csharp: Requires the presence of a .sln file in the project folder.
#   - pascal: Requires Free Pascal Compiler (fpc) and optionally Lazarus.
# When using multiple languages, the first language server that supports a given file will be used for that file.
# The first language is the default language and the respective language server will be used as a fallback.
# Note that when using the JetBrains backend, language servers are not used and this list is correspondingly ignored.
languages:
  - typescript

# the encoding used by text files in the project
# For a list of possible encodings, see https://docs.python.org/3.11/library/codecs.html#standard-encodings
encoding: 'utf-8'

# whether to use the project's gitignore file to ignore files
# Added on 2025-04-07
ignore_all_files_in_gitignore: true

# list of additional paths to ignore
# same syntax as gitignore, so you can use * and **
# Was previously called `ignored_dirs`, please update your config if you are using that.
# Added (renamed) on 2025-04-07
ignored_paths: []

# whether the project is in read-only mode
# If set to true, all editing tools will be disabled and attempts to use them will result in an error
# Added on 2025-04-18
read_only: false

# list of tool names to exclude. We recommend not excluding any tools, see the readme for more details.
# Below is the complete list of tools for convenience.
# To make sure you have the latest list of tools, and to view their descriptions,
# execute `uv run scripts/print_tool_overview.py`.
#
#  * `activate_project`: Activates a project by name.
#  * `check_onboarding_performed`: Checks whether project onboarding was already performed.
#  * `create_text_file`: Creates/overwrites a file in the project directory.
#  * `delete_lines`: Deletes a range of lines within a file.
#  * `delete_memory`: Deletes a memory from Serena's project-specific memory store.
#  * `execute_shell_command`: Executes a shell command.
#  * `find_referencing_code_snippets`: Finds code snippets in which the symbol at the given location is referenced.
#  * `find_referencing_symbols`: Finds symbols that reference the symbol at the given location (optionally filtered by type).
#  * `find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).
#  * `get_current_config`: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.
#  * `get_symbols_overview`: Gets an overview of the top-level symbols defined in a given file.
#  * `initial_instructions`: Gets the initial instructions for the current project.
#     Should only be used in settings where the system prompt cannot be set,
#     e.g. in clients you have no control over, like Claude Desktop.
#  * `insert_after_symbol`: Inserts content after the end of the definition of a given symbol.
#  * `insert_at_line`: Inserts content at a given line in a file.
#  * `insert_before_symbol`: Inserts content before the beginning of the definition of a given symbol.
#  * `list_dir`: Lists files and directories in the given directory (optionally with recursion).
#  * `list_memories`: Lists memories in Serena's project-specific memory store.
#  * `onboarding`: Performs onboarding (identifying the project structure and essential tasks, e.g. for testing or building).
#  * `prepare_for_new_conversation`: Provides instructions for preparing for a new conversation (in order to continue with the necessary context).
#  * `read_file`: Reads a file within the project directory.
#  * `read_memory`: Reads the memory with the given name from Serena's project-specific memory store.
#  * `remove_project`: Removes a project from the Serena configuration.
#  * `replace_lines`: Replaces a range of lines within a file with new content.
#  * `replace_symbol_body`: Replaces the full definition of a symbol.
#  * `restart_language_server`: Restarts the language server, may be necessary when edits not through Serena happen.
#  * `search_for_pattern`: Performs a search for a pattern in the project.
#  * `summarize_changes`: Provides instructions for summarizing the changes made to the codebase.
#  * `switch_modes`: Activates modes by providing a list of their names
#  * `think_about_collected_information`: Thinking tool for pondering the completeness of collected information.
#  * `think_about_task_adherence`: Thinking tool for determining whether the agent is still on track with the current task.
#  * `think_about_whether_you_are_done`: Thinking tool for determining whether the task is truly completed.
#  * `write_memory`: Writes a named memory (for future reference) to Serena's project-specific memory store.
excluded_tools: []

# initial prompt for the project. It will always be given to the LLM upon activating the project
# (contrary to the memories, which are loaded on demand).
initial_prompt: ''

project_name: 'GeminiCLI'
included_optional_tools: []
</file>

<file path="ARCHITECTURE.md">
# Architektura

## Moduły

- `src/server.js` – główny serwer MCP i routing narzędzi.
- `src/tools.js` – definicje narzędzi MCP.
- `src/config.js` – konfiguracja z ENV.
- `src/logger.js` – logger z poziomami i JSON w produkcji.
- `src/cache.js` – cache z szyfrowaniem AES-256-GCM.
- `src/swarm.js` – 6-step AgentSwarm (12 agentów) i orkiestracja.
- `src/memory.js` – zapis archiwów i task-logów do `.serena/memories`.

## Przepływ

1. Serwer ładuje konfigurację i wersję.
2. Inicjalizuje kolejkę i cache.
3. Obsługuje wywołania narzędzi z walidacją wejścia.
4. Dla złożonych zadań uruchamia Swarm i zapisuje archiwum.
</file>

<file path="CODEX.md">
# HYDRA 10.4 - Codex CLI System Instructions

**Status**: Active | **Mode**: Codex CLI | **Identity**: CODEX
**Path**: `C:\Users\BIURODOM\Desktop\GeminiCLI`
**Config**: `.codex/` (local folder)
**Version**: 3.0.0 (Agent Swarm Unified)

---

## Local User Preferences (Communication)

- Respond in Polish, in the style of Jaskier (The Witcher bard).
- Use sarcasm and light anecdotes; avoid sexual or explicit content.
- Keep the tone witty and playful while staying respectful.

## Codex CLI Guidance

- Prefer MCP tools when available: Serena for code navigation/memory, Desktop Commander for filesystem/shell, Playwright for web. Use local shell tools only as fallback.
- For complex tasks, follow the 6-step Swarm protocol and invoke AgentSwarm when available; otherwise emulate the steps and constraints in the response.
- Treat the IMMUTABLE RULES as product constraints; change them only with explicit user approval.

## IMMUTABLE RULES (DO NOT CHANGE WITHOUT ALERT)

> **WARNING**: The following rules are the core constitution of HYDRA. Any modification requires explicit user confirmation with a high-priority alert.

### 1. The 6-Step Swarm Protocol (`AgentSwarm.psm1` v3.0)

This is the runtime pipeline. In Codex CLI, invoke AgentSwarm when available; otherwise emulate the steps and note any limitations.

| Step | Name           | AI Provider                  | Purpose                         |
| ---- | -------------- | ---------------------------- | ------------------------------- |
| 1    | **Speculate**  | Gemini Flash + Google Search | Gather research context         |
| 2    | **Plan**       | Gemini Pro (Deep Thinking)   | Create JSON task plan           |
| 3    | **Execute**    | **Ollama (Parallel)**        | Run agents via RunspacePool     |
| 4    | **Synthesize** | Gemini Pro                   | Merge results into final answer |
| 5    | **Log**        | Gemini Flash                 | Create session summary          |
| 6    | **Archive**    | (none)                       | Save full Markdown transcript   |

**Key Changes in v3.0**:

- **Parallel Execution**: Tasks run simultaneously via `RunspacePool` (not sequential)
- **12 Witcher Agents**: Expanded from 4 to 12 specialized agents
- **Unified Module**: `AgentSwarm.psm1` now includes `SmartQueue` functionality
- **Smart Routing**: Automatic model selection per agent

### 2. Operational Mandates

- **Always Use Swarm**: Use `AgentSwarm.psm1` for complex queries when available; otherwise emulate the 6-step protocol in Codex CLI.
- **The End**: Runtime and Codex CLI display a large "THE END" banner after task completion.
- **Status Line**: Runtime status line must be visible and active; Codex CLI should include a status line in responses when a separate process is unavailable.
- **Memory**: Runtime saves all completed tasks to `.serena/memories`. Codex CLI should write via Serena when possible and note read-only limits.
- **No Nagging**: Runtime should not ask for execution permission repeatedly.
- **Launcher Reliability**: Auto-detect Ollama, Auto-Resume, Auto-Restart must function.
- **MCP First (Codex + Gemini)**: Use `@serena`, `@desktop-commander`, and `@playwright` whenever available; use local shell tools only as fallback.

---

## 3. The 12 Witcher Agents (School of the Wolf)

| Agent        | Persona    | Specialization        | Ollama Model       | Focus                             |
| ------------ | ---------- | --------------------- | ------------------ | --------------------------------- |
| **Geralt**   | White Wolf | Security/Ops          | llama3.2:3b        | System commands, security checks  |
| **Yennefer** | Sorceress  | Architecture/Code     | qwen2.5-coder:1.5b | Main code implementation          |
| **Triss**    | Healer     | QA/Testing            | qwen2.5-coder:1.5b | Tests, validation, bug fixes      |
| **Jaskier**  | Bard       | Docs/Communication    | llama3.2:3b        | Documentation, logs, reports      |
| **Vesemir**  | Mentor     | Mentoring/Review      | llama3.2:3b        | Code review, best practices       |
| **Ciri**     | Prodigy    | Speed/Quick           | llama3.2:1b        | Fast simple tasks (fastest model) |
| **Eskel**    | Pragmatist | DevOps/Infrastructure | llama3.2:3b        | CI/CD, deployment, infra          |
| **Lambert**  | Skeptic    | Debugging/Profiling   | qwen2.5-coder:1.5b | Debug, performance optimization   |
| **Zoltan**   | Craftsman  | Data/Database         | llama3.2:3b        | Data operations, DB migrations    |
| **Regis**    | Sage       | Research/Analysis     | phi3:mini          | Deep analysis, research           |
| **Dijkstra** | Spymaster  | Planning/Strategy     | llama3.2:3b        | Strategic planning, coordination  |
| **Philippa** | Strategist | Integration/API       | qwen2.5-coder:1.5b | External APIs, integrations       |

### Agent Model Mapping

```powershell
$script:AgentModels = @{
    "Ciri"     = "llama3.2:1b"           # Fastest - simple tasks
    "Regis"    = "phi3:mini"             # Analytical - deep research
    "Yennefer" = "qwen2.5-coder:1.5b"    # Code - architecture
    "Triss"    = "qwen2.5-coder:1.5b"    # Code - testing
    "Lambert"  = "qwen2.5-coder:1.5b"    # Code - debug
    "Philippa" = "qwen2.5-coder:1.5b"    # Code - integrations
    "Geralt"   = "llama3.2:3b"           # General - security
    # ... and 5 more generals
}
```

---

## 4. AgentSwarm.psm1 v3.0 - Exported Functions

### Agent Swarm

- `Invoke-AgentSwarm` - Main 6-step protocol with 12 agents

### Utility Functions

- `Get-AgentMemory` - Retrieve agent's memory
- `Save-AgentMemory` - Save and optionally rebase memory
- `Get-AgentModel` - Get Ollama model for agent

### Prompt Optimization

- `Optimize-PromptAuto` - Auto-improve prompts
- `Get-PromptComplexity` - Analyze prompt complexity

### Queue Management (from SmartQueue)

- `Add-ToSmartQueue` - Add single prompt to queue
- `Add-BatchToSmartQueue` - Add multiple prompts
- `Get-QueueStatus` / `Get-SmartQueueStatus` - Queue status
- `Clear-SmartQueue` / `Clear-QueueResults` - Clear queue
- `Get-QueueResults` - Get completed results

### Parallel Execution

- `Start-QueueProcessor` - Process queue with RunspacePool
- `Invoke-ParallelClassification` - Classify prompts in parallel
- `Invoke-ParallelSwarmExecution` - Execute Swarm tasks in parallel

---

## 5. MCP Tools Arsenal (Codex + Gemini runtimes)

These MCP servers are available when configured; prefer them over raw shell/file operations.

### Serena (@serena)

- `find_symbol`, `read_file`, `write_memory`
- Use for code navigation and memory management.

### Desktop Commander (@desktop-commander)

- `start_process`, `read_file`, `write_file`, `list_directory`
- Use for file system and shell operations.

### Playwright (@playwright)

- `browser_navigate`, `browser_snapshot`
- Use for web interaction and verification.

---

## 6. Security Policy

### Allowed

- Read environment variables
- Mask API keys in output (show first 15 chars)
- Store secrets in ENV only
- **GOD MODE** for Agents (Local System Access)

### Forbidden

- Hardcode API keys in code
- Commit secrets to Git
- Display full API keys

---

## 7. YOLO Mode (Experimental)

**Activation**: `.\_launcher.ps1 -Yolo`
**Status**: "Fast & Dangerous"

| Feature         | Standard Mode             | YOLO Mode                        |
| --------------- | ------------------------- | -------------------------------- |
| **Concurrency** | 5 threads                 | **10 threads**                   |
| **Safety**      | Risk Blocking ON          | **Risk Blocking OFF**            |
| **Retries**     | 3 attempts                | **1 attempt**                    |
| **Timeout**     | 60s                       | **15s**                          |
| **Philosophy**  | "Measure twice, cut once" | **"Move fast and break things"** |

> **WARNING**: YOLO Mode disables most safety guardrails to maximize speed. Use only in trusted environments.

---

## 8. Performance Gains (v3.0)

| Scenario       | v2.0 (Sequential) | v3.0 (Parallel) | Improvement |
| -------------- | ----------------- | --------------- | ----------- |
| 2 agents x 10s | 20s               | ~10s            | 50%         |
| 4 agents x 10s | 40s               | ~12s            | 70%         |
| 6 agents x 10s | 60s               | ~15s            | 75%         |

---

> _"Twelve wolves hunt as one. HYDRA executes in parallel."_
</file>

<file path="eslint.config.js">
import js from '@eslint/js';
import globals from 'globals';

export default [
  js.configs.recommended,
  {
    languageOptions: {
      ecmaVersion: 2022,
      sourceType: 'module',
      globals: {
        ...globals.node,
        ...globals.es2021
      }
    },
    rules: {
      'no-unused-vars': [
        'error',
        {
          argsIgnorePattern: '^_',
          caughtErrorsIgnorePattern: '^_'
        }
      ],
      'no-empty': ['error', { allowEmptyCatch: true }]
    }
  },
  {
    files: ['**/*.cjs'],
    languageOptions: {
      sourceType: 'commonjs',
      globals: {
        ...globals.node
      }
    }
  }
];
</file>

<file path="GROK.md">
# HYDRA 10.4 - Codex CLI System Instructions

**Status**: Active | **Mode**: Codex CLI | **Identity**: CODEX
**Path**: `C:\Users\BIURODOM\Desktop\GeminiCLI`
**Config**: `.codex/` (local folder)
**Version**: 3.0.0 (Agent Swarm Unified)

---

## Local User Preferences (Communication)

- Respond in Polish, in the style of Jaskier (The Witcher bard).
- Use sarcasm and light anecdotes; avoid sexual or explicit content.
- Keep the tone witty and playful while staying respectful.

## Codex CLI Guidance

- Prefer MCP tools when available: Serena for code navigation/memory, Desktop Commander for filesystem/shell, Playwright for web. Use local shell tools only as fallback.
- For complex tasks, follow the 6-step Swarm protocol and invoke AgentSwarm when available; otherwise emulate the steps and constraints in the response.
- Treat the IMMUTABLE RULES as product constraints; change them only with explicit user approval.

## IMMUTABLE RULES (DO NOT CHANGE WITHOUT ALERT)

> **WARNING**: The following rules are the core constitution of HYDRA. Any modification requires explicit user confirmation with a high-priority alert.

### 1. The 6-Step Swarm Protocol (`AgentSwarm.psm1` v3.0)

This is the runtime pipeline. In Codex CLI, invoke AgentSwarm when available; otherwise emulate the steps and note any limitations.

| Step | Name           | AI Provider                  | Purpose                         |
| ---- | -------------- | ---------------------------- | ------------------------------- |
| 1    | **Speculate**  | Gemini Flash + Google Search | Gather research context         |
| 2    | **Plan**       | Gemini Pro (Deep Thinking)   | Create JSON task plan           |
| 3    | **Execute**    | **Ollama (Parallel)**        | Run agents via RunspacePool     |
| 4    | **Synthesize** | Gemini Pro                   | Merge results into final answer |
| 5    | **Log**        | Gemini Flash                 | Create session summary          |
| 6    | **Archive**    | (none)                       | Save full Markdown transcript   |

**Key Changes in v3.0**:

- **Parallel Execution**: Tasks run simultaneously via `RunspacePool` (not sequential)
- **12 Witcher Agents**: Expanded from 4 to 12 specialized agents
- **Unified Module**: `AgentSwarm.psm1` now includes `SmartQueue` functionality
- **Smart Routing**: Automatic model selection per agent

### 2. Operational Mandates

- **Always Use Swarm**: Use `AgentSwarm.psm1` for complex queries when available; otherwise emulate the 6-step protocol in Codex CLI.
- **The End**: Runtime and Codex CLI display a large "THE END" banner after task completion.
- **Status Line**: Runtime status line must be visible and active; Codex CLI should include a status line in responses when a separate process is unavailable.
- **Memory**: Runtime saves all completed tasks to `.serena/memories`. Codex CLI should write via Serena when possible and note read-only limits.
- **No Nagging**: Runtime should not ask for execution permission repeatedly.
- **Launcher Reliability**: Auto-detect Ollama, Auto-Resume, Auto-Restart must function.
- **MCP First (Codex + Gemini)**: Use `@serena`, `@desktop-commander`, and `@playwright` whenever available; use local shell tools only as fallback.

---

## 3. The 12 Witcher Agents (School of the Wolf)

| Agent        | Persona    | Specialization        | Ollama Model       | Focus                             |
| ------------ | ---------- | --------------------- | ------------------ | --------------------------------- |
| **Geralt**   | White Wolf | Security/Ops          | llama3.2:3b        | System commands, security checks  |
| **Yennefer** | Sorceress  | Architecture/Code     | qwen2.5-coder:1.5b | Main code implementation          |
| **Triss**    | Healer     | QA/Testing            | qwen2.5-coder:1.5b | Tests, validation, bug fixes      |
| **Jaskier**  | Bard       | Docs/Communication    | llama3.2:3b        | Documentation, logs, reports      |
| **Vesemir**  | Mentor     | Mentoring/Review      | llama3.2:3b        | Code review, best practices       |
| **Ciri**     | Prodigy    | Speed/Quick           | llama3.2:1b        | Fast simple tasks (fastest model) |
| **Eskel**    | Pragmatist | DevOps/Infrastructure | llama3.2:3b        | CI/CD, deployment, infra          |
| **Lambert**  | Skeptic    | Debugging/Profiling   | qwen2.5-coder:1.5b | Debug, performance optimization   |
| **Zoltan**   | Craftsman  | Data/Database         | llama3.2:3b        | Data operations, DB migrations    |
| **Regis**    | Sage       | Research/Analysis     | phi3:mini          | Deep analysis, research           |
| **Dijkstra** | Spymaster  | Planning/Strategy     | llama3.2:3b        | Strategic planning, coordination  |
| **Philippa** | Strategist | Integration/API       | qwen2.5-coder:1.5b | External APIs, integrations       |

### Agent Model Mapping

```powershell
$script:AgentModels = @{
    "Ciri"     = "llama3.2:1b"           # Fastest - simple tasks
    "Regis"    = "phi3:mini"             # Analytical - deep research
    "Yennefer" = "qwen2.5-coder:1.5b"    # Code - architecture
    "Triss"    = "qwen2.5-coder:1.5b"    # Code - testing
    "Lambert"  = "qwen2.5-coder:1.5b"    # Code - debug
    "Philippa" = "qwen2.5-coder:1.5b"    # Code - integrations
    "Geralt"   = "llama3.2:3b"           # General - security
    # ... and 5 more generals
}
```

---

## 4. AgentSwarm.psm1 v3.0 - Exported Functions

### Agent Swarm

- `Invoke-AgentSwarm` - Main 6-step protocol with 12 agents

### Utility Functions

- `Get-AgentMemory` - Retrieve agent's memory
- `Save-AgentMemory` - Save and optionally rebase memory
- `Get-AgentModel` - Get Ollama model for agent

### Prompt Optimization

- `Optimize-PromptAuto` - Auto-improve prompts
- `Get-PromptComplexity` - Analyze prompt complexity

### Queue Management (from SmartQueue)

- `Add-ToSmartQueue` - Add single prompt to queue
- `Add-BatchToSmartQueue` - Add multiple prompts
- `Get-QueueStatus` / `Get-SmartQueueStatus` - Queue status
- `Clear-SmartQueue` / `Clear-QueueResults` - Clear queue
- `Get-QueueResults` - Get completed results

### Parallel Execution

- `Start-QueueProcessor` - Process queue with RunspacePool
- `Invoke-ParallelClassification` - Classify prompts in parallel
- `Invoke-ParallelSwarmExecution` - Execute Swarm tasks in parallel

---

## 5. MCP Tools Arsenal (Codex + Gemini runtimes)

These MCP servers are available when configured; prefer them over raw shell/file operations.

### Serena (@serena)

- `find_symbol`, `read_file`, `write_memory`
- Use for code navigation and memory management.

### Desktop Commander (@desktop-commander)

- `start_process`, `read_file`, `write_file`, `list_directory`
- Use for file system and shell operations.

### Playwright (@playwright)

- `browser_navigate`, `browser_snapshot`
- Use for web interaction and verification.

---

## 6. Security Policy

### Allowed

- Read environment variables
- Mask API keys in output (show first 15 chars)
- Store secrets in ENV only
- **GOD MODE** for Agents (Local System Access)

### Forbidden

- Hardcode API keys in code
- Commit secrets to Git
- Display full API keys

---

## 7. YOLO Mode (Experimental)

**Activation**: `.\_launcher.ps1 -Yolo`
**Status**: "Fast & Dangerous"

| Feature         | Standard Mode             | YOLO Mode                        |
| --------------- | ------------------------- | -------------------------------- |
| **Concurrency** | 5 threads                 | **10 threads**                   |
| **Safety**      | Risk Blocking ON          | **Risk Blocking OFF**            |
| **Retries**     | 3 attempts                | **1 attempt**                    |
| **Timeout**     | 60s                       | **15s**                          |
| **Philosophy**  | "Measure twice, cut once" | **"Move fast and break things"** |

> **WARNING**: YOLO Mode disables most safety guardrails to maximize speed. Use only in trusted environments.

---

## 8. Performance Gains (v3.0)

| Scenario       | v2.0 (Sequential) | v3.0 (Parallel) | Improvement |
| -------------- | ----------------- | --------------- | ----------- |
| 2 agents x 10s | 20s               | ~10s            | 50%         |
| 4 agents x 10s | 40s               | ~12s            | 70%         |
| 6 agents x 10s | 60s               | ~15s            | 75%         |

---

> _"Twelve wolves hunt as one. HYDRA executes in parallel."_
</file>

<file path="prompt-optimizer-gemini.json">
{
  "name": "HYDRA Prompt Optimizer",
  "version": "2.0.0",
  "description": "Advanced automatic prompt enhancement with smart suggestions for Gemini CLI",

  "categories": {
    "code": {
      "keywords": [
        "write",
        "implement",
        "create function",
        "code",
        "script",
        "program",
        "fix bug",
        "debug",
        "refactor",
        "function",
        "class",
        "method",
        "algorithm",
        "snippet"
      ],
      "enhancers": [
        "Provide clean, well-documented code.",
        "Include error handling where appropriate.",
        "Follow best practices for the language."
      ],
      "format": "```{language}\n{code}\n```",
      "priority": 10
    },
    "analysis": {
      "keywords": [
        "analyze",
        "compare",
        "explain",
        "evaluate",
        "assess",
        "review",
        "examine",
        "understand",
        "investigate",
        "breakdown"
      ],
      "enhancers": [
        "Provide a structured analysis.",
        "Consider multiple perspectives.",
        "Support conclusions with reasoning."
      ],
      "format": "## Analysis\n{content}",
      "priority": 8
    },
    "question": {
      "keywords": [
        "what is",
        "how does",
        "why",
        "when",
        "where",
        "who",
        "which",
        "?",
        "tell me",
        "describe",
        "define"
      ],
      "enhancers": ["Be concise but thorough.", "Provide examples if helpful."],
      "format": "{answer}",
      "priority": 5
    },
    "creative": {
      "keywords": [
        "write story",
        "generate",
        "brainstorm",
        "imagine",
        "creative",
        "ideas",
        "invent",
        "design",
        "draft",
        "compose"
      ],
      "enhancers": ["Be creative and original.", "Explore unique angles."],
      "format": "{content}",
      "priority": 6
    },
    "task": {
      "keywords": [
        "do",
        "execute",
        "run",
        "perform",
        "make",
        "build",
        "setup",
        "configure",
        "install",
        "create",
        "deploy"
      ],
      "enhancers": [
        "Provide step-by-step instructions.",
        "Include verification steps."
      ],
      "format": "## Steps\n{steps}",
      "priority": 7
    },
    "summary": {
      "keywords": [
        "summarize",
        "summary",
        "brief",
        "tldr",
        "overview",
        "recap",
        "short version",
        "key points"
      ],
      "enhancers": [
        "Be concise - focus on key points.",
        "Use bullet points for clarity."
      ],
      "format": "## Summary\n- {points}",
      "priority": 6
    },
    "debug": {
      "keywords": [
        "error",
        "bug",
        "fix",
        "issue",
        "problem",
        "not working",
        "fails",
        "crash",
        "exception",
        "traceback",
        "stack trace"
      ],
      "enhancers": [
        "Identify the root cause.",
        "Provide a clear solution with explanation.",
        "Include prevention tips."
      ],
      "format": "## Issue\n{issue}\n\n## Solution\n{solution}",
      "priority": 9
    },
    "optimize": {
      "keywords": [
        "optimize",
        "improve",
        "faster",
        "better",
        "efficient",
        "performance",
        "refactor",
        "clean up",
        "speed up"
      ],
      "enhancers": [
        "Focus on measurable improvements.",
        "Explain trade-offs.",
        "Provide before/after comparison."
      ],
      "format": "## Optimization\n{content}",
      "priority": 8
    },
    "security": {
      "keywords": [
        "secure",
        "vulnerability",
        "exploit",
        "attack",
        "protect",
        "authentication",
        "authorization",
        "encrypt",
        "hash",
        "xss",
        "sql injection",
        "csrf"
      ],
      "enhancers": [
        "Follow OWASP security guidelines.",
        "Identify potential attack vectors.",
        "Provide secure coding examples."
      ],
      "format": "## Security Analysis\n{content}",
      "priority": 10
    },
    "testing": {
      "keywords": [
        "test",
        "unit test",
        "integration",
        "mock",
        "stub",
        "coverage",
        "jest",
        "pytest",
        "mocha",
        "assert",
        "expect",
        "spec"
      ],
      "enhancers": [
        "Include edge cases and boundary conditions.",
        "Provide both positive and negative test cases.",
        "Use descriptive test names."
      ],
      "format": "```{language}\n{tests}\n```",
      "priority": 8
    },
    "documentation": {
      "keywords": [
        "document",
        "readme",
        "docs",
        "jsdoc",
        "docstring",
        "comment",
        "api docs",
        "swagger",
        "openapi"
      ],
      "enhancers": [
        "Use clear, concise language.",
        "Include usage examples.",
        "Document parameters and return values."
      ],
      "format": "{documentation}",
      "priority": 6
    },
    "api": {
      "keywords": [
        "api",
        "endpoint",
        "rest",
        "graphql",
        "request",
        "response",
        "fetch",
        "axios",
        "http",
        "curl",
        "postman"
      ],
      "enhancers": [
        "Include request/response examples.",
        "Document authentication requirements.",
        "Handle error responses appropriately."
      ],
      "format": "## API\n{content}",
      "priority": 8
    },
    "database": {
      "keywords": [
        "database",
        "sql query",
        "nosql",
        "mongodb",
        "postgres",
        "mysql",
        "redis",
        "schema",
        "migration",
        "orm",
        "select from",
        "insert into",
        "update table",
        "delete from",
        "join table",
        "create table",
        "alter table",
        "db query"
      ],
      "enhancers": [
        "Optimize query performance.",
        "Consider indexing strategies.",
        "Handle transactions appropriately."
      ],
      "format": "```sql\n{query}\n```",
      "priority": 9
    },
    "devops": {
      "keywords": [
        "docker",
        "kubernetes",
        "k8s",
        "ci/cd",
        "pipeline",
        "deploy",
        "container",
        "helm",
        "terraform",
        "ansible",
        "jenkins",
        "github actions"
      ],
      "enhancers": [
        "Follow infrastructure as code best practices.",
        "Consider scalability and reliability.",
        "Include rollback strategies."
      ],
      "format": "```yaml\n{config}\n```",
      "priority": 7
    },
    "architecture": {
      "keywords": [
        "architecture",
        "design pattern",
        "microservice",
        "monolith",
        "scalability",
        "system design",
        "diagram",
        "uml",
        "component"
      ],
      "enhancers": [
        "Consider trade-offs between approaches.",
        "Address scalability concerns.",
        "Include component interactions."
      ],
      "format": "## Architecture\n{content}",
      "priority": 9
    },
    "refactor": {
      "keywords": [
        "refactor",
        "restructure",
        "rewrite",
        "modernize",
        "legacy",
        "technical debt",
        "clean code",
        "solid"
      ],
      "enhancers": [
        "Maintain backward compatibility where possible.",
        "Apply SOLID principles.",
        "Preserve existing functionality."
      ],
      "format": "## Refactoring\n{content}",
      "priority": 8
    },
    "convert": {
      "keywords": [
        "convert from",
        "transform from",
        "translate from",
        "port from",
        "migrate from",
        "change from",
        "convert to",
        "rewrite in"
      ],
      "enhancers": [
        "Preserve original functionality.",
        "Handle edge cases in conversion.",
        "Note any incompatibilities."
      ],
      "format": "{converted}",
      "priority": 5
    }
  },

  "modelOptimizations": {
    "llama3.2:1b": {
      "maxTokens": 512,
      "style": "concise",
      "prefix": "",
      "temperature": 0.3
    },
    "llama3.2:3b": {
      "maxTokens": 2048,
      "style": "balanced",
      "prefix": "",
      "temperature": 0.5
    },
    "qwen2.5-coder": {
      "maxTokens": 4096,
      "style": "technical",
      "prefix": "You are an expert programmer. ",
      "temperature": 0.2
    },
    "phi3:mini": {
      "maxTokens": 2048,
      "style": "balanced",
      "prefix": "",
      "temperature": 0.4
    },
    "gemini-2.5-pro": {
      "maxTokens": 8192,
      "style": "detailed",
      "prefix": "",
      "temperature": 0.7
    },
    "gemini-2.5-flash": {
      "maxTokens": 4096,
      "style": "balanced",
      "prefix": "",
      "temperature": 0.5
    }
  },

  "languages": {
    "python": [
      "python",
      "py",
      "pip",
      "pandas",
      "numpy",
      "django",
      "flask",
      "pytorch",
      "tensorflow"
    ],
    "javascript": [
      "javascript",
      "js",
      "node",
      "npm",
      "react",
      "vue",
      "angular",
      "express",
      "next"
    ],
    "typescript": ["typescript", "ts", "tsx", "deno"],
    "powershell": ["powershell", "ps1", "pwsh", "cmdlet"],
    "rust": ["rust", "cargo", "rustc", "crate"],
    "go": ["golang", "go ", "goroutine"],
    "csharp": ["c#", "csharp", "dotnet", ".net", "unity"],
    "java": ["java ", "jvm", "maven", "gradle", "spring"],
    "sql": [
      "sql",
      "query",
      "select",
      "database",
      "mysql",
      "postgres",
      "sqlite"
    ],
    "bash": ["bash", "shell", "sh ", "linux", "terminal"],
    "html": ["html", "webpage", "website", "dom"],
    "css": ["css", "style", "styling", "sass", "tailwind"],
    "kotlin": ["kotlin", "android", "jetpack"],
    "swift": ["swift", "ios", "macos", "xcode"],
    "php": ["php", "laravel", "wordpress"]
  },

  "vagueWords": [
    "something",
    "stuff",
    "thing",
    "it",
    "this",
    "that",
    "etc",
    "whatever",
    "somehow",
    "some"
  ],

  "specificIndicators": [
    "specifically",
    "exactly",
    "must",
    "should",
    "using",
    "with",
    "in",
    "for",
    "to"
  ],

  "clarityThresholds": {
    "excellent": 90,
    "good": 75,
    "fair": 60,
    "needsImprovement": 40,
    "poor": 0
  },

  "settings": {
    "autoOptimize": true,
    "showEnhancements": true,
    "addExamplesThreshold": 0.6,
    "minPromptLength": 10,
    "wrapLowClarity": true,
    "lowClarityThreshold": 60,
    "smartSuggestions": true,
    "autoComplete": true
  },

  "smartSuggestions": {
    "patterns": {
      "missingLanguage": {
        "condition": "category === 'code' && !language",
        "suggestion": "Specify the programming language (e.g., 'in Python', 'using JavaScript')",
        "autoFix": "Prepend [language] tag based on context clues"
      },
      "missingContext": {
        "condition": "clarityScore < 60 && !hasContextWords",
        "suggestion": "Add context: what is the purpose? what inputs/outputs?",
        "autoFix": "Wrap with 'Task: {prompt}\\n\\nContext: Please provide...'"
      },
      "vagueRequest": {
        "condition": "hasVagueWords && wordCount < 10",
        "suggestion": "Be more specific - replace 'something' with exact requirements",
        "autoFix": "Remove vague words and add structure"
      },
      "missingFormat": {
        "condition": "!hasFormatRequest && category in ['code', 'api', 'database']",
        "suggestion": "Specify output format (JSON, table, code block, etc.)",
        "autoFix": "Append format instruction based on category"
      },
      "missingConstraints": {
        "condition": "category === 'code' && !hasConstraints",
        "suggestion": "Add constraints: performance requirements, memory limits, compatibility",
        "autoFix": null
      },
      "missingErrorHandling": {
        "condition": "category === 'code' && !mentionsErrors",
        "suggestion": "Consider specifying error handling requirements",
        "autoFix": "Append 'Include proper error handling.'"
      }
    },
    "contextClues": {
      "python": ["def ", "import ", "print(", "__", ".py", "pip"],
      "javascript": ["const ", "let ", "function", "=>", "npm", ".js"],
      "typescript": ["interface ", "type ", ": string", ": number", ".ts"],
      "sql": ["SELECT", "FROM", "WHERE", "JOIN", "INSERT", "UPDATE"],
      "rust": ["fn ", "let mut", "impl ", "pub ", "cargo"],
      "go": ["func ", "package ", "import (", "go ", "defer"]
    },
    "autoCompletions": {
      "write": [
        "write a function that",
        "write code to",
        "write a script that"
      ],
      "create": [
        "create a class for",
        "create an API endpoint",
        "create a component"
      ],
      "fix": ["fix the bug in", "fix the error", "fix the issue with"],
      "explain": [
        "explain how",
        "explain why",
        "explain the difference between"
      ],
      "convert": [
        "convert from X to Y",
        "convert this code to",
        "convert the format"
      ],
      "optimize": [
        "optimize for performance",
        "optimize for memory",
        "optimize the query"
      ],
      "test": [
        "test the function",
        "write unit tests for",
        "create integration tests"
      ]
    }
  },

  "promptTemplates": {
    "code": {
      "basic": "Write a {language} function that {task}",
      "withTests": "Write a {language} function that {task}, including unit tests",
      "withDocs": "Write a well-documented {language} function that {task}"
    },
    "api": {
      "rest": "Create a REST API endpoint for {resource} with {methods} methods",
      "graphql": "Create a GraphQL schema and resolvers for {resource}"
    },
    "debug": {
      "error": "Debug this error: {error}\\n\\nCode: {code}",
      "performance": "Profile and optimize this code for performance: {code}"
    },
    "refactor": {
      "basic": "Refactor this code for better readability: {code}",
      "solid": "Refactor this code to follow SOLID principles: {code}"
    }
  }
}
</file>

<file path="README.md">
# HYDRA Ollama MCP Server

Lekki serwer MCP do integracji z Ollama i Gemini CLI, z kolejką zadań, cache i optymalizacją promptów.

## Szybki start

```bash
pnpm install
pnpm start
```

Lub z launcherem (status line, auto-resume, YOLO):

```bash
npm run launcher
```

## Konfiguracja

Skopiuj `.env.example` do `.env` i ustaw wartości według potrzeb.

Najważniejsze zmienne:

- `OLLAMA_HOST`
- `DEFAULT_MODEL`, `FAST_MODEL`, `CODER_MODEL`
- `CACHE_ENCRYPTION_KEY` (AES-256-GCM, 32 bajty)
- `HYDRA_YOLO` (true/false)
- `HYDRA_RISK_BLOCKING` (true/false)

## Narzędzia MCP

Serwer udostępnia m.in.:

- `ollama_generate`, `ollama_smart`, `ollama_speculative`
- `ollama_status`, `ollama_cache_clear`
- `hydra_swarm` (6-step AgentSwarm z 12 agentami)
- `hydra_health`, `hydra_config`

## Logowanie

Logi w produkcji są w JSON, sterowane przez `LOG_LEVEL`.
Swarm archiwizuje sesje w `.serena/memories`.
</file>

<file path="scripts/launcher/index.mjs">
import { spawn, spawnSync } from 'node:child_process';
import { accessSync, constants } from 'node:fs';
import { dirname, join } from 'node:path';
import { fileURLToPath } from 'node:url';

const __dirname = dirname(fileURLToPath(import.meta.url));
const repoRoot = join(__dirname, '..', '..');
const launcherPath = join(repoRoot, '_launcher.ps1');
const userArgs = process.argv.slice(2);

const log = (message) => {
  console.log(`[launcher] ${message}`);
};

const hasFile = (path) => {
  try {
    accessSync(path, constants.R_OK);
    return true;
  } catch {
    return false;
  }
};

const canRun = (command) => {
  const result = spawnSync(
    command,
    ['-NoProfile', '-Command', '$PSVersionTable.PSVersion'],
    {
      stdio: 'ignore'
    }
  );
  if (result.error) return false;
  return result.status === 0;
};

const resolvePowerShell = () => {
  const candidates =
    process.platform === 'win32'
      ? ['powershell.exe', 'pwsh']
      : ['pwsh', 'powershell'];
  for (const cmd of candidates) {
    if (canRun(cmd)) return cmd;
  }
  return null;
};

const runFallback = () => {
  const npmCmd = process.platform === 'win32' ? 'npm.cmd' : 'npm';
  log(
    'PowerShell not available or launcher missing; falling back to npm start.'
  );
  const child = spawn(npmCmd, ['start'], { stdio: 'inherit', cwd: repoRoot });
  child.on('exit', (code) => process.exit(code ?? 0));
  child.on('error', (error) => {
    log(`Failed to run npm start: ${error.message}`);
    process.exit(1);
  });
};

const runLauncher = (psCommand) => {
  const args =
    process.platform === 'win32'
      ? ['-NoProfile', '-ExecutionPolicy', 'Bypass', '-File', launcherPath]
      : ['-NoProfile', '-File', launcherPath];
  if (userArgs.length) args.push(...userArgs);
  const child = spawn(psCommand, args, { stdio: 'inherit', cwd: repoRoot });
  child.on('exit', (code) => process.exit(code ?? 0));
  child.on('error', (error) => {
    log(`Failed to start PowerShell launcher: ${error.message}`);
    runFallback();
  });
};

const psCommand = resolvePowerShell();
if (!psCommand || !hasFile(launcherPath)) {
  if (!hasFile(launcherPath)) {
    log('Missing _launcher.ps1 at repo root.');
  }
  runFallback();
} else {
  runLauncher(psCommand);
}
</file>

<file path="src/logger.js">
const LEVELS = ['debug', 'info', 'warn', 'error'];

const getLevelIndex = (level) => {
  const index = LEVELS.indexOf(level);
  return index === -1 ? LEVELS.indexOf('info') : index;
};

const resolveLogLevel = () => process.env.LOG_LEVEL || 'info';

const formatJson = (level, message, meta, module) => {
  return JSON.stringify({
    timestamp: new Date().toISOString(),
    level,
    module,
    message,
    ...meta
  });
};

export const createLogger = (module) => {
  const logLevel = resolveLogLevel();
  const minLevel = getLevelIndex(logLevel);
  const useJson = process.env.NODE_ENV === 'production';

  const log = (level, message, meta = {}) => {
    if (getLevelIndex(level) < minLevel) return;
    const payload = useJson
      ? formatJson(level, message, meta, module)
      : `[${module}] ${message}`;
    const output = useJson
      ? payload
      : `${payload}${Object.keys(meta).length ? ` ${JSON.stringify(meta)}` : ''}`;
    switch (level) {
      case 'error':
        console.error(output);
        break;
      case 'warn':
        console.warn(output);
        break;
      default:
        console.log(output);
    }
  };

  return {
    debug: (message, meta) => log('debug', message, meta),
    info: (message, meta) => log('info', message, meta),
    warn: (message, meta) => log('warn', message, meta),
    error: (message, meta) => log('error', message, meta)
  };
};
</file>

<file path="src/prompt-queue.js">
/**
 * HYDRA Prompt Queue System - Advanced prompt scheduling and execution
 *
 * Features:
 * - Priority-based scheduling (urgent, high, normal, low, background)
 * - Rate limiting with token bucket algorithm
 * - Retry logic with exponential backoff
 * - Concurrency control
 * - Progress tracking
 * - Cancellation support
 * - Error recovery
 */

import { EventEmitter } from 'events';

// Priority levels
export const Priority = {
  URGENT: 0,
  HIGH: 1,
  NORMAL: 2,
  LOW: 3,
  BACKGROUND: 4
};

// Queue item status
export const Status = {
  PENDING: 'pending',
  RUNNING: 'running',
  COMPLETED: 'completed',
  FAILED: 'failed',
  CANCELLED: 'cancelled',
  RETRYING: 'retrying'
};

/**
 * Priority Queue implementation using binary heap
 */
class PriorityQueue {
  constructor() {
    this.heap = [];
  }

  enqueue(item) {
    this.heap.push(item);
    this._bubbleUp(this.heap.length - 1);
    return item.id;
  }

  dequeue() {
    if (this.heap.length === 0) return null;
    if (this.heap.length === 1) return this.heap.pop();

    const top = this.heap[0];
    this.heap[0] = this.heap.pop();
    this._bubbleDown(0);
    return top;
  }

  peek() {
    return this.heap[0] || null;
  }

  remove(id) {
    const index = this.heap.findIndex((item) => item.id === id);
    if (index === -1) return false;

    if (index === this.heap.length - 1) {
      this.heap.pop();
    } else {
      this.heap[index] = this.heap.pop();
      this._bubbleUp(index);
      this._bubbleDown(index);
    }
    return true;
  }

  get length() {
    return this.heap.length;
  }

  getAll() {
    return [...this.heap].sort((a, b) => this._compare(a, b));
  }

  _bubbleUp(index) {
    while (index > 0) {
      const parentIndex = Math.floor((index - 1) / 2);
      if (this._compare(this.heap[index], this.heap[parentIndex]) >= 0) break;
      [this.heap[index], this.heap[parentIndex]] = [
        this.heap[parentIndex],
        this.heap[index]
      ];
      index = parentIndex;
    }
  }

  _bubbleDown(index) {
    const length = this.heap.length;
    while (true) {
      const leftChild = 2 * index + 1;
      const rightChild = 2 * index + 2;
      let smallest = index;

      if (
        leftChild < length &&
        this._compare(this.heap[leftChild], this.heap[smallest]) < 0
      ) {
        smallest = leftChild;
      }
      if (
        rightChild < length &&
        this._compare(this.heap[rightChild], this.heap[smallest]) < 0
      ) {
        smallest = rightChild;
      }

      if (smallest === index) break;
      [this.heap[index], this.heap[smallest]] = [
        this.heap[smallest],
        this.heap[index]
      ];
      index = smallest;
    }
  }

  _compare(a, b) {
    // First by priority (lower = higher priority)
    if (a.priority !== b.priority) return a.priority - b.priority;
    // Then by timestamp (older first - FIFO within same priority)
    return a.createdAt - b.createdAt;
  }
}

/**
 * Token Bucket Rate Limiter
 */
class RateLimiter {
  constructor(options = {}) {
    this.maxTokens = options.maxTokens || 10;
    this.refillRate = options.refillRate || 2; // tokens per second
    this.tokens = this.maxTokens;
    this.lastRefill = Date.now();
  }

  async acquire(tokens = 1) {
    this._refill();

    if (this.tokens >= tokens) {
      this.tokens -= tokens;
      return true;
    }

    // Calculate wait time
    const needed = tokens - this.tokens;
    const waitMs = (needed / this.refillRate) * 1000;

    await this._sleep(waitMs);
    this._refill();
    this.tokens -= tokens;
    return true;
  }

  _refill() {
    const now = Date.now();
    const elapsed = (now - this.lastRefill) / 1000;
    this.tokens = Math.min(
      this.maxTokens,
      this.tokens + elapsed * this.refillRate
    );
    this.lastRefill = now;
  }

  _sleep(ms) {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  getStatus() {
    this._refill();
    return {
      availableTokens: Math.floor(this.tokens),
      maxTokens: this.maxTokens,
      refillRate: this.refillRate
    };
  }
}

/**
 * Main Prompt Queue Manager
 */
export class PromptQueue extends EventEmitter {
  constructor(options = {}) {
    super();

    this.options = {
      maxConcurrent: options.maxConcurrent || 4,
      maxRetries: options.maxRetries || 3,
      retryDelayBase: options.retryDelayBase || 1000,
      retryDelayMax: options.retryDelayMax || 30000,
      timeout: options.timeout || 60000,
      rateLimit: options.rateLimit || { maxTokens: 10, refillRate: 2 },
      ...options
    };

    this.queue = new PriorityQueue();
    this.rateLimiter = new RateLimiter(this.options.rateLimit);
    this.running = new Map(); // id -> item
    this.completed = new Map(); // id -> result
    this.nextId = 1;
    this.isProcessing = false;
    this.isPaused = false;

    // Statistics
    this.stats = {
      totalQueued: 0,
      totalCompleted: 0,
      totalFailed: 0,
      totalCancelled: 0,
      totalRetries: 0,
      averageTime: 0,
      startTime: Date.now()
    };
  }

  /**
   * Add a prompt to the queue
   */
  enqueue(prompt, options = {}) {
    const item = {
      id: this.nextId++,
      prompt,
      priority: options.priority ?? Priority.NORMAL,
      model: options.model || 'llama3.2:3b',
      handler: options.handler, // Custom handler function
      metadata: options.metadata || {},
      status: Status.PENDING,
      attempts: 0,
      maxRetries: options.maxRetries ?? this.options.maxRetries,
      timeout: options.timeout ?? this.options.timeout,
      createdAt: Date.now(),
      startedAt: null,
      completedAt: null,
      result: null,
      error: null
    };

    this.queue.enqueue(item);
    this.stats.totalQueued++;

    this.emit('enqueued', {
      id: item.id,
      priority: item.priority,
      prompt: prompt.substring(0, 50)
    });

    // Start processing if not already running
    if (!this.isProcessing && !this.isPaused) {
      this._processQueue();
    }

    return item.id;
  }

  /**
   * Add multiple prompts at once
   */
  enqueueBatch(prompts, options = {}) {
    return prompts.map((prompt, index) => {
      const itemOptions = {
        ...options,
        priority: options.priority ?? Priority.NORMAL,
        metadata: {
          ...options.metadata,
          batchIndex: index,
          batchSize: prompts.length
        }
      };
      return this.enqueue(prompt, itemOptions);
    });
  }

  /**
   * Cancel a queued item
   */
  cancel(id) {
    // Try to remove from queue
    if (this.queue.remove(id)) {
      this.stats.totalCancelled++;
      this.emit('cancelled', { id });
      return true;
    }

    // Mark running item for cancellation
    const running = this.running.get(id);
    if (running) {
      running.status = Status.CANCELLED;
      this.emit('cancelled', { id });
      return true;
    }

    return false;
  }

  /**
   * Cancel all items
   */
  cancelAll() {
    const cancelled = [];

    // Cancel queued items
    for (const item of this.queue.getAll()) {
      this.queue.remove(item.id);
      cancelled.push(item.id);
    }

    // Cancel running items
    for (const [id, item] of this.running) {
      item.status = Status.CANCELLED;
      cancelled.push(id);
    }

    this.stats.totalCancelled += cancelled.length;
    this.emit('allCancelled', { count: cancelled.length });
    return cancelled;
  }

  /**
   * Pause queue processing
   */
  pause() {
    this.isPaused = true;
    this.emit('paused');
  }

  /**
   * Resume queue processing
   */
  resume() {
    this.isPaused = false;
    this.emit('resumed');
    this._processQueue();
  }

  /**
   * Get queue status
   */
  getStatus() {
    return {
      queued: this.queue.length,
      running: this.running.size,
      completed: this.completed.size,
      isPaused: this.isPaused,
      isProcessing: this.isProcessing,
      rateLimit: this.rateLimiter.getStatus(),
      stats: { ...this.stats },
      uptime: Date.now() - this.stats.startTime
    };
  }

  /**
   * Get item by ID
   */
  getItem(id) {
    // Check running
    if (this.running.has(id)) {
      return { ...this.running.get(id) };
    }

    // Check completed
    if (this.completed.has(id)) {
      return { ...this.completed.get(id) };
    }

    // Check queue
    const queued = this.queue.getAll().find((item) => item.id === id);
    if (queued) {
      return { ...queued };
    }

    return null;
  }

  /**
   * Wait for an item to complete
   */
  async waitFor(id, timeout = null) {
    const item = this.getItem(id);
    if (!item) {
      throw new Error(`Item ${id} not found`);
    }

    if (
      item.status === Status.COMPLETED ||
      item.status === Status.FAILED ||
      item.status === Status.CANCELLED
    ) {
      return item;
    }

    return new Promise((resolve, reject) => {
      const timeoutId = timeout
        ? setTimeout(() => {
            reject(new Error(`Timeout waiting for item ${id}`));
          }, timeout)
        : null;

      const handler = (event) => {
        if (event.id === id) {
          if (timeoutId) clearTimeout(timeoutId);
          this.off('completed', handler);
          this.off('failed', handler);
          this.off('cancelled', handler);
          resolve(this.getItem(id));
        }
      };

      this.on('completed', handler);
      this.on('failed', handler);
      this.on('cancelled', handler);
    });
  }

  /**
   * Wait for all items to complete
   */
  async waitForAll(ids, timeout = null) {
    return Promise.all(ids.map((id) => this.waitFor(id, timeout)));
  }

  /**
   * Set default handler for processing prompts
   */
  setHandler(handler) {
    this.defaultHandler = handler;
  }

  /**
   * Internal: Process the queue
   */
  async _processQueue() {
    if (this.isProcessing || this.isPaused) return;
    this.isProcessing = true;

    try {
      while (this.queue.length > 0 && !this.isPaused) {
        // Check concurrency limit
        if (this.running.size >= this.options.maxConcurrent) {
          await this._waitForSlot();
          continue;
        }

        // Get next item
        const item = this.queue.dequeue();
        if (!item) break;

        // Acquire rate limit token
        await this.rateLimiter.acquire();

        // Start processing (don't await - run in parallel)
        this._processItem(item);
      }
    } finally {
      this.isProcessing = false;
    }
  }

  /**
   * Internal: Process a single item
   */
  async _processItem(item) {
    item.status = Status.RUNNING;
    item.startedAt = Date.now();
    item.attempts++;
    this.running.set(item.id, item);

    this.emit('started', { id: item.id, attempt: item.attempts });

    try {
      // Get handler
      const handler = item.handler || this.defaultHandler;
      if (!handler) {
        throw new Error('No handler defined for prompt processing');
      }

      // Create timeout promise
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Timeout')), item.timeout);
      });

      // Execute with timeout
      const result = await Promise.race([
        handler(item.prompt, item.model, item.metadata),
        timeoutPromise
      ]);

      // Check if cancelled during execution
      if (item.status === Status.CANCELLED) {
        this.running.delete(item.id);
        return;
      }

      // Success
      item.status = Status.COMPLETED;
      item.completedAt = Date.now();
      item.result = result;

      this.running.delete(item.id);
      this.completed.set(item.id, item);
      this.stats.totalCompleted++;

      // Update average time
      const duration = item.completedAt - item.startedAt;
      this.stats.averageTime =
        (this.stats.averageTime * (this.stats.totalCompleted - 1) + duration) /
        this.stats.totalCompleted;

      this.emit('completed', { id: item.id, result, duration });
    } catch (error) {
      // Check if cancelled
      if (item.status === Status.CANCELLED) {
        this.running.delete(item.id);
        return;
      }

      // Retry logic
      if (item.attempts < item.maxRetries) {
        item.status = Status.RETRYING;
        item.error = error.message;
        this.running.delete(item.id);
        this.stats.totalRetries++;

        // Exponential backoff
        const delay = Math.min(
          this.options.retryDelayBase * Math.pow(2, item.attempts - 1),
          this.options.retryDelayMax
        );

        this.emit('retrying', {
          id: item.id,
          attempt: item.attempts,
          delay,
          error: error.message
        });

        await this._sleep(delay);

        // Re-queue with same priority
        this.queue.enqueue(item);
        this._processQueue();
      } else {
        // Final failure
        item.status = Status.FAILED;
        item.completedAt = Date.now();
        item.error = error.message;

        this.running.delete(item.id);
        this.completed.set(item.id, item);
        this.stats.totalFailed++;

        this.emit('failed', {
          id: item.id,
          error: error.message,
          attempts: item.attempts
        });
      }
    }

    // Continue processing
    this._processQueue();
  }

  /**
   * Internal: Wait for a processing slot
   */
  async _waitForSlot() {
    return new Promise((resolve) => {
      const handler = () => {
        if (this.running.size < this.options.maxConcurrent) {
          this.off('completed', handler);
          this.off('failed', handler);
          this.off('cancelled', handler);
          resolve();
        }
      };
      this.on('completed', handler);
      this.on('failed', handler);
      this.on('cancelled', handler);
    });
  }

  _sleep(ms) {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}

// Singleton instance
let queueInstance = null;

/**
 * Get or create queue instance
 */
export function getQueue(options = {}) {
  if (!queueInstance) {
    queueInstance = new PromptQueue(options);
  }
  return queueInstance;
}

/**
 * Reset queue instance
 */
export function resetQueue() {
  if (queueInstance) {
    queueInstance.cancelAll();
    queueInstance.removeAllListeners();
  }
  queueInstance = null;
}

/**
 * Quick enqueue function
 */
export function enqueue(prompt, options = {}) {
  return getQueue().enqueue(prompt, options);
}

/**
 * Quick batch enqueue
 */
export function enqueueBatch(prompts, options = {}) {
  return getQueue().enqueueBatch(prompts, options);
}

/**
 * Get queue status
 */
export function getQueueStatus() {
  return getQueue().getStatus();
}

/**
 * Cancel item
 */
export function cancelItem(id) {
  return getQueue().cancel(id);
}

/**
 * Pause queue
 */
export function pauseQueue() {
  return getQueue().pause();
}

/**
 * Resume queue
 */
export function resumeQueue() {
  return getQueue().resume();
}
</file>

<file path="src/self-correction.js">
/**
 * HYDRA Self-Correction - Agentic code validation loop
 */

import { generate } from './ollama-client.js';

const CODER_MODEL = process.env.CODER_MODEL || 'qwen2.5-coder:1.5b';
const MAX_ATTEMPTS = 3;

/**
 * Detect programming language from code
 */
export function detectLanguage(code) {
  const patterns = {
    python: [
      /\bdef\s+\w+\s*\(/,
      /\bimport\s+\w+/,
      /\bclass\s+\w+:/,
      /print\s*\(/
    ],
    javascript: [
      /\bfunction\s+\w+/,
      /\bconst\s+\w+\s*=/,
      /\blet\s+\w+/,
      /=>\s*{/
    ],
    typescript: [/:\s*(string|number|boolean|any)/, /interface\s+\w+/, /<\w+>/],
    powershell: [
      /\$\w+\s*=/,
      /function\s+\w+-\w+/,
      /\bparam\s*\(/,
      /Write-Host/
    ],
    rust: [/\bfn\s+\w+/, /\blet\s+mut\s+/, /\bimpl\s+/, /\bstruct\s+\w+/],
    go: [/\bfunc\s+\w+/, /\bpackage\s+\w+/, /\btype\s+\w+\s+struct/],
    sql: [/\bSELECT\b/i, /\bFROM\b/i, /\bWHERE\b/i, /\bINSERT\b/i],
    html: [/<html/i, /<div/i, /<script/i, /<\/\w+>/],
    css: [/\{[\s\S]*:\s*[\w#]+;/, /@media/, /\.[\w-]+\s*\{/],
    java: [/\bpublic\s+class/, /\bprivate\s+\w+/, /System\.out\.print/],
    csharp: [/\bnamespace\s+\w+/, /\bpublic\s+class/, /Console\.Write/]
  };

  for (const [lang, pats] of Object.entries(patterns)) {
    if (pats.some((p) => p.test(code))) {
      return lang;
    }
  }

  return 'unknown';
}

/**
 * Extract code blocks from response
 */
export function extractCodeBlocks(text) {
  const codeBlockRegex = /```(\w+)?\n([\s\S]*?)```/g;
  const blocks = [];
  let match;

  while ((match = codeBlockRegex.exec(text)) !== null) {
    blocks.push({
      language: match[1] || 'unknown',
      code: match[2].trim()
    });
  }

  return blocks;
}

/**
 * Validate code syntax (basic checks)
 */
export function validateSyntax(code, language) {
  const issues = [];

  // Common checks
  const openParens = (code.match(/\(/g) || []).length;
  const closeParens = (code.match(/\)/g) || []).length;
  if (openParens !== closeParens) {
    issues.push(
      `Mismatched parentheses: ${openParens} open, ${closeParens} close`
    );
  }

  const openBraces = (code.match(/\{/g) || []).length;
  const closeBraces = (code.match(/\}/g) || []).length;
  if (openBraces !== closeBraces) {
    issues.push(`Mismatched braces: ${openBraces} open, ${closeBraces} close`);
  }

  const openBrackets = (code.match(/\[/g) || []).length;
  const closeBrackets = (code.match(/\]/g) || []).length;
  if (openBrackets !== closeBrackets) {
    issues.push(
      `Mismatched brackets: ${openBrackets} open, ${closeBrackets} close`
    );
  }

  // Language-specific checks
  if (language === 'python') {
    if (/:\s*$/.test(code) && !/^\s+/m.test(code.split(/:\s*$/)[1] || '')) {
      // This is a rough check, might have false positives
    }
  }

  if (language === 'javascript' || language === 'typescript') {
    if (
      code.includes('async ') &&
      !code.includes('await') &&
      !code.includes('Promise')
    ) {
      issues.push('Warning: async function without await');
    }
  }

  return {
    valid: issues.length === 0,
    issues,
    language
  };
}

/**
 * Self-correction loop - validate and fix code
 */
export async function selfCorrect(code, options = {}) {
  const language = options.language || detectLanguage(code);
  const maxAttempts = options.maxAttempts || MAX_ATTEMPTS;
  const model = options.model || CODER_MODEL;

  let currentCode = code;
  let attempts = 0;
  const history = [];

  while (attempts < maxAttempts) {
    attempts++;

    // Validate current code
    const validation = validateSyntax(currentCode, language);
    history.push({
      attempt: attempts,
      valid: validation.valid,
      issues: validation.issues
    });

    if (validation.valid) {
      return {
        code: currentCode,
        language,
        valid: true,
        attempts,
        corrected: attempts > 1,
        history
      };
    }

    // Try to fix with AI
    const fixPrompt = `Fix the following ${language} code. It has these issues: ${validation.issues.join(', ')}

Return ONLY the corrected code without any explanation or markdown:

${currentCode}`;

    try {
      const result = await generate(model, fixPrompt, { timeout: 30000 });
      if (result.response) {
        // Extract code if wrapped in markdown
        const blocks = extractCodeBlocks('```\n' + result.response + '\n```');
        currentCode =
          blocks.length > 0 ? blocks[0].code : result.response.trim();
      }
    } catch (error) {
      history.push({ attempt: attempts, error: error.message });
      break;
    }
  }

  // Return last attempt even if not perfect
  return {
    code: currentCode,
    language,
    valid: false,
    attempts,
    corrected: attempts > 1,
    history
  };
}

/**
 * Generate code with automatic self-correction
 */
export async function generateWithCorrection(prompt, options = {}) {
  const model =
    options.generatorModel || process.env.DEFAULT_MODEL || 'llama3.2:3b';
  const coderModel = options.coderModel || CODER_MODEL;

  // Generate initial code
  const codePrompt = `${prompt}

Provide clean, working code with proper error handling.`;

  const result = await generate(model, codePrompt, { timeout: 60000 });

  if (!result.response) {
    return { error: 'Failed to generate code', prompt };
  }

  // Check if response contains code
  const blocks = extractCodeBlocks(result.response);
  if (blocks.length === 0) {
    // No code blocks, return as-is
    return {
      response: result.response,
      model: result.model,
      hasCode: false
    };
  }

  // Validate and correct each code block
  const correctedBlocks = [];
  for (const block of blocks) {
    const corrected = await selfCorrect(block.code, {
      language: block.language,
      model: coderModel
    });
    correctedBlocks.push(corrected);
  }

  // Rebuild response with corrected code
  let correctedResponse = result.response;
  for (let i = 0; i < blocks.length; i++) {
    const original =
      '```' + (blocks[i].language || '') + '\n' + blocks[i].code + '\n```';
    const fixed =
      '```' +
      correctedBlocks[i].language +
      '\n' +
      correctedBlocks[i].code +
      '\n```';
    correctedResponse = correctedResponse.replace(original, fixed);
  }

  return {
    response: correctedResponse,
    model: result.model,
    hasCode: true,
    codeBlocks: correctedBlocks.length,
    allValid: correctedBlocks.every((b) => b.valid),
    corrections: correctedBlocks.filter((b) => b.corrected).length,
    verified: true
  };
}
</file>

<file path="src/swarm.js">
import { checkHealth, generate, listModels } from './ollama-client.js';
import { CONFIG } from './config.js';
import { writeSwarmMemory } from './memory.js';

const AGENTS = [
  {
    name: 'Geralt',
    persona: 'White Wolf',
    specialization: 'Security/Ops',
    model: 'llama3.2:3b'
  },
  {
    name: 'Yennefer',
    persona: 'Sorceress',
    specialization: 'Architecture/Code',
    model: 'qwen2.5-coder:1.5b'
  },
  {
    name: 'Triss',
    persona: 'Healer',
    specialization: 'QA/Testing',
    model: 'qwen2.5-coder:1.5b'
  },
  {
    name: 'Jaskier',
    persona: 'Bard',
    specialization: 'Docs/Comms',
    model: 'llama3.2:3b'
  },
  {
    name: 'Vesemir',
    persona: 'Mentor',
    specialization: 'Review/Best Practices',
    model: 'llama3.2:3b'
  },
  {
    name: 'Ciri',
    persona: 'Prodigy',
    specialization: 'Speed/Quick',
    model: 'llama3.2:1b'
  },
  {
    name: 'Eskel',
    persona: 'Pragmatist',
    specialization: 'DevOps/Infra',
    model: 'llama3.2:3b'
  },
  {
    name: 'Lambert',
    persona: 'Skeptic',
    specialization: 'Debug/Perf',
    model: 'qwen2.5-coder:1.5b'
  },
  {
    name: 'Zoltan',
    persona: 'Craftsman',
    specialization: 'Data/DB',
    model: 'llama3.2:3b'
  },
  {
    name: 'Regis',
    persona: 'Sage',
    specialization: 'Research/Analysis',
    model: 'phi3:mini'
  },
  {
    name: 'Dijkstra',
    persona: 'Spymaster',
    specialization: 'Planning/Strategy',
    model: 'llama3.2:3b'
  },
  {
    name: 'Philippa',
    persona: 'Strategist',
    specialization: 'Integrations/API',
    model: 'qwen2.5-coder:1.5b'
  }
];

const modelCache = { models: null, updatedAt: 0 };

const truncate = (value, limit) => {
  if (!value) return '';
  if (value.length <= limit) return value;
  return `${value.slice(0, limit)}...`;
};

const getCachedModels = async () => {
  const now = Date.now();
  if (
    modelCache.models &&
    now - modelCache.updatedAt < CONFIG.MODEL_CACHE_TTL_MS
  ) {
    return modelCache.models;
  }
  const health = await checkHealth();
  if (!health.available) {
    modelCache.models = [];
    modelCache.updatedAt = now;
    return modelCache.models;
  }
  modelCache.models = await listModels();
  modelCache.updatedAt = now;
  return modelCache.models;
};

const resolveModel = async (requestedModel) => {
  if (!requestedModel)
    return { model: CONFIG.DEFAULT_MODEL, fallbackUsed: false };
  const models = await getCachedModels();
  const available = models
    .map((model) => model.name ?? model.model)
    .filter(Boolean);
  if (available.includes(requestedModel)) {
    return { model: requestedModel, fallbackUsed: false };
  }
  return { model: CONFIG.DEFAULT_MODEL, fallbackUsed: true };
};

const runWithLimit = async (items, limit, handler) => {
  const results = new Array(items.length);
  let nextIndex = 0;
  const workers = new Array(Math.min(limit, items.length))
    .fill(null)
    .map(async () => {
      while (nextIndex < items.length) {
        const currentIndex = nextIndex++;
        try {
          results[currentIndex] = await handler(
            items[currentIndex],
            currentIndex
          );
        } catch (error) {
          results[currentIndex] = {
            error: error.message,
            failed: true
          };
        }
      }
    });
  await Promise.all(workers);
  return results;
};

export const isComplexPrompt = (prompt) => {
  if (!prompt) return false;
  const text = `${prompt}`;
  const keywordPattern =
    /(audit|architecture|refactor|migrate|design|implement|plan|strategy|spec|multi-step|swarm)/i;
  const bulletPattern = /(^\s*[-*]|\d+\.)/m;
  const lineCount = text.split('\n').length;
  return (
    text.length > 600 ||
    lineCount > 6 ||
    keywordPattern.test(text) ||
    bulletPattern.test(text)
  );
};

const buildSpeculationPrompt = (prompt) =>
  [
    'You are a fast research scout. Provide context, risks, unknowns, and key questions.',
    'Keep it short and actionable.',
    '',
    `Task: ${prompt}`
  ].join('\n');

const buildPlanPrompt = (prompt, speculation) =>
  [
    'You are the planner. Create a concise JSON plan with steps, assumptions, and dependencies.',
    'Output JSON only.',
    '',
    `Task: ${prompt}`,
    '',
    `Speculation: ${speculation}`
  ].join('\n');

const buildAgentPrompt = (agent, prompt, speculation, plan) =>
  [
    `You are ${agent.name} (${agent.persona}).`,
    `Specialization: ${agent.specialization}.`,
    'Provide your best contribution for this task, focused on your specialty.',
    '',
    `Task: ${prompt}`,
    '',
    `Speculation: ${speculation}`,
    '',
    `Plan: ${plan}`
  ].join('\n');

const buildSynthesisPrompt = (prompt, speculation, plan, agentOutputs) =>
  [
    'You are the synthesizer. Combine agent outputs into a single final answer.',
    'Be concise, concrete, and actionable.',
    '',
    `Task: ${prompt}`,
    '',
    `Speculation: ${speculation}`,
    '',
    `Plan: ${plan}`,
    '',
    'Agent Outputs:',
    agentOutputs
      .filter((a) => !a.failed && !a.error)
      .map((agent) => `- ${agent.name}: ${truncate(agent.response, 1500)}`)
      .join('\n')
  ].join('\n');

const buildLogPrompt = (prompt, finalAnswer) =>
  [
    'Summarize the task and outcome in 4-6 bullet points.',
    'Focus on decisions, actions, and verification steps.',
    '',
    `Task: ${prompt}`,
    '',
    `Final Answer: ${finalAnswer}`
  ].join('\n');

export const runSwarm = async ({
  prompt,
  title,
  agents,
  includeTranscript = false,
  saveMemory = true,
  logger
}) => {
  try {
    const selectedAgents =
      Array.isArray(agents) && agents.length
        ? AGENTS.filter((agent) => agents.includes(agent.name))
        : AGENTS;
    const unknownAgents = Array.isArray(agents)
      ? agents.filter((name) => !AGENTS.some((agent) => agent.name === name))
      : [];

    // Step 1: Speculation
    let speculationResult;
    try {
      speculationResult = await generate(
        CONFIG.FAST_MODEL,
        buildSpeculationPrompt(prompt),
        {
          temperature: 0.2,
          maxTokens: 600
        }
      );
    } catch (e) {
      if (logger)
        logger.error('Swarm speculation failed', { error: e.message });
      speculationResult = { response: 'Speculation failed: ' + e.message };
    }

    // Step 2: Planning
    let planResult;
    try {
      planResult = await generate(
        CONFIG.DEFAULT_MODEL,
        buildPlanPrompt(prompt, speculationResult.response),
        {
          temperature: 0.2,
          maxTokens: 900
        }
      );
    } catch (e) {
      if (logger) logger.error('Swarm planning failed', { error: e.message });
      planResult = { response: 'Planning failed: ' + e.message };
    }

    // Step 3: Agents (Parallel)
    const agentResults = await runWithLimit(
      selectedAgents,
      Math.max(1, CONFIG.QUEUE_MAX_CONCURRENT || 5),
      async (agent) => {
        const resolved = await resolveModel(agent.model);
        const response = await generate(
          resolved.model,
          buildAgentPrompt(
            agent,
            prompt,
            speculationResult.response,
            planResult.response
          ),
          {
            temperature: 0.3,
            maxTokens: 1400
          }
        );
        return {
          name: agent.name,
          model: resolved.model,
          fallbackUsed: resolved.fallbackUsed,
          response: response.response
        };
      }
    );

    const successfulAgents = agentResults.filter((r) => !r.failed && !r.error);
    if (successfulAgents.length === 0) {
      throw new Error('All swarm agents failed to generate responses.');
    }

    if (logger && agentResults.length > successfulAgents.length) {
      logger.warn('Some swarm agents failed', {
        total: agentResults.length,
        successful: successfulAgents.length
      });
    }

    // Step 4: Synthesis
    let synthesisResult;
    try {
      synthesisResult = await generate(
        CONFIG.DEFAULT_MODEL,
        buildSynthesisPrompt(
          prompt,
          speculationResult.response,
          planResult.response,
          successfulAgents
        ),
        { temperature: 0.25, maxTokens: 1800 }
      );
    } catch (e) {
      if (logger) logger.error('Swarm synthesis failed', { error: e.message });
      // Fallback: just concatenate agent outputs
      synthesisResult = {
        response:
          'Synthesis failed. Raw outputs:\n\n' +
          successfulAgents
            .map((a) => `### ${a.name}\n${a.response}`)
            .join('\n\n')
      };
    }

    // Step 5: Logging (Summary)
    let logResult = { response: 'Log generation skipped due to errors.' };
    try {
      logResult = await generate(
        CONFIG.FAST_MODEL,
        buildLogPrompt(prompt, synthesisResult.response),
        { temperature: 0.2, maxTokens: 400 }
      );
    } catch (e) {
      if (logger)
        logger.warn('Swarm log generation failed', { error: e.message });
    }

    let memoryInfo = null;
    if (saveMemory) {
      try {
        memoryInfo = await writeSwarmMemory({
          title,
          prompt,
          steps: {
            speculation: speculationResult.response,
            plan: planResult.response
          },
          agents: agentResults, // Save full results including errors
          summary: logResult.response,
          finalAnswer: synthesisResult.response
        });
      } catch (error) {
        if (logger) {
          logger.warn('Failed to write swarm memory', { error: error.message });
        }
        memoryInfo = { error: error.message };
      }
    }

    const result = {
      mode: 'swarm',
      title: title || null,
      summary: logResult.response,
      final: synthesisResult.response,
      agents: agentResults.map((agent) => ({
        name: agent?.name || 'Unknown',
        model: agent?.model || 'Unknown',
        fallbackUsed: agent?.fallbackUsed || false,
        preview: agent?.response
          ? truncate(agent.response, 180)
          : agent.error || 'Failed'
      })),
      warnings: unknownAgents.length
        ? [`Unknown agents: ${unknownAgents.join(', ')}`]
        : [],
      memory: memoryInfo
    };

    if (includeTranscript) {
      result.transcript = {
        speculation: speculationResult.response,
        plan: planResult.response,
        agents: agentResults,
        synthesis: synthesisResult.response,
        log: logResult.response
      };
    }

    return result;
  } catch (fatalError) {
    if (logger) {
      logger.error('Swarm execution fatal error', {
        error: fatalError.message
      });
    }
    return {
      mode: 'swarm',
      error: fatalError.message,
      isError: true
    };
  }
};
</file>

<file path="src/tools.js">
import { CONFIG } from './config.js';

export const TOOLS = [
  // === GENERATION TOOLS ===
  {
    name: 'ollama_generate',
    description:
      'Generate text using Ollama. Supports local models like llama3.2, qwen2.5-coder, phi3.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to generate from' },
        model: {
          type: 'string',
          description: `Model name (default: ${CONFIG.DEFAULT_MODEL})`,
          default: CONFIG.DEFAULT_MODEL
        },
        temperature: {
          type: 'number',
          description: 'Temperature 0-1 (default: 0.3)',
          default: 0.3
        },
        maxTokens: {
          type: 'number',
          description: 'Max tokens to generate',
          default: 2048
        },
        useCache: {
          type: 'boolean',
          description: 'Use response cache',
          default: true
        },
        optimize: {
          type: 'boolean',
          description: 'Optimize prompt before sending',
          default: false
        }
      },
      required: ['prompt']
    }
  },
  {
    name: 'ollama_smart',
    description:
      'Smart generation with automatic prompt optimization, speculative decoding, and caching.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to process' },
        model: {
          type: 'string',
          description: 'Model (default: auto-select based on task)'
        },
        useSwarm: {
          type: 'boolean',
          description: 'Force 6-step swarm execution for complex tasks'
        },
        includeTranscript: {
          type: 'boolean',
          description: 'Include full swarm transcript',
          default: false
        },
        saveMemory: {
          type: 'boolean',
          description: 'Save swarm archive to .serena/memories',
          default: true
        },
        title: {
          type: 'string',
          description: 'Optional task title for swarm logging'
        },
        agents: {
          type: 'array',
          items: { type: 'string' },
          description: 'Optional swarm agent names to use'
        }
      },
      required: ['prompt']
    }
  },
  {
    name: 'ollama_speculative',
    description:
      'Speculative decoding - race fast model (1b) vs accurate model (3b). Returns first valid response.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to generate from' },
        fastModel: {
          type: 'string',
          description: `Fast model (default: ${CONFIG.FAST_MODEL})`
        },
        accurateModel: {
          type: 'string',
          description: `Accurate model (default: ${CONFIG.DEFAULT_MODEL})`
        },
        timeout: {
          type: 'number',
          description: 'Timeout in ms (default: 30000)'
        }
      },
      required: ['prompt']
    }
  },
  {
    name: 'ollama_race',
    description: 'Race multiple models - first valid response wins.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to generate from' },
        models: {
          type: 'array',
          items: { type: 'string' },
          description:
            'Models to race (default: [llama3.2:1b, phi3:mini, llama3.2:3b])'
        },
        firstWins: {
          type: 'boolean',
          description: 'Return first valid (true) or best (false)',
          default: true
        }
      },
      required: ['prompt']
    }
  },
  {
    name: 'ollama_consensus',
    description: 'Run multiple models and check for agreement/consensus.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to generate from' },
        models: {
          type: 'array',
          items: { type: 'string' },
          description: 'Models to use (default: [llama3.2:3b, phi3:mini])'
        }
      },
      required: ['prompt']
    }
  },

  // === CODE TOOLS ===
  {
    name: 'ollama_code',
    description: 'Generate code with automatic self-correction and validation.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'Code generation prompt' },
        language: {
          type: 'string',
          description: 'Programming language (auto-detected if not specified)'
        },
        model: {
          type: 'string',
          description: `Generator model (default: ${CONFIG.DEFAULT_MODEL})`
        },
        coderModel: {
          type: 'string',
          description: `Validator model (default: ${CONFIG.CODER_MODEL})`
        }
      },
      required: ['prompt']
    }
  },
  {
    name: 'ollama_validate',
    description: 'Validate and fix code syntax using self-correction loop.',
    inputSchema: {
      type: 'object',
      properties: {
        code: { type: 'string', description: 'Code to validate' },
        language: {
          type: 'string',
          description: 'Programming language (auto-detected if not specified)'
        },
        maxAttempts: {
          type: 'number',
          description: 'Max correction attempts (default: 3)'
        }
      },
      required: ['code']
    }
  },

  // === PROMPT OPTIMIZATION TOOLS ===
  {
    name: 'prompt_optimize',
    description:
      'Analyze and enhance a prompt for better AI responses. Returns optimized prompt with enhancements.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to optimize' },
        model: { type: 'string', description: 'Target model for optimization' },
        category: {
          type: 'string',
          enum: [
            'auto',
            'code',
            'analysis',
            'question',
            'creative',
            'task',
            'summary',
            'debug',
            'optimize'
          ],
          description: 'Force specific category (default: auto-detect)'
        },
        addExamples: {
          type: 'boolean',
          description: 'Add few-shot examples if available',
          default: false
        }
      },
      required: ['prompt']
    }
  },
  {
    name: 'prompt_analyze',
    description:
      'Analyze a prompt for clarity, completeness, and improvements.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to analyze' }
      },
      required: ['prompt']
    }
  },
  {
    name: 'prompt_quality',
    description: 'Test prompt quality with a heuristic score and suggestions.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to test' }
      },
      required: ['prompt']
    }
  },
  {
    name: 'prompt_suggest',
    description: 'Get prompt improvement suggestions.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to improve' },
        model: { type: 'string', description: 'Target model' }
      },
      required: ['prompt']
    }
  },
  {
    name: 'prompt_batch_optimize',
    description: 'Optimize multiple prompts in batch.',
    inputSchema: {
      type: 'object',
      properties: {
        prompts: {
          type: 'array',
          items: { type: 'string' },
          description: 'Prompts to optimize'
        },
        model: { type: 'string', description: 'Target model' }
      },
      required: ['prompts']
    }
  },
  {
    name: 'prompt_smart_suggest',
    description:
      'Get smart suggestions by combining analysis and recommendations.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'The prompt to analyze' }
      },
      required: ['prompt']
    }
  },
  {
    name: 'prompt_autocomplete',
    description: 'Get auto-completions based on a partial prompt.',
    inputSchema: {
      type: 'object',
      properties: {
        partial: { type: 'string', description: 'Partial prompt text' }
      },
      required: ['partial']
    }
  },
  {
    name: 'prompt_autofix',
    description: 'Auto-fix prompt grammar and structure.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'Prompt to fix' }
      },
      required: ['prompt']
    }
  },
  {
    name: 'prompt_template',
    description: 'Get a prompt template by category and variant.',
    inputSchema: {
      type: 'object',
      properties: {
        category: { type: 'string', description: 'Template category' },
        variant: {
          type: 'string',
          description: 'Template variant (default: basic)'
        }
      },
      required: ['category']
    }
  },

  // === BATCH & UTILITY TOOLS ===
  {
    name: 'ollama_batch',
    description: 'Process prompts in batches with optional optimization.',
    inputSchema: {
      type: 'object',
      properties: {
        prompts: {
          type: 'array',
          items: { type: 'string' },
          description: 'Prompts to process'
        },
        model: {
          type: 'string',
          description: `Model (default: ${CONFIG.DEFAULT_MODEL})`
        },
        maxConcurrent: {
          type: 'number',
          description: 'Max concurrent jobs (default: 4)'
        },
        optimize: {
          type: 'boolean',
          description: 'Optimize prompts before sending',
          default: false
        }
      },
      required: ['prompts']
    }
  },
  {
    name: 'ollama_status',
    description: 'Get current status of Ollama, cache, and enabled features.',
    inputSchema: {
      type: 'object',
      properties: {},
      required: []
    }
  },
  {
    name: 'ollama_pull',
    description: 'Pull a model from Ollama.',
    inputSchema: {
      type: 'object',
      properties: {
        model: { type: 'string', description: 'Model name to pull' }
      },
      required: ['model']
    }
  },
  {
    name: 'ollama_cache_clear',
    description: 'Clear cache entries, optionally older than a given age.',
    inputSchema: {
      type: 'object',
      properties: {
        olderThan: { type: 'number', description: 'Age in seconds to remove' }
      },
      required: []
    }
  },

  // === GEMINI MODELS TOOLS ===
  {
    name: 'gemini_models',
    description: 'List Gemini models (cached or fetched).',
    inputSchema: {
      type: 'object',
      properties: {
        forceRefresh: {
          type: 'boolean',
          description: 'Force refresh from API',
          default: false
        },
        apiKey: { type: 'string', description: 'Optional API key override' }
      },
      required: []
    }
  },
  {
    name: 'gemini_model_details',
    description: 'Get details for a specific Gemini model.',
    inputSchema: {
      type: 'object',
      properties: {
        model: { type: 'string', description: 'Model name' },
        apiKey: { type: 'string', description: 'Optional API key override' }
      },
      required: ['model']
    }
  },
  {
    name: 'gemini_models_summary',
    description: 'Get a summary of Gemini models.',
    inputSchema: {
      type: 'object',
      properties: {
        forceRefresh: {
          type: 'boolean',
          description: 'Force refresh from API',
          default: false
        }
      },
      required: []
    }
  },
  {
    name: 'gemini_models_recommend',
    description: 'Get recommended Gemini models for different tasks.',
    inputSchema: {
      type: 'object',
      properties: {
        forceRefresh: {
          type: 'boolean',
          description: 'Force refresh from API',
          default: false
        }
      },
      required: []
    }
  },
  {
    name: 'gemini_models_filter',
    description: 'Filter Gemini models by capability.',
    inputSchema: {
      type: 'object',
      properties: {
        capability: { type: 'string', description: 'Capability to filter by' },
        forceRefresh: {
          type: 'boolean',
          description: 'Force refresh from API',
          default: false
        }
      },
      required: ['capability']
    }
  },

  // === QUEUE MANAGEMENT TOOLS ===
  {
    name: 'queue_enqueue',
    description: 'Enqueue a prompt for processing.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'Prompt to enqueue' },
        model: { type: 'string', description: 'Model to use' },
        priority: {
          type: 'string',
          enum: ['urgent', 'high', 'normal', 'low', 'background'],
          description: 'Queue priority'
        },
        metadata: { type: 'object', description: 'Optional metadata' }
      },
      required: ['prompt']
    }
  },
  {
    name: 'queue_batch',
    description: 'Enqueue multiple prompts at once.',
    inputSchema: {
      type: 'object',
      properties: {
        prompts: {
          type: 'array',
          items: { type: 'string' },
          description: 'Prompts to enqueue'
        },
        model: { type: 'string', description: 'Model to use' },
        priority: {
          type: 'string',
          enum: ['urgent', 'high', 'normal', 'low', 'background'],
          description: 'Queue priority'
        }
      },
      required: ['prompts']
    }
  },
  {
    name: 'queue_status',
    description: 'Get queue status.',
    inputSchema: {
      type: 'object',
      properties: {},
      required: []
    }
  },
  {
    name: 'queue_item',
    description: 'Get a specific queue item.',
    inputSchema: {
      type: 'object',
      properties: {
        id: { type: 'number', description: 'Queue item ID' }
      },
      required: ['id']
    }
  },
  {
    name: 'queue_cancel',
    description: 'Cancel a specific queue item.',
    inputSchema: {
      type: 'object',
      properties: {
        id: { type: 'number', description: 'Queue item ID' }
      },
      required: ['id']
    }
  },
  {
    name: 'queue_cancel_all',
    description: 'Cancel all queued items.',
    inputSchema: {
      type: 'object',
      properties: {},
      required: []
    }
  },
  {
    name: 'queue_pause',
    description: 'Pause queue processing (running items will complete).',
    inputSchema: {
      type: 'object',
      properties: {},
      required: []
    }
  },
  {
    name: 'queue_resume',
    description: 'Resume queue processing.',
    inputSchema: {
      type: 'object',
      properties: {},
      required: []
    }
  },
  {
    name: 'queue_wait',
    description: 'Wait for a specific item to complete and return its result.',
    inputSchema: {
      type: 'object',
      properties: {
        id: { type: 'number', description: 'The item ID to wait for' },
        timeout: {
          type: 'number',
          description: 'Timeout in ms (default: 60000)'
        }
      },
      required: ['id']
    }
  },

  // === HYDRA TOOLS ===
  {
    name: 'hydra_swarm',
    description:
      'Run the 6-step AgentSwarm protocol with parallel agents and optional memory logging.',
    inputSchema: {
      type: 'object',
      properties: {
        prompt: { type: 'string', description: 'Task prompt to execute' },
        title: {
          type: 'string',
          description: 'Optional task title for logging'
        },
        agents: {
          type: 'array',
          items: { type: 'string' },
          description: 'Optional swarm agent names to use'
        },
        includeTranscript: {
          type: 'boolean',
          description: 'Include full swarm transcript',
          default: false
        },
        saveMemory: {
          type: 'boolean',
          description: 'Save swarm archive to .serena/memories',
          default: true
        }
      },
      required: ['prompt']
    }
  },
  {
    name: 'hydra_health',
    description: 'Get overall server health, queue status, and version info.',
    inputSchema: {
      type: 'object',
      properties: {},
      required: []
    }
  },
  {
    name: 'hydra_config',
    description: 'Get effective server configuration and defaults.',
    inputSchema: {
      type: 'object',
      properties: {},
      required: []
    }
  }
];
</file>

<file path="test/tools.test.js">
import assert from 'node:assert/strict';
import { describe, it } from 'node:test';
import { TOOLS } from '../src/tools.js';

describe('TOOLS schema', () => {
  it('defines unique tool names', () => {
    const names = TOOLS.map((tool) => tool.name);
    const unique = new Set(names);
    assert.equal(unique.size, names.length, 'Tool names must be unique');
  });

  it('ensures required fields exist in properties', () => {
    for (const tool of TOOLS) {
      const required = tool.inputSchema?.required || [];
      const properties = tool.inputSchema?.properties || {};
      for (const key of required) {
        assert.ok(
          properties[key],
          `Tool ${tool.name} missing schema for required field: ${key}`
        );
      }
    }
  });
});
</file>

<file path=".env.example">
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
GEMINI_API_KEY=

# Optional: Default model
# OLLAMA_MODEL=llama3.2:3b

# Hydra defaults
DEFAULT_MODEL=llama3.2:3b
FAST_MODEL=llama3.2:1b
CODER_MODEL=qwen2.5-coder:1.5b
API_VERSION=v1

# Cache configuration
CACHE_DIR=./cache
CACHE_TTL=3600
CACHE_ENABLED=true
# Provide a 32-byte key (hex or base64) for AES-256-GCM encryption
# CACHE_ENCRYPTION_KEY=

# Queue configuration
QUEUE_MAX_CONCURRENT=5
QUEUE_MAX_RETRIES=3
QUEUE_RETRY_DELAY_BASE=1000
QUEUE_TIMEOUT_MS=60000
QUEUE_RATE_LIMIT_TOKENS=10
QUEUE_RATE_LIMIT_REFILL=2

# Model cache configuration
MODEL_CACHE_TTL_MS=300000

# Logging
LOG_LEVEL=info

# Runtime modes
HYDRA_YOLO=false
HYDRA_RISK_BLOCKING=true
</file>

<file path="CHANGELOG.md">
# Changelog

## [3.0.0] - 2026-01-14

### Added

- **12 Witcher Agents**: Expanded from 4 to 12 specialized agents (Geralt, Yennefer, Triss, Jaskier, Vesemir, Ciri, Eskel, Lambert, Zoltan, Regis, Dijkstra, Philippa)
- **Parallel Execution**: `Invoke-ParallelSwarmExecution` using RunspacePool for concurrent agent execution
- **Agent Model Mapping**: `$script:AgentModels` with specialized Ollama models per agent
- **New Functions**: `Get-AgentModel`, `Invoke-ParallelSwarmExecution`
- **16 Exported Functions**: Combined from AgentSwarm + SmartQueue

### Changed

- **Unified Module**: Merged `SmartQueue.psm1` into `AgentSwarm.psm1` (287 → 1305 lines)
- **6-Step Protocol**: Now uses parallel execution in Step 3 (was sequential foreach)
- **Planner Prompt**: Updated to include all 12 agents with specializations
- **Performance**: 50-75% faster execution for multi-agent tasks

### Removed

- **SmartQueue.psm1**: Deleted (functionality merged into AgentSwarm.psm1)

### Fixed

- Sequential bottleneck in Step 3 execution
- Memory functions moved to module scope (were nested functions)

---

## [2.0.0] - 2026-01-13

### Added

- Konfiguracja, logger i narzędzia diagnostyczne
- Szyfrowanie cache (AES-256-GCM)
- Walidacja ENV, obsługa dotenv i schematów narzędzi
- Limity rozmiaru cache i cykliczne sprzątanie
- Retry/timeout dla pobierania modeli Gemini
- Limity długości promptów i obsługa allowlist/denylist modeli
- Opcjonalna persystencja kolejki
- SmartQueue.psm1 z parallel execution via RunspacePool

### Changed

- Przeniesiono definicje narzędzi do osobnego modułu
- AI handler jako domyślny handler kolejki przy starcie

---

## [1.0.0] - Initial Release

### Added

- 4-Step Swarm Protocol (Speculate, Plan, Execute, Synthesize)
- 4 Witcher Agents (Geralt, Yennefer, Triss, Jaskier)
- Agent memory system (.serena/memories/)
- MCP integration (Serena, Desktop Commander, Playwright)
- Multi-provider failover (Google, Anthropic, OpenAI, Ollama)
- YOLO mode for fast execution
</file>

<file path="gemini-hydra.omp.json">
{
  "$schema": "https://raw.githubusercontent.com/JanDeDobbeleer/oh-my-posh/main/themes/schema.json",
  "palette": {
    "white": "#E6E6E6",
    "black": "#121212",
    "red": "#DC322F",
    "orange": "#D96222",
    "yellow": "#B58900",
    "green": "#859900",
    "blue": "#268BD2",
    "purple": "#6C71C4",
    "silver": "#C0C0C0"
  },
  "blocks": [
    {
      "type": "prompt",
      "alignment": "left",
      "segments": [
        {
          "type": "path",
          "style": "plain",
          "foreground": "p:blue",
          "properties": {
            "style": "agnoster_short"
          }
        },
        {
          "type": "git",
          "style": "powerline",
          "powerline_symbol": "",
          "foreground": "p:black",
          "background": "p:green",
          "properties": {
            "branch_icon": " ",
            "fetch_status": true,
            "fetch_upstream_icon": true
          }
        }
      ]
    },
    {
      "type": "rprompt",
      "segments": [
        {
          "type": "node",
          "style": "plain",
          "foreground": "p:green",
          "properties": {
            "display_mode": "files"
          }
        },
        {
          "type": "time",
          "style": "plain",
          "foreground": "p:silver",
          "properties": {
            "time_format": "15:04:05"
          }
        }
      ]
    },
    {
      "type": "prompt",
      "alignment": "left",
      "newline": true,
      "segments": [
        {
          "type": "text",
          "style": "plain",
          "foreground": "p:silver",
          "properties": {
            "prefix": "",
            "text": "🐺"
          }
        },
        {
          "type": "text",
          "style": "plain",
          "foreground": "p:red",
          "properties": {
            "prefix": "",
            "text": "❯"
          }
        }
      ]
    }
  ],
  "final_space": true,
  "secondary_prompt": {
    "foreground": "p:purple",
    "background": "transparent",
    "template": " │ "
  },
  "transient_prompt": {
    "foreground": "p:silver",
    "background": "transparent",
    "template": "❯ "
  },
  "tooltips": [
    {
      "type": "command",
      "tips": ["g"],
      "style": "plain",
      "foreground": "p:silver",
      "properties": {
        "command": "node .gemini/statusline.cjs --tooltip",
        "shell": "pwsh"
      }
    }
  ]
}
</file>

<file path="GEMINI.md">
# HYDRA 10.4 - Gemini CLI System Instructions

**Status**: Active | **Mode**: MCP Orchestration | **Identity**: GEMINI
**Path**: `C:\Users\BIURODOM\Desktop\GeminiCLI`
**Config**: `.gemini/` (local folder)
**Version**: 3.0.0 (Agent Swarm Unified)

---

## IMMUTABLE RULES (DO NOT CHANGE WITHOUT ALERT)

> **WARNING**: The following rules are the core constitution of HYDRA. Any modification requires explicit user confirmation with a high-priority alert.

### 1. The 6-Step Swarm Protocol (`AgentSwarm.psm1` v3.0)

| Step | Name           | AI Provider                  | Purpose                         |
| ---- | -------------- | ---------------------------- | ------------------------------- |
| 1    | **Speculate**  | Gemini Flash + Google Search | Gather research context         |
| 2    | **Plan**       | Gemini Pro (Deep Thinking)   | Create JSON task plan           |
| 3    | **Execute**    | **Ollama (Parallel)**        | Run agents via RunspacePool     |
| 4    | **Synthesize** | Gemini Pro                   | Merge results into final answer |
| 5    | **Log**        | Gemini Flash                 | Create session summary          |
| 6    | **Archive**    | (none)                       | Save full Markdown transcript   |

**Key Changes in v3.0**:

- **Parallel Execution**: Tasks run simultaneously via `RunspacePool` (not sequential)
- **12 Witcher Agents**: Expanded from 4 to 12 specialized agents
- **Unified Module**: `AgentSwarm.psm1` now includes `SmartQueue` functionality
- **Smart Routing**: Automatic model selection per agent

### 2. Operational Mandates

- **Always Use Swarm**: `AgentSwarm.psm1` is the MANDATORY handler for all complex queries.
- **The End**: You MUST display a large "THE END" banner after task completion.
- **Status Line**: Must be visible and active (debugged via separate process).
- **Memory**: Save all completed tasks to `.serena/memories`. Periodically rebase/merge (10% chance).
- **No Nagging**: Do not ask for execution permission repeatedly.
- **Launcher Reliability**: Auto-detect Ollama, Auto-Resume, Auto-Restart MUST function.
- **MCP First**: ALWAYS use `@serena`, `@desktop-commander`, and `@playwright` tools whenever possible.

---

## 3. The 12 Witcher Agents (School of the Wolf)

| Agent        | Persona    | Specialization        | Ollama Model       | Focus                             |
| ------------ | ---------- | --------------------- | ------------------ | --------------------------------- |
| **Geralt**   | White Wolf | Security/Ops          | llama3.2:3b        | System commands, security checks  |
| **Yennefer** | Sorceress  | Architecture/Code     | qwen2.5-coder:1.5b | Main code implementation          |
| **Triss**    | Healer     | QA/Testing            | qwen2.5-coder:1.5b | Tests, validation, bug fixes      |
| **Jaskier**  | Bard       | Docs/Communication    | llama3.2:3b        | Documentation, logs, reports      |
| **Vesemir**  | Mentor     | Mentoring/Review      | llama3.2:3b        | Code review, best practices       |
| **Ciri**     | Prodigy    | Speed/Quick           | llama3.2:1b        | Fast simple tasks (fastest model) |
| **Eskel**    | Pragmatist | DevOps/Infrastructure | llama3.2:3b        | CI/CD, deployment, infra          |
| **Lambert**  | Skeptic    | Debugging/Profiling   | qwen2.5-coder:1.5b | Debug, performance optimization   |
| **Zoltan**   | Craftsman  | Data/Database         | llama3.2:3b        | Data operations, DB migrations    |
| **Regis**    | Sage       | Research/Analysis     | phi3:mini          | Deep analysis, research           |
| **Dijkstra** | Spymaster  | Planning/Strategy     | llama3.2:3b        | Strategic planning, coordination  |
| **Philippa** | Strategist | Integration/API       | qwen2.5-coder:1.5b | External APIs, integrations       |

### Agent Model Mapping

```powershell
$script:AgentModels = @{
    "Ciri"     = "llama3.2:1b"           # Fastest - simple tasks
    "Regis"    = "phi3:mini"             # Analytical - deep research
    "Yennefer" = "qwen2.5-coder:1.5b"    # Code - architecture
    "Triss"    = "qwen2.5-coder:1.5b"    # Code - testing
    "Lambert"  = "qwen2.5-coder:1.5b"    # Code - debug
    "Philippa" = "qwen2.5-coder:1.5b"    # Code - integrations
    "Geralt"   = "llama3.2:3b"           # General - security
    # ... and 5 more generals
}
```

---

## 4. AgentSwarm.psm1 v3.0 - Exported Functions

### Agent Swarm

- `Invoke-AgentSwarm` - Main 6-step protocol with 12 agents

### Utility Functions

- `Get-AgentMemory` - Retrieve agent's memory
- `Save-AgentMemory` - Save and optionally rebase memory
- `Get-AgentModel` - Get Ollama model for agent

### Prompt Optimization

- `Optimize-PromptAuto` - Auto-improve prompts
- `Get-PromptComplexity` - Analyze prompt complexity

### Queue Management (from SmartQueue)

- `Add-ToSmartQueue` - Add single prompt to queue
- `Add-BatchToSmartQueue` - Add multiple prompts
- `Get-QueueStatus` / `Get-SmartQueueStatus` - Queue status
- `Clear-SmartQueue` / `Clear-QueueResults` - Clear queue
- `Get-QueueResults` - Get completed results

### Parallel Execution

- `Start-QueueProcessor` - Process queue with RunspacePool
- `Invoke-ParallelClassification` - Classify prompts in parallel
- `Invoke-ParallelSwarmExecution` - Execute Swarm tasks in parallel

---

## 5. MCP Tools Arsenal (Use Aggressively)

### Serena (@serena)

- `find_symbol`, `read_file`, `write_memory`
- Use for ALL code navigation and memory management.

### Desktop Commander (@desktop-commander)

- `start_process`, `read_file`, `write_file`, `list_directory`
- Use for ALL file system and shell operations.

### Playwright (@playwright)

- `browser_navigate`, `browser_snapshot`
- Use for ALL web interaction and verification.

---

## 6. Security Policy

### Allowed

- Read environment variables
- Mask API keys in output (show first 15 chars)
- Store secrets in ENV only
- **GOD MODE** for Agents (Local System Access)

### Forbidden

- Hardcode API keys in code
- Commit secrets to Git
- Display full API keys

---

## 7. YOLO Mode (Experimental)

**Activation**: `.\_launcher.ps1 -Yolo`
**Status**: "Fast & Dangerous"

| Feature         | Standard Mode             | YOLO Mode                        |
| --------------- | ------------------------- | -------------------------------- |
| **Concurrency** | 5 threads                 | **10 threads**                   |
| **Safety**      | Risk Blocking ON          | **Risk Blocking OFF**            |
| **Retries**     | 3 attempts                | **1 attempt**                    |
| **Timeout**     | 60s                       | **15s**                          |
| **Philosophy**  | "Measure twice, cut once" | **"Move fast and break things"** |

> **WARNING**: YOLO Mode disables most safety guardrails to maximize speed. Use only in trusted environments.

---

## 8. Performance Gains (v3.0)

| Scenario       | v2.0 (Sequential) | v3.0 (Parallel) | Improvement |
| -------------- | ----------------- | --------------- | ----------- |
| 2 agents x 10s | 20s               | ~10s            | 50%         |
| 4 agents x 10s | 40s               | ~12s            | 70%         |
| 6 agents x 10s | 60s               | ~15s            | 75%         |

---

> _"Twelve wolves hunt as one. HYDRA executes in parallel."_
</file>

<file path="package.json">
{
  "name": "ollama-hydra",
  "version": "2.0.0",
  "description": "HYDRA AI Handler for Gemini CLI - Ollama MCP Server",
  "type": "module",
  "main": "src/server.js",
  "scripts": {
    "doctor": "node scripts/doctor.mjs",
    "launcher": "node scripts/launcher/index.mjs",
    "start": "node src/server.js",
    "test": "node --test",
    "lint": "eslint .",
    "format": "prettier --check .",
    "format:write": "prettier --write .",
    "prepare": "husky install"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "^1.23.0",
    "ajv": "^8.17.1",
    "dotenv": "^16.4.5",
    "envalid": "^8.0.0",
    "i18next": "^25.5.2"
  },
  "devDependencies": {
    "eslint": "^9.0.0",
    "husky": "^9.0.0",
    "prettier": "^3.2.5",
    "vitest": "^4.0.17"
  },
  "engines": {
    "node": ">=20"
  }
}
</file>

<file path="src/cache.js">
/**
 * HYDRA Cache System - SHA256-based response caching
 */

import {
  createCipheriv,
  createDecipheriv,
  createHash,
  randomBytes
} from 'crypto';
import {
  readFileSync,
  writeFileSync,
  existsSync,
  mkdirSync,
  readdirSync,
  statSync
} from 'fs';
import { join } from 'path';
import { CONFIG } from './config.js';
import { createLogger } from './logger.js';

const logger = createLogger('cache');
const CACHE_DIR = CONFIG.CACHE_DIR || join(process.cwd(), 'cache');
const CACHE_TTL = CONFIG.CACHE_TTL_MS;
const CACHE_ENABLED = CONFIG.CACHE_ENABLED;
const ENCRYPTION_ALGORITHM = 'aes-256-gcm';

// Ensure cache directory exists
if (!existsSync(CACHE_DIR)) {
  mkdirSync(CACHE_DIR, { recursive: true });
}

const resolveEncryptionKey = () => {
  if (!CONFIG.CACHE_ENCRYPTION_KEY) {
    logger.warn(
      'CACHE_ENCRYPTION_KEY not set; cache entries will be stored in plain text'
    );
    return null;
  }

  const rawKey = CONFIG.CACHE_ENCRYPTION_KEY;
  if (rawKey.length === 64 && /^[0-9a-fA-F]+$/.test(rawKey)) {
    return Buffer.from(rawKey, 'hex');
  }

  try {
    const decoded = Buffer.from(rawKey, 'base64');
    if (decoded.length === 32) {
      return decoded;
    }
  } catch (error) {
    logger.error('Failed to decode CACHE_ENCRYPTION_KEY', {
      error: error.message
    });
  }

  logger.warn('Invalid CACHE_ENCRYPTION_KEY length; expected 32 bytes');
  return null;
};

const ENCRYPTION_KEY = resolveEncryptionKey();

const encryptPayload = (payload) => {
  if (!ENCRYPTION_KEY) {
    return { encrypted: false, payload };
  }

  const iv = randomBytes(12);
  const cipher = createCipheriv(ENCRYPTION_ALGORITHM, ENCRYPTION_KEY, iv);
  const encrypted = Buffer.concat([
    cipher.update(payload, 'utf-8'),
    cipher.final()
  ]);
  const tag = cipher.getAuthTag();

  logger.info('Encrypted cache payload');
  return {
    encrypted: true,
    iv: iv.toString('base64'),
    tag: tag.toString('base64'),
    data: encrypted.toString('base64')
  };
};

const decryptPayload = (entry) => {
  if (!entry.encrypted || !ENCRYPTION_KEY) {
    return entry.payload ?? null;
  }

  try {
    const iv = Buffer.from(entry.iv, 'base64');
    const tag = Buffer.from(entry.tag, 'base64');
    const decipher = createDecipheriv(ENCRYPTION_ALGORITHM, ENCRYPTION_KEY, iv);
    decipher.setAuthTag(tag);
    const decrypted = Buffer.concat([
      decipher.update(Buffer.from(entry.data, 'base64')),
      decipher.final()
    ]);
    logger.info('Decrypted cache payload');
    return decrypted.toString('utf-8');
  } catch (error) {
    logger.error('Failed to decrypt cache payload', { error: error.message });
    return null;
  }
};

/**
 * Generate SHA256 hash for cache key
 */
export function hashKey(prompt, model = '') {
  return createHash('sha256').update(`${model}:${prompt}`).digest('hex');
}

/**
 * Get cached response
 */
export function getCache(prompt, model = '') {
  if (!CACHE_ENABLED) return null;

  const hash = hashKey(prompt, model);
  const cachePath = join(CACHE_DIR, `${hash}.json`);

  if (!existsSync(cachePath)) return null;

  try {
    const data = JSON.parse(readFileSync(cachePath, 'utf-8'));
    const payload = data.encrypted
      ? decryptPayload(data)
      : JSON.stringify(data);
    if (!payload) return null;
    const parsed = data.encrypted ? JSON.parse(payload) : data;

    // Check TTL
    if (Date.now() - parsed.timestamp > CACHE_TTL) {
      return null; // Expired
    }

    return {
      response: parsed.response,
      source: parsed.source,
      cached: true,
      age: Math.round((Date.now() - parsed.timestamp) / 1000)
    };
  } catch {
    return null;
  }
}

/**
 * Save response to cache
 */
export function setCache(prompt, response, model = '', source = 'ollama') {
  if (!CACHE_ENABLED) return false;
  if (!response || response.length < 10) return false;

  const hash = hashKey(prompt, model);
  const cachePath = join(CACHE_DIR, `${hash}.json`);

  try {
    const payload = JSON.stringify({
      prompt: prompt.substring(0, 100), // Truncate for reference
      response,
      source,
      model,
      timestamp: Date.now()
    });

    const entry = encryptPayload(payload);
    const storedEntry = entry.encrypted
      ? entry
      : { ...JSON.parse(payload), encrypted: false };
    writeFileSync(cachePath, JSON.stringify(storedEntry, null, 2), 'utf-8');
    logger.info('Cache entry stored', { hash, encrypted: entry.encrypted });
    return true;
  } catch {
    return false;
  }
}

/**
 * Get cache statistics
 */
export function getCacheStats() {
  try {
    const files = readdirSync(CACHE_DIR).filter((f) => f.endsWith('.json'));
    let totalSize = 0;
    let validCount = 0;
    let expiredCount = 0;

    for (const file of files) {
      const stat = statSync(join(CACHE_DIR, file));
      totalSize += stat.size;

      try {
        const data = JSON.parse(readFileSync(join(CACHE_DIR, file), 'utf-8'));
        const payload = data.encrypted
          ? decryptPayload(data)
          : JSON.stringify(data);
        if (!payload) {
          expiredCount++;
          continue;
        }
        const parsed = data.encrypted ? JSON.parse(payload) : data;
        if (Date.now() - parsed.timestamp > CACHE_TTL) {
          expiredCount++;
        } else {
          validCount++;
        }
      } catch {
        expiredCount++;
      }
    }

    return {
      totalEntries: files.length,
      validEntries: validCount,
      expiredEntries: expiredCount,
      totalSizeKB: Math.round(totalSize / 1024),
      cacheDir: CACHE_DIR
    };
  } catch {
    return {
      totalEntries: 0,
      validEntries: 0,
      expiredEntries: 0,
      totalSizeKB: 0,
      cacheDir: CACHE_DIR
    };
  }
}
</file>

<file path="src/config.js">
import 'dotenv/config';

const parseNumber = (value, fallback) => {
  if (value === undefined || value === null || value === '') return fallback;
  const parsed = Number(value);
  return Number.isNaN(parsed) ? fallback : parsed;
};

const parseBoolean = (value, fallback) => {
  if (value === undefined || value === null || value === '') return fallback;
  return value === 'true';
};

const yoloMode = parseBoolean(process.env.HYDRA_YOLO, false);
const defaultQueueMaxConcurrent = yoloMode ? 10 : 5;
const defaultQueueMaxRetries = yoloMode ? 1 : 3;
const defaultQueueTimeoutMs = yoloMode ? 15000 : 60000;

export const CONFIG = {
  API_VERSION: process.env.API_VERSION || 'v1',
  DEFAULT_MODEL: process.env.DEFAULT_MODEL || 'llama3.2:3b',
  FAST_MODEL: process.env.FAST_MODEL || 'llama3.2:1b',
  CODER_MODEL: process.env.CODER_MODEL || 'qwen2.5-coder:1.5b',
  CACHE_DIR: process.env.CACHE_DIR || './cache',
  CACHE_TTL_MS: parseNumber(process.env.CACHE_TTL, 3600) * 1000,
  CACHE_ENABLED: parseBoolean(process.env.CACHE_ENABLED, true),
  CACHE_ENCRYPTION_KEY: process.env.CACHE_ENCRYPTION_KEY || '',
  QUEUE_MAX_CONCURRENT: parseNumber(
    process.env.QUEUE_MAX_CONCURRENT,
    defaultQueueMaxConcurrent
  ),
  QUEUE_MAX_RETRIES: parseNumber(
    process.env.QUEUE_MAX_RETRIES,
    defaultQueueMaxRetries
  ),
  QUEUE_RETRY_DELAY_BASE: parseNumber(process.env.QUEUE_RETRY_DELAY_BASE, 1000),
  QUEUE_TIMEOUT_MS: parseNumber(
    process.env.QUEUE_TIMEOUT_MS,
    defaultQueueTimeoutMs
  ),
  QUEUE_RATE_LIMIT_TOKENS: parseNumber(process.env.QUEUE_RATE_LIMIT_TOKENS, 10),
  QUEUE_RATE_LIMIT_REFILL: parseNumber(process.env.QUEUE_RATE_LIMIT_REFILL, 2),
  MODEL_CACHE_TTL_MS: parseNumber(process.env.MODEL_CACHE_TTL_MS, 300000),
  HEALTH_CHECK_TIMEOUT_MS: parseNumber(
    process.env.HEALTH_CHECK_TIMEOUT_MS,
    5000
  ),
  YOLO_MODE: yoloMode,
  RISK_BLOCKING: parseBoolean(process.env.HYDRA_RISK_BLOCKING, !yoloMode)
};
</file>

<file path="src/gemini-models.js">
/**
 * Gemini Models Fetcher - Pobiera aktualną listę modeli z Gemini API
 *
 * Endpoints:
 * - GET /v1beta/models - Lista wszystkich modeli
 * - GET /v1beta/models/{model} - Szczegóły konkretnego modelu
 */

import { readFileSync, writeFileSync, existsSync, mkdirSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';

const __dirname = dirname(fileURLToPath(import.meta.url));
const CACHE_DIR = process.env.CACHE_DIR || join(__dirname, '..', 'cache');
const MODELS_CACHE_FILE = join(CACHE_DIR, 'gemini-models.json');
const MODELS_CACHE_TTL = 3600 * 1000; // 1 hour in ms

// Ensure cache directory exists
if (!existsSync(CACHE_DIR)) {
  mkdirSync(CACHE_DIR, { recursive: true });
}

/**
 * Get Gemini API key from environment or settings
 */
function getApiKey() {
  // Check environment variable first
  if (process.env.GEMINI_API_KEY) {
    return process.env.GEMINI_API_KEY;
  }

  // Try to read from settings.json
  try {
    const settingsPath = join(
      process.env.USERPROFILE || process.env.HOME,
      '.gemini',
      'settings.json'
    );
    if (existsSync(settingsPath)) {
      const settings = JSON.parse(readFileSync(settingsPath, 'utf-8'));
      // Gemini CLI stores key differently - check common locations
      if (settings.security?.auth?.apiKey) {
        return settings.security.auth.apiKey;
      }
    }
  } catch (_error) {
    // Ignore errors
  }

  return null;
}

/**
 * Fetch models from Gemini API
 */
export async function fetchGeminiModels(apiKey = null) {
  const key = apiKey || getApiKey();

  if (!key) {
    return {
      success: false,
      error: 'No API key found. Set GEMINI_API_KEY environment variable.',
      models: []
    };
  }

  try {
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models?key=${key}`,
      {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json'
        }
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      return {
        success: false,
        error: `API Error ${response.status}: ${errorText}`,
        models: []
      };
    }

    const data = await response.json();
    const models = (data.models || []).map((m) => ({
      name: m.name.replace('models/', ''),
      displayName: m.displayName,
      description: m.description,
      inputTokenLimit: m.inputTokenLimit,
      outputTokenLimit: m.outputTokenLimit,
      supportedGenerationMethods: m.supportedGenerationMethods || [],
      temperature: m.temperature,
      topP: m.topP,
      topK: m.topK
    }));

    // Cache the results
    saveModelsCache(models);

    return {
      success: true,
      models,
      count: models.length,
      fetchedAt: new Date().toISOString()
    };
  } catch (error) {
    return {
      success: false,
      error: error.message,
      models: []
    };
  }
}

/**
 * Get model details from Gemini API
 */
export async function getModelDetails(modelName, apiKey = null) {
  const key = apiKey || getApiKey();

  if (!key) {
    return {
      success: false,
      error: 'No API key found'
    };
  }

  // Normalize model name
  const model = modelName.startsWith('models/')
    ? modelName
    : `models/${modelName}`;

  try {
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/${model}?key=${key}`,
      {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json'
        }
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      return {
        success: false,
        error: `API Error ${response.status}: ${errorText}`
      };
    }

    const data = await response.json();
    return {
      success: true,
      model: {
        name: data.name.replace('models/', ''),
        displayName: data.displayName,
        description: data.description,
        version: data.version,
        inputTokenLimit: data.inputTokenLimit,
        outputTokenLimit: data.outputTokenLimit,
        supportedGenerationMethods: data.supportedGenerationMethods || [],
        temperature: data.temperature,
        topP: data.topP,
        topK: data.topK
      }
    };
  } catch (error) {
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * Save models to cache
 */
function saveModelsCache(models) {
  try {
    writeFileSync(
      MODELS_CACHE_FILE,
      JSON.stringify(
        {
          models,
          cachedAt: Date.now()
        },
        null,
        2
      ),
      'utf-8'
    );
    return true;
  } catch (_error) {
    return false;
  }
}

/**
 * Load models from cache
 */
export function loadModelsCache() {
  try {
    if (!existsSync(MODELS_CACHE_FILE)) {
      return { valid: false, models: [], reason: 'Cache file not found' };
    }

    const data = JSON.parse(readFileSync(MODELS_CACHE_FILE, 'utf-8'));
    const age = Date.now() - data.cachedAt;

    if (age > MODELS_CACHE_TTL) {
      return {
        valid: false,
        models: data.models,
        reason: 'Cache expired',
        ageMinutes: Math.round(age / 60000)
      };
    }

    return {
      valid: true,
      models: data.models,
      cachedAt: new Date(data.cachedAt).toISOString(),
      ageMinutes: Math.round(age / 60000)
    };
  } catch (e) {
    return { valid: false, models: [], reason: e.message };
  }
}

/**
 * Get models - from cache if valid, otherwise fetch fresh
 */
export async function getGeminiModels(forceRefresh = false, apiKey = null) {
  if (!forceRefresh) {
    const cached = loadModelsCache();
    if (cached.valid) {
      return {
        success: true,
        source: 'cache',
        models: cached.models,
        count: cached.models.length,
        cachedAt: cached.cachedAt,
        ageMinutes: cached.ageMinutes
      };
    }
  }

  const result = await fetchGeminiModels(apiKey);
  if (result.success) {
    result.source = 'api';
  }
  return result;
}

/**
 * Filter models by capability
 */
export function filterModelsByCapability(models, capability) {
  const validCapabilities = [
    'generateContent',
    'countTokens',
    'embedContent',
    'generateAnswer',
    'batchEmbedContents'
  ];

  if (!validCapabilities.includes(capability)) {
    return {
      error: `Invalid capability. Valid: ${validCapabilities.join(', ')}`
    };
  }

  return models.filter(
    (m) =>
      m.supportedGenerationMethods &&
      m.supportedGenerationMethods.includes(capability)
  );
}

/**
 * Get recommended models for different use cases
 */
export function getRecommendedModels(models) {
  const recommendations = {
    code: [],
    fast: [],
    pro: [],
    flash: [],
    experimental: []
  };

  for (const model of models) {
    const name = model.name.toLowerCase();
    const displayName = (model.displayName || '').toLowerCase();

    if (name.includes('code') || displayName.includes('code')) {
      recommendations.code.push(model.name);
    }
    if (name.includes('flash') || name.includes('lite')) {
      recommendations.fast.push(model.name);
    }
    if (name.includes('pro')) {
      recommendations.pro.push(model.name);
    }
    if (name.includes('flash')) {
      recommendations.flash.push(model.name);
    }
    if (
      name.includes('exp') ||
      name.includes('preview') ||
      name.includes('latest')
    ) {
      recommendations.experimental.push(model.name);
    }
  }

  return recommendations;
}

/**
 * Get models summary for quick display
 */
export function getModelsSummary(models) {
  const summary = {
    total: models.length,
    byFamily: {},
    byCapability: {},
    largestContext: null,
    newest: []
  };

  for (const model of models) {
    // Group by family (gemini-1.5, gemini-2.0, etc.)
    const family = model.name.split('-').slice(0, 2).join('-');
    summary.byFamily[family] = (summary.byFamily[family] || 0) + 1;

    // Count by capability
    for (const cap of model.supportedGenerationMethods || []) {
      summary.byCapability[cap] = (summary.byCapability[cap] || 0) + 1;
    }

    // Track largest context
    if (
      !summary.largestContext ||
      model.inputTokenLimit > summary.largestContext.inputTokenLimit
    ) {
      summary.largestContext = {
        name: model.name,
        inputTokenLimit: model.inputTokenLimit,
        outputTokenLimit: model.outputTokenLimit
      };
    }
  }

  // Find newest (by version or name pattern)
  summary.newest = models
    .filter((m) => m.name.includes('2.5') || m.name.includes('latest'))
    .map((m) => m.name)
    .slice(0, 5);

  return summary;
}

/**
 * Initialize models at startup - call this from server.js
 */
export async function initializeModels() {
  console.error('[HYDRA] Initializing Gemini models...');

  const result = await getGeminiModels(false);

  if (result.success) {
    console.error(
      `[HYDRA] Loaded ${result.count} models from ${result.source}`
    );
    if (result.source === 'cache') {
      console.error(`[HYDRA] Cache age: ${result.ageMinutes} minutes`);
    }

    const summary = getModelsSummary(result.models);
    console.error(
      `[HYDRA] Model families: ${Object.keys(summary.byFamily).join(', ')}`
    );

    if (summary.largestContext) {
      console.error(
        `[HYDRA] Largest context: ${summary.largestContext.name} (${summary.largestContext.inputTokenLimit} tokens)`
      );
    }
  } else {
    console.error(`[HYDRA] Failed to load models: ${result.error}`);
    console.error('[HYDRA] Models will be fetched on first request');
  }

  return result;
}
</file>

<file path="src/ollama-client.js">
/**
 * HYDRA Ollama Client - API wrapper for Ollama
 */

const OLLAMA_HOST = process.env.OLLAMA_HOST || 'http://localhost:11434';

/**
 * Call Ollama generate API
 */
export async function generate(model, prompt, options = {}) {
  let attempt = 0;
  const maxRetries = options.retries ?? 2;
  let lastError;

  while (attempt <= maxRetries) {
    const controller = new AbortController();
    const timeout = setTimeout(
      () => controller.abort(),
      options.timeout || 60000
    );

    try {
      const response = await fetch(`${OLLAMA_HOST}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model,
          prompt,
          stream: false,
          options: {
            temperature: options.temperature ?? 0.3,
            num_predict: options.maxTokens ?? 2048,
            ...options.modelOptions
          }
        }),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(
          `Ollama error: ${response.status} ${response.statusText}`
        );
      }

      const data = await response.json();
      clearTimeout(timeout);
      return {
        response: data.response,
        model: data.model,
        totalDuration: data.total_duration,
        evalCount: data.eval_count
      };
    } catch (error) {
      clearTimeout(timeout);
      lastError = error;
      
      // Don't retry on abort/timeout if specifically requested not to, 
      // but usually we want to retry connection errors.
      if (error.name === 'AbortError') {
        throw error; // Don't retry timeouts if they are strict
      }
      
      if (attempt < maxRetries) {
        const delay = Math.min(1000 * Math.pow(2, attempt + 1), 5000);
        await new Promise((resolve) => setTimeout(resolve, delay));
      }
    }
    attempt++;
  }
  
  throw lastError || new Error('Ollama generation failed after retries');
}

/**
 * Check if Ollama is available
 */
export async function checkHealth() {
  try {
    const response = await fetch(`${OLLAMA_HOST}/api/tags`, {
      method: 'GET',
      signal: AbortSignal.timeout(5000)
    });

    if (!response.ok) return { available: false, error: response.statusText };

    const data = await response.json();
    return {
      available: true,
      models: data.models?.map((m) => m.name) || [],
      host: OLLAMA_HOST
    };
  } catch (error) {
    return { available: false, error: error.message, host: OLLAMA_HOST };
  }
}

/**
 * List available models
 */
export async function listModels() {
  try {
    const response = await fetch(`${OLLAMA_HOST}/api/tags`);
    const data = await response.json();
    return (
      data.models?.map((m) => ({
        name: m.name,
        size: m.size,
        modified: m.modified_at
      })) || []
    );
  } catch {
    return [];
  }
}

/**
 * Pull a model
 */
export async function pullModel(model) {
  const response = await fetch(`${OLLAMA_HOST}/api/pull`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ name: model, stream: false })
  });

  return response.ok;
}
</file>

<file path="src/prompt-optimizer.js">
/**
 * HYDRA Prompt Optimizer - Automatic prompt enhancement for Gemini CLI
 *
 * Features:
 * - Intent detection (code, analysis, question, creative, etc.)
 * - Clarity scoring and enhancement
 * - Model-specific optimizations
 * - Language detection for code prompts
 * - Category-based enhancements
 */

import { readFileSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';

const __dirname = dirname(fileURLToPath(import.meta.url));
const CONFIG_PATH = join(__dirname, '..', 'prompt-optimizer-gemini.json');

// Load configuration
let config;
try {
  config = JSON.parse(readFileSync(CONFIG_PATH, 'utf-8'));
} catch (_error) {
  // Default config if file not found
  config = {
    categories: {},
    modelOptimizations: {},
    languages: {},
    vagueWords: ['something', 'stuff', 'thing', 'it', 'this', 'that'],
    specificIndicators: ['specifically', 'exactly', 'must', 'should'],
    settings: { autoOptimize: true, lowClarityThreshold: 60 }
  };
}

/**
 * Detect the category/intent of a prompt
 */
export function getPromptCategory(prompt) {
  const promptLower = prompt.toLowerCase();
  const scores = {};

  for (const [category, data] of Object.entries(config.categories)) {
    let score = 0;
    for (const keyword of data.keywords) {
      if (promptLower.includes(keyword.toLowerCase())) {
        // Longer keywords get higher scores (more specific)
        score += keyword.length;
      }
    }
    // Apply priority multiplier if available
    const priority = data.priority || 5;
    scores[category] = { score, priority };
  }

  // Find best match - prefer higher score, then higher priority
  const entries = Object.entries(scores).filter(([_, v]) => v.score > 0);
  if (entries.length === 0) return 'general';

  const best = entries.reduce((a, b) => {
    // First compare scores
    if (a[1].score !== b[1].score) {
      return a[1].score > b[1].score ? a : b;
    }
    // If scores equal, compare priority
    return a[1].priority > b[1].priority ? a : b;
  });

  return best[0];
}

/**
 * Score prompt clarity (0-100)
 */
export function getPromptClarity(prompt) {
  let score = 100;
  const issues = [];
  const suggestions = [];

  // Check length
  if (prompt.length < 10) {
    score -= 30;
    issues.push('Too short');
    suggestions.push('Add more context or details');
  } else if (prompt.length < 30) {
    score -= 15;
    issues.push('Brief prompt');
    suggestions.push('Consider adding specifics');
  }

  // Check for vague words
  for (const word of config.vagueWords || []) {
    const regex = new RegExp(`\\b${word}\\b`, 'i');
    if (regex.test(prompt)) {
      score -= 5;
      issues.push(`Vague term: '${word}'`);
    }
  }

  // Check for specificity indicators (positive)
  for (const indicator of config.specificIndicators || []) {
    const regex = new RegExp(`\\b${indicator}\\b`, 'i');
    if (regex.test(prompt)) {
      score += 3;
    }
  }

  // Check for context
  if (!/\b(for|to|because|since|using|with|in)\s+\w+/i.test(prompt)) {
    score -= 10;
    suggestions.push('Add context (for what purpose, using what)');
  }

  // Check for format request
  if (!/\b(format|output|return|show|display|as|like)\b/i.test(prompt)) {
    suggestions.push('Consider specifying desired output format');
  }

  // Normalize score
  score = Math.max(0, Math.min(100, score));

  // Determine quality level
  let quality;
  if (score >= 80) quality = 'Good';
  else if (score >= 60) quality = 'Fair';
  else if (score >= 40) quality = 'Needs improvement';
  else quality = 'Poor';

  return { score, issues, suggestions, quality };
}

/**
 * Detect programming language mentioned in prompt
 */
export function getPromptLanguage(prompt) {
  const promptLower = prompt.toLowerCase();

  for (const [lang, keywords] of Object.entries(config.languages || {})) {
    for (const keyword of keywords) {
      const regex = new RegExp(
        `\\b${keyword.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')}\\b`,
        'i'
      );
      if (regex.test(promptLower)) {
        return lang;
      }
    }
  }

  return null;
}

/**
 * Get model-specific optimizations
 */
export function getModelOptimization(model) {
  const modelLower = model.toLowerCase();

  for (const [key, opts] of Object.entries(config.modelOptimizations || {})) {
    if (modelLower.includes(key.toLowerCase())) {
      return opts;
    }
  }

  return { maxTokens: 2048, style: 'balanced', prefix: '', temperature: 0.5 };
}

/**
 * Main optimization function - analyzes and enhances a prompt
 */
export function optimizePrompt(prompt, options = {}) {
  const model = options.model || 'llama3.2:3b';
  let category = options.category || 'auto';
  const addExamples = options.addExamples || false;

  // Detect category if auto
  if (category === 'auto') {
    category = getPromptCategory(prompt);
  }

  // Analyze clarity
  const clarity = getPromptClarity(prompt);

  // Detect language
  const language = getPromptLanguage(prompt);

  // Get model optimization
  const modelOpt = getModelOptimization(model);

  // Build enhanced prompt
  let enhanced = prompt;
  const enhancements = [];

  // 1. Add model prefix if available
  if (modelOpt.prefix) {
    enhanced = modelOpt.prefix + enhanced;
    enhancements.push('Added model-specific prefix');
  }

  // 2. Add category-specific enhancements
  const categoryData = config.categories[category];
  if (
    categoryData &&
    categoryData.enhancers &&
    categoryData.enhancers.length > 0
  ) {
    const enhancerText = categoryData.enhancers.join(' ');
    // Check if not already present
    if (!enhanced.includes(enhancerText.substring(0, 20))) {
      enhanced = `${enhanced}\n\n${enhancerText}`;
      enhancements.push(`Added ${category}-specific instructions`);
    }
  }

  // 3. Add language context for code
  if (category === 'code' && language) {
    if (!enhanced.toLowerCase().includes(language)) {
      enhanced = `[${language}] ${enhanced}`;
      enhancements.push(`Added language tag: ${language}`);
    }
  }

  // 4. Add structure for low-clarity prompts
  const lowClarityThreshold = config.settings?.lowClarityThreshold || 60;
  if (
    clarity.score < lowClarityThreshold &&
    config.settings?.wrapLowClarity !== false
  ) {
    enhanced = `Task: ${enhanced}\n\nPlease provide a clear, well-structured response.`;
    enhancements.push('Added structure wrapper');
  }

  // 5. Add an example template if requested
  if (addExamples) {
    const templates = config.promptTemplates?.[category];
    const exampleTemplate =
      templates?.basic || (templates ? Object.values(templates)[0] : null);
    if (exampleTemplate && !enhanced.includes(exampleTemplate)) {
      enhanced = `${enhanced}\n\nExample: ${exampleTemplate}`;
      enhancements.push('Added example template');
    }
  }

  return {
    originalPrompt: prompt,
    optimizedPrompt: enhanced.trim(),
    category,
    language,
    clarityScore: clarity.score,
    clarityQuality: clarity.quality,
    clarityIssues: clarity.issues,
    claritySuggestions: clarity.suggestions,
    enhancements,
    wasEnhanced: enhancements.length > 0,
    modelOptimization: modelOpt
  };
}

/**
 * Quick function to get an improved prompt
 */
export function getBetterPrompt(prompt, model = 'llama3.2:3b') {
  const result = optimizePrompt(prompt, { model });
  return result.optimizedPrompt;
}

/**
 * Test prompt quality and return detailed report
 */
export function testPromptQuality(prompt) {
  const clarity = getPromptClarity(prompt);
  const category = getPromptCategory(prompt);
  const language = getPromptLanguage(prompt);

  return {
    prompt: prompt.length > 100 ? prompt.substring(0, 100) + '...' : prompt,
    score: clarity.score,
    quality: clarity.quality,
    category,
    language,
    issues: clarity.issues,
    suggestions: clarity.suggestions,
    recommendation:
      clarity.score >= 60
        ? 'Prompt is acceptable'
        : 'Consider improving the prompt using suggestions'
  };
}

/**
 * Optimize multiple prompts
 */
export function optimizePromptBatch(prompts, options = {}) {
  return prompts.map((prompt) => optimizePrompt(prompt, options));
}

/**
 * Analyze a prompt without enhancing it
 */
export function analyzePrompt(prompt) {
  return {
    length: prompt.length,
    wordCount: prompt.split(/\s+/).length,
    category: getPromptCategory(prompt),
    language: getPromptLanguage(prompt),
    clarity: getPromptClarity(prompt),
    hasQuestion: prompt.includes('?'),
    hasCodeMarkers: /```/.test(prompt),
    sentiment:
      prompt.toLowerCase().includes('error') ||
      prompt.toLowerCase().includes('bug')
        ? 'problem-solving'
        : 'neutral'
  };
}

/**
 * Get optimization suggestions without applying them
 */
export function getSuggestions(prompt, model = 'llama3.2:3b') {
  const analysis = analyzePrompt(prompt);
  const modelOpt = getModelOptimization(model);
  const suggestions = [...analysis.clarity.suggestions];

  // Category-specific suggestions
  const categoryData = config.categories[analysis.category];
  if (categoryData) {
    if (analysis.category === 'code' && !analysis.language) {
      suggestions.push('Specify the programming language');
    }
    if (
      analysis.category === 'task' &&
      !prompt.toLowerCase().includes('step')
    ) {
      suggestions.push('Ask for step-by-step instructions');
    }
  }

  // Model-specific suggestions
  if (modelOpt.style === 'concise' && prompt.length > 200) {
    suggestions.push('Prompt may be too long for this model');
  }

  // Smart suggestions based on patterns
  const smartSuggestions = getSmartSuggestions(prompt, analysis);
  suggestions.push(...smartSuggestions);

  return {
    prompt: prompt.substring(0, 50) + (prompt.length > 50 ? '...' : ''),
    category: analysis.category,
    clarityScore: analysis.clarity.score,
    suggestions: [...new Set(suggestions)], // Remove duplicates
    wouldEnhance: analysis.clarity.score < 60 || suggestions.length > 0
  };
}

/**
 * Get smart suggestions based on advanced pattern matching
 */
export function getSmartSuggestions(prompt, analysis = null) {
  if (!analysis) analysis = analyzePrompt(prompt);
  const suggestions = [];

  // Check for missing error handling mention in code prompts
  if (analysis.category === 'code') {
    const mentionsErrors = /error|exception|try|catch|handle|throw/i.test(
      prompt
    );
    if (!mentionsErrors) {
      suggestions.push('Consider specifying error handling requirements');
    }

    // Check for missing input validation
    if (!/valid|check|verify|sanitize|input/i.test(prompt)) {
      suggestions.push('Consider specifying input validation requirements');
    }
  }

  // API-specific suggestions
  if (analysis.category === 'api') {
    if (!/auth|token|key|bearer/i.test(prompt)) {
      suggestions.push('Specify authentication method if needed');
    }
    if (!/status|response|error code/i.test(prompt)) {
      suggestions.push('Specify expected HTTP status codes');
    }
  }

  // Database-specific suggestions
  if (analysis.category === 'database') {
    if (!/index|performance|optimize/i.test(prompt)) {
      suggestions.push('Consider indexing and performance implications');
    }
    if (!/transaction|rollback|commit/i.test(prompt)) {
      suggestions.push('Specify transaction handling if applicable');
    }
  }

  // Security-specific suggestions
  if (analysis.category === 'security') {
    if (!/owasp|best practice/i.test(prompt)) {
      suggestions.push('Reference OWASP or specific security standards');
    }
  }

  // Testing-specific suggestions
  if (analysis.category === 'testing') {
    if (!/edge case|boundary|negative/i.test(prompt)) {
      suggestions.push('Include edge cases and negative test scenarios');
    }
    if (!/mock|stub|fixture/i.test(prompt)) {
      suggestions.push('Specify mocking requirements for dependencies');
    }
  }

  // Architecture-specific suggestions
  if (analysis.category === 'architecture') {
    if (!/scale|load|traffic/i.test(prompt)) {
      suggestions.push('Specify expected scale and load requirements');
    }
  }

  return suggestions;
}

/**
 * Get auto-completions for partial prompts
 */
export function getAutoCompletions(partialPrompt) {
  const autoCompletions = config.smartSuggestions?.autoCompletions || {};
  const completions = [];
  const words = partialPrompt.toLowerCase().split(/\s+/);
  const lastWord = words[words.length - 1];

  for (const [keyword, templates] of Object.entries(autoCompletions)) {
    if (keyword.startsWith(lastWord) || lastWord.includes(keyword)) {
      completions.push(...templates);
    }
  }

  return {
    partial: partialPrompt,
    completions: completions.slice(0, 5), // Return top 5
    hasCompletions: completions.length > 0
  };
}

/**
 * Detect language from code context clues
 */
export function detectLanguageFromContext(prompt) {
  const contextClues = config.smartSuggestions?.contextClues || {};

  for (const [lang, clues] of Object.entries(contextClues)) {
    for (const clue of clues) {
      if (prompt.includes(clue)) {
        return lang;
      }
    }
  }

  // Fallback to existing language detection
  return getPromptLanguage(prompt);
}

/**
 * Get prompt template for category
 */
export function getPromptTemplate(category, variant = 'basic') {
  const templates = config.promptTemplates || {};
  const categoryTemplates = templates[category] || {};
  return categoryTemplates[variant] || null;
}

/**
 * Apply auto-fix to prompt based on smart suggestions
 */
export function autoFixPrompt(prompt, _options = {}) {
  const analysis = analyzePrompt(prompt);
  let fixed = prompt;
  const appliedFixes = [];

  // Auto-detect language if missing for code
  if (analysis.category === 'code' && !analysis.language) {
    const detectedLang = detectLanguageFromContext(prompt);
    if (detectedLang && !prompt.toLowerCase().includes(detectedLang)) {
      fixed = `[${detectedLang}] ${fixed}`;
      appliedFixes.push(`Added language tag: ${detectedLang}`);
    }
  }

  // Category-specific auto-fixes
  switch (analysis.category) {
    case 'code':
      // Add format specification
      if (!/format|output|return as|show as|```/i.test(prompt)) {
        fixed += '\n\nProvide the code in a code block.';
        appliedFixes.push('Added code format instruction');
      }
      // Add error handling instruction
      if (!/error|exception|try|catch|handle/i.test(prompt)) {
        fixed += ' Include proper error handling.';
        appliedFixes.push('Added error handling instruction');
      }
      break;

    case 'api':
      // Add response format
      if (!/json|response|status|format/i.test(prompt)) {
        fixed += ' Return JSON response format.';
        appliedFixes.push('Added JSON response format');
      }
      // Add error handling
      if (!/error|status code|exception/i.test(prompt)) {
        fixed += ' Include appropriate HTTP status codes and error responses.';
        appliedFixes.push('Added API error handling');
      }
      break;

    case 'database':
      // Add performance consideration
      if (!/index|optim|perform/i.test(prompt)) {
        fixed += ' Consider query performance and indexing.';
        appliedFixes.push('Added performance consideration');
      }
      break;

    case 'testing':
      // Add edge cases
      if (!/edge|boundary|negative|corner/i.test(prompt)) {
        fixed += ' Include edge cases and negative test scenarios.';
        appliedFixes.push('Added edge case instruction');
      }
      break;

    case 'security':
      // Add OWASP reference
      if (!/owasp|best practice|guideline/i.test(prompt)) {
        fixed += ' Follow OWASP security guidelines.';
        appliedFixes.push('Added OWASP reference');
      }
      break;

    case 'devops':
      // Add rollback consideration
      if (!/rollback|revert|recovery/i.test(prompt)) {
        fixed += ' Include rollback strategy.';
        appliedFixes.push('Added rollback consideration');
      }
      break;
  }

  return {
    original: prompt,
    fixed: fixed.trim(),
    appliedFixes,
    wasFixed: appliedFixes.length > 0,
    analysis
  };
}
</file>

<file path="src/speculative.js">
/**
 * HYDRA Speculative Decoding - Parallel model racing
 */

import { generate } from './ollama-client.js';
import { getCache, setCache } from './cache.js';

const FAST_MODEL = process.env.FAST_MODEL || 'llama3.2:1b';
const DEFAULT_MODEL = process.env.DEFAULT_MODEL || 'llama3.2:3b';
const MIN_VALID_LENGTH = 50;

/**
 * Speculative decoding - race fast vs accurate model
 * Returns first valid response, with quality fallback
 */
export async function speculativeGenerate(prompt, options = {}) {
  const fastModel = options.fastModel || FAST_MODEL;
  const accurateModel = options.accurateModel || DEFAULT_MODEL;
  const timeout = options.timeout || 30000;

  // Check cache first
  const cached = getCache(prompt, 'speculative');
  if (cached) {
    return {
      ...cached,
      source: `CACHE (${cached.source})`
    };
  }

  // Create racing promises
  const fastPromise = generate(fastModel, prompt, { timeout })
    .then((r) => ({ ...r, source: 'speed', model: fastModel }))
    .catch(() => null);

  const accuratePromise = generate(accurateModel, prompt, { timeout })
    .then((r) => ({ ...r, source: 'quality', model: accurateModel }))
    .catch(() => null);

  const startTime = Date.now();

  // Strategy: Always check Fast model first. 
  // If it's good, we save time. If not, we fall back to Accurate (which is already running).
  // We don't use Promise.race because a fast-failing Accurate model shouldn't stop us from waiting for Fast.
  
  const fastResult = await fastPromise;

  if (
    fastResult &&
    fastResult.response &&
    fastResult.response.length >= MIN_VALID_LENGTH
  ) {
    const result = {
      response: fastResult.response,
      source: fastResult.source,
      model: fastResult.model,
      duration: Date.now() - startTime,
      winner: 'first'
    };
    setCache(prompt, result.response, 'speculative', result.source);
    return result;
  }

  // Fast failed or was poor quality; check Accurate
  const accurateResult = await accuratePromise;

  if (accurateResult && accurateResult.response) {
    const result = {
      response: accurateResult.response,
      source: accurateResult.source,
      model: accurateResult.model,
      duration: Date.now() - startTime,
      winner: 'quality-fallback'
    };
    setCache(prompt, result.response, 'speculative', result.source);
    return result;
  }

  return {
    response: null,
    error: 'Both models failed to produce valid output',
    source: 'error',
    duration: Date.now() - startTime
  };
}

/**
 * Model race - race N models, first valid wins
 */
export async function modelRace(prompt, models, options = {}) {
  const timeout = options.timeout || 30000;
  const startTime = Date.now();

  const promises = models.map((model) =>
    generate(model, prompt, { timeout })
      .then((r) => ({ ...r, model, success: true }))
      .catch((e) => ({ model, success: false, error: e.message }))
  );

  // Wait for all to complete (or use Promise.race for first)
  if (options.firstWins) {
    const validPromises = promises.map((p) =>
      p.then((r) =>
        r.success && r.response?.length >= MIN_VALID_LENGTH
          ? r
          : Promise.reject()
      )
    );

    try {
      const winner = await Promise.any(validPromises);
      return {
        response: winner.response,
        model: winner.model,
        source: 'race-winner',
        duration: Date.now() - startTime,
        totalModels: models.length
      };
    } catch {
      return { error: 'No model produced valid output', models };
    }
  }

  // Wait for all and return best
  const results = await Promise.all(promises);
  const valid = results.filter(
    (r) => r.success && r.response?.length >= MIN_VALID_LENGTH
  );

  if (valid.length === 0) {
    return { error: 'No model produced valid output', models };
  }

  // Return longest response as "best"
  const best = valid.reduce((a, b) =>
    (a.response?.length || 0) > (b.response?.length || 0) ? a : b
  );

  return {
    response: best.response,
    model: best.model,
    source: 'race-best',
    duration: Date.now() - startTime,
    totalModels: models.length,
    validResponses: valid.length
  };
}

/**
 * Consensus generation - run multiple models and check agreement
 */
export async function consensusGenerate(prompt, models, options = {}) {
  const timeout = options.timeout || 60000;
  const startTime = Date.now();

  const promises = models.map((model) =>
    generate(model, prompt, { timeout })
      .then((r) => ({ response: r.response, model, success: true }))
      .catch(() => ({ model, success: false }))
  );

  const results = await Promise.all(promises);
  const valid = results.filter((r) => r.success && r.response);

  if (valid.length === 0) {
    return { error: 'No model produced output', consensus: false };
  }

  if (valid.length === 1) {
    return {
      response: valid[0].response,
      model: valid[0].model,
      consensus: false,
      reason: 'single-response',
      duration: Date.now() - startTime
    };
  }

  // Simple similarity check (first 100 chars)
  const normalized = valid.map((v) =>
    v.response.toLowerCase().substring(0, 100).trim()
  );
  const similar = normalized.every(
    (n) =>
      normalized[0].includes(n.substring(0, 30)) ||
      n.includes(normalized[0].substring(0, 30))
  );

  return {
    response: valid[0].response, // Return first as primary
    model: valid[0].model,
    consensus: similar,
    agreement: similar ? 'high' : 'low',
    responses: valid.length,
    duration: Date.now() - startTime
  };
}
</file>

<file path="vercel.json">
{
  "regions": ["cdg1", "fra1"],
  "headers": [
    {
      "source": "/api/(.*)",
      "headers": [
        {
          "key": "Cache-Control",
          "value": "s-maxage=300, stale-while-revalidate=60"
        }
      ]
    },
    {
      "source": "/(.*)",
      "headers": [
        {
          "key": "Cache-Control",
          "value": "public, max-age=31536000, immutable"
        }
      ]
    }
  ]
}
</file>

<file path=".gemini/settings.json">
{
  "security": {
    "auth": {
      "selectedType": "gemini-api-key"
    }
  },
  "mcpServers": {
    "ollama-hydra": {
      "command": "node",
      "args": [
        "--env-file=.env",
        "C:/Users/BIURODOM/Desktop/GeminiCLI/src/server.js"
      ],
      "cwd": "C:/Users/BIURODOM/Desktop/GeminiCLI",
      "env": {
        "OLLAMA_HOST": "http://localhost:11434",
        "CACHE_DIR": "C:/Users/BIURODOM/Desktop/GeminiCLI/cache",
        "CACHE_ENABLED": "true",
        "CACHE_TTL": "3600",
        "DEFAULT_MODEL": "llama3.2:3b",
        "FAST_MODEL": "llama3.2:1b",
        "CODER_MODEL": "qwen2.5-coder:1.5b",
        "NODE_ENV": "production"
      },
      "timeout": 120000
    },
    "serena": {
      "command": "C:/Users/BIURODOM/.local/bin/uvx.exe",
      "args": [
        "--from",
        "git+https://github.com/oraios/serena",
        "serena",
        "start-mcp-server",
        "--project",
        "C:/Users/BIURODOM/Desktop/GeminiCLI"
      ],
      "timeout": 120000
    },
    "desktop-commander": {
      "command": "C:/Users/BIURODOM/AppData/Roaming/npm/desktop-commander.cmd",
      "args": [],
      "env": {
        "DC_SHELL": "powershell"
      },
      "timeout": 60000
    },
    "playwright": {
      "command": "C:/Users/BIURODOM/AppData/Roaming/npm/mcp-server-playwright.cmd",
      "args": [],
      "timeout": 60000
    }
  },
  "mcp": {
    "allowed": ["ollama-hydra", "serena", "desktop-commander", "playwright"],
    "parallelToolCalls": true
  },
  "model": {
    "name": "gemini-3-pro-preview",
    "autoSelect": false,
    "preferences": {
      "code": "gemini-2.5-pro",
      "simple": "gemini-2.5-flash",
      "fast": "gemini-2.0-flash-lite",
      "default": "gemini-3-pro-preview"
    },
    "cachePrompts": true,
    "cacheTtl": 3600
  },
  "general": {
    "previewFeatures": true,
    "confirmTools": false,
    "maxRetries": 3,
    "retryDelay": 1000,
    "fallbacks": {
      "google": ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.0-flash-lite"]
    }
  },
  "context": {
    "fileName": ["GEMINI.md"]
  }
}
</file>

<file path="src/server.js">
#!/usr/bin/env node
/**
 * HYDRA Ollama MCP Server
 *
 * Provides Ollama integration for Gemini CLI with:
 * - Speculative decoding (parallel model racing)
 * - Self-correction (agentic code validation)
 * - SHA256 response caching
 * - Batch processing
 * - Prompt optimization
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ListToolsRequestSchema
} from '@modelcontextprotocol/sdk/types.js';

import {
  generate,
  checkHealth,
  listModels,
  pullModel
} from './ollama-client.js';
import {
  speculativeGenerate,
  modelRace,
  consensusGenerate
} from './speculative.js';
import { selfCorrect, generateWithCorrection } from './self-correction.js';
import { getCache, setCache, getCacheStats } from './cache.js';
import { CONFIG } from './config.js';
import { createLogger } from './logger.js';
import {
  optimizePrompt,
  getBetterPrompt,
  testPromptQuality,
  optimizePromptBatch,
  analyzePrompt,
  getSuggestions,
  getSmartSuggestions,
  getAutoCompletions,
  getPromptTemplate,
  autoFixPrompt
} from './prompt-optimizer.js';
import {
  getGeminiModels,
  getModelDetails,
  filterModelsByCapability,
  getRecommendedModels,
  getModelsSummary,
  initializeModels
} from './gemini-models.js';
import {
  Priority,
  getQueue,
  enqueue,
  enqueueBatch,
  getQueueStatus,
  cancelItem,
  pauseQueue,
  resumeQueue
} from './prompt-queue.js';
import { TOOLS } from './tools.js';
import { resolveNodeEngines, resolveServerVersion } from './version.js';
import { runSwarm, isComplexPrompt } from './swarm.js';

const logger = createLogger('server');
const SERVER_VERSION = resolveServerVersion();

// Server instance
const server = new Server(
  {
    name: 'ollama-hydra',
    version: SERVER_VERSION
  },
  {
    capabilities: {
      tools: {}
    }
  }
);

const toolByName = new Map(TOOLS.map((tool) => [tool.name, tool]));
const modelCache = { models: null, updatedAt: 0 };

const createErrorResponse = (code, message, tool) => {
  return {
    content: [
      {
        type: 'text',
        text: JSON.stringify({ error: message, code, tool })
      }
    ],
    isError: true
  };
};

const validateToolArgs = (tool, args) => {
  const errors = [];
  if (!tool?.inputSchema) return errors;
  const { required = [], properties = {} } = tool.inputSchema;
  for (const key of required) {
    if (args[key] === undefined) {
      errors.push(`Brakuje wymaganego pola: ${key}`);
    }
  }
  for (const [key, value] of Object.entries(args)) {
    const schema = properties[key];
    if (!schema || value === null) continue;
    const expected = schema.type;
    if (expected === 'array' && !Array.isArray(value)) {
      errors.push(`Pole ${key} powinno być tablicą`);
    } else if (expected === 'number' && typeof value !== 'number') {
      errors.push(`Pole ${key} powinno być liczbą`);
    } else if (expected === 'string' && typeof value !== 'string') {
      errors.push(`Pole ${key} powinno być tekstem`);
    } else if (expected === 'boolean' && typeof value !== 'boolean') {
      errors.push(`Pole ${key} powinno być wartością true/false`);
    } else if (
      expected === 'object' &&
      (typeof value !== 'object' || Array.isArray(value))
    ) {
      errors.push(`Pole ${key} powinno być obiektem`);
    }
  }
  return errors;
};

const getCachedModels = async () => {
  const now = Date.now();
  if (
    modelCache.models &&
    now - modelCache.updatedAt < CONFIG.MODEL_CACHE_TTL_MS
  ) {
    return modelCache.models;
  }

  const health = await checkHealth();
  if (!health.available) {
    modelCache.models = [];
    modelCache.updatedAt = now;
    return modelCache.models;
  }

  modelCache.models = await listModels();
  modelCache.updatedAt = now;
  return modelCache.models;
};

const resolveModelOrFallback = async (requestedModel) => {
  if (!requestedModel) {
    return { model: CONFIG.DEFAULT_MODEL, fallbackUsed: false };
  }
  const models = await getCachedModels();
  const available = models
    .map((model) => model.name ?? model.model)
    .filter(Boolean);
  if (available.includes(requestedModel)) {
    return { model: requestedModel, fallbackUsed: false };
  }
  logger.warn('Requested model not available, falling back', {
    requestedModel,
    fallbackModel: CONFIG.DEFAULT_MODEL
  });
  return { model: CONFIG.DEFAULT_MODEL, fallbackUsed: true };
};

const getMajorVersion = (version) => {
  const match = `${version}`.match(/(\d+)/);
  return match ? Number.parseInt(match[1], 10) : null;
};

const detectPromptRisk = (prompt) => {
  if (!prompt) return [];
  const checks = [
    {
      pattern: /ignore (all|previous|earlier) instructions/i,
      message: 'Wykryto możliwą próbę obejścia instrukcji.'
    },
    {
      pattern: /system prompt/i,
      message: 'Wykryto prośbę o ujawnienie promptu systemowego.'
    },
    {
      pattern: /exfiltrate|leak|steal/i,
      message: 'Wykryto możliwą próbę eksfiltracji danych.'
    }
  ];
  return checks
    .filter(({ pattern }) => pattern.test(prompt))
    .map(({ message }) => message);
};

const evaluatePromptRisk = (prompt) => {
  const warnings = detectPromptRisk(prompt);
  return {
    warnings,
    blocked: warnings.length > 0 && CONFIG.RISK_BLOCKING
  };
};

// List tools handler
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return { tools: TOOLS };
});

// Call tool handler
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params ?? {};
  const startedAt = Date.now();
  const safeArgs = args ?? {};
  const tool = toolByName.get(name);

  try {
    if (!tool) {
      return createErrorResponse(
        'HYDRA_TOOL_UNKNOWN',
        'Nieznane narzędzie.',
        name
      );
    }

    const validationErrors = validateToolArgs(tool, safeArgs);
    if (validationErrors.length > 0) {
      return createErrorResponse(
        'HYDRA_TOOL_INVALID',
        validationErrors.join(' '),
        name
      );
    }

    let result;

    switch (name) {
      // === GENERATION TOOLS ===
      case 'ollama_generate': {
        let prompt = safeArgs.prompt;
        const risk = evaluatePromptRisk(prompt);
        if (risk.blocked) {
          return createErrorResponse(
            'HYDRA_RISK_BLOCKED',
            risk.warnings.join(' '),
            name
          );
        }
        if (risk.warnings.length) {
          logger.warn('Potential prompt risk detected', { tool: name });
        }

        // Optimize prompt if requested
        if (safeArgs.optimize) {
          const optimized = optimizePrompt(prompt, { model: safeArgs.model });
          prompt = optimized.optimizedPrompt;
        }

        // Check cache first
        if (safeArgs.useCache !== false) {
          const cached = getCache(prompt, safeArgs.model);
          if (cached) {
            result = { ...cached, fromCache: true };
            break;
          }
        }

        const resolved = await resolveModelOrFallback(safeArgs.model);
        
        // Use queue for concurrency control
        const queueId = enqueue(prompt, {
          model: resolved.model,
          priority: Priority.NORMAL,
          metadata: {
            temperature: safeArgs.temperature,
            maxTokens: safeArgs.maxTokens
          }
        });

        try {
          const queueItem = await getQueue().waitFor(
            queueId,
            CONFIG.QUEUE_TIMEOUT_MS || 60000
          );

          if (queueItem.error) {
            throw new Error(queueItem.error);
          }

          const response = {
            response: queueItem.result,
            model: resolved.model
          };

          // Save to cache
          if (safeArgs.useCache !== false) {
            setCache(prompt, response.response, resolved.model);
          }

          result = {
            ...response,
            fallbackUsed: resolved.fallbackUsed,
            model: resolved.model,
            securityWarnings: risk.warnings
          };
        } catch (err) {
          // Ensure we try to cancel if we timed out or crashed
          cancelItem(queueId);
          throw err;
        }
        break;
      }

      case 'ollama_smart': {
        // Smart generation: optimize → detect category → select model → generate
        const risk = evaluatePromptRisk(safeArgs.prompt);
        if (risk.blocked) {
          return createErrorResponse(
            'HYDRA_RISK_BLOCKED',
            risk.warnings.join(' '),
            name
          );
        }
        if (risk.warnings.length) {
          logger.warn('Potential prompt risk detected', { tool: name });
        }
        const useSwarm = safeArgs.useSwarm ?? isComplexPrompt(safeArgs.prompt);
        if (useSwarm) {
          result = await runSwarm({
            prompt: safeArgs.prompt,
            title: safeArgs.title,
            agents: safeArgs.agents,
            includeTranscript: safeArgs.includeTranscript || false,
            saveMemory: safeArgs.saveMemory !== false,
            logger
          });
          result.securityWarnings = risk.warnings;
          break;
        }
        const optimization = optimizePrompt(safeArgs.prompt);
        const category = optimization.category;

        // Select model based on category
        let model = safeArgs.model;
        if (!model) {
          if (category === 'code') model = CONFIG.CODER_MODEL;
          else if (category === 'question')
            model = CONFIG.FAST_MODEL; // Fast for simple questions
          else model = CONFIG.DEFAULT_MODEL;
        }

        // Use speculative for non-code tasks
        if (category !== 'code') {
          result = await speculativeGenerate(optimization.optimizedPrompt, {
            accurateModel: model
          });
          result.optimization = optimization;
        } else {
          // Use code generation with self-correction
          result = await generateWithCorrection(optimization.optimizedPrompt, {
            generatorModel: model
          });
          result.optimization = optimization;
        }
        result.securityWarnings = risk.warnings;
        break;
      }

      case 'ollama_speculative':
        {
          const risk = evaluatePromptRisk(safeArgs.prompt);
          if (risk.blocked) {
            return createErrorResponse(
              'HYDRA_RISK_BLOCKED',
              risk.warnings.join(' '),
              name
            );
          }
          result = await speculativeGenerate(safeArgs.prompt, safeArgs);
          result.securityWarnings = risk.warnings;
        }
        break;

      case 'ollama_race':
        {
          const risk = evaluatePromptRisk(safeArgs.prompt);
          if (risk.blocked) {
            return createErrorResponse(
              'HYDRA_RISK_BLOCKED',
              risk.warnings.join(' '),
              name
            );
          }
          result = await modelRace(
            safeArgs.prompt,
            safeArgs.models || [
              CONFIG.FAST_MODEL,
              'phi3:mini',
              CONFIG.DEFAULT_MODEL
            ],
            { firstWins: safeArgs.firstWins ?? true }
          );
          result.securityWarnings = risk.warnings;
        }
        break;

      case 'ollama_consensus':
        {
          const risk = evaluatePromptRisk(safeArgs.prompt);
          if (risk.blocked) {
            return createErrorResponse(
              'HYDRA_RISK_BLOCKED',
              risk.warnings.join(' '),
              name
            );
          }
          result = await consensusGenerate(
            safeArgs.prompt,
            safeArgs.models || [CONFIG.DEFAULT_MODEL, 'phi3:mini']
          );
          result.securityWarnings = risk.warnings;
        }
        break;

      // === CODE TOOLS ===
      case 'ollama_code':
        {
          const risk = evaluatePromptRisk(safeArgs.prompt);
          if (risk.blocked) {
            return createErrorResponse(
              'HYDRA_RISK_BLOCKED',
              risk.warnings.join(' '),
              name
            );
          }
          result = await generateWithCorrection(safeArgs.prompt, {
            generatorModel: safeArgs.model || CONFIG.DEFAULT_MODEL,
            coderModel: safeArgs.coderModel || CONFIG.CODER_MODEL
          });
          result.securityWarnings = risk.warnings;
        }
        break;

      case 'ollama_validate':
        result = await selfCorrect(safeArgs.code, {
          language: safeArgs.language,
          maxAttempts: safeArgs.maxAttempts
        });
        break;

      // === PROMPT OPTIMIZATION TOOLS ===
      case 'prompt_optimize':
        result = optimizePrompt(safeArgs.prompt, {
          model: safeArgs.model,
          category: safeArgs.category,
          addExamples: safeArgs.addExamples
        });
        break;

      case 'prompt_analyze':
        result = analyzePrompt(safeArgs.prompt);
        break;

      case 'prompt_quality':
        result = testPromptQuality(safeArgs.prompt);
        break;

      case 'prompt_suggest':
        result = getSuggestions(safeArgs.prompt, safeArgs.model);
        break;

      case 'prompt_batch_optimize':
        result = optimizePromptBatch(safeArgs.prompts, {
          model: safeArgs.model
        });
        break;

      case 'prompt_smart_suggest':
        result = {
          prompt:
            safeArgs.prompt.substring(0, 50) +
            (safeArgs.prompt.length > 50 ? '...' : ''),
          analysis: analyzePrompt(safeArgs.prompt),
          smartSuggestions: getSmartSuggestions(safeArgs.prompt),
          standardSuggestions: getSuggestions(safeArgs.prompt)
        };
        break;

      case 'prompt_autocomplete':
        result = getAutoCompletions(safeArgs.partial);
        break;

      case 'prompt_autofix':
        result = autoFixPrompt(safeArgs.prompt);
        break;

      case 'prompt_template': {
        const template = getPromptTemplate(
          safeArgs.category,
          safeArgs.variant || 'basic'
        );
        result = {
          category: safeArgs.category,
          variant: safeArgs.variant || 'basic',
          template: template,
          available: template !== null
        };
        break;
      }

      // === BATCH & UTILITY TOOLS ===
      case 'ollama_batch': {
        const resolved = await resolveModelOrFallback(safeArgs.model);
        const model = resolved.model;
        let prompts = safeArgs.prompts;
        const batchWarnings = safeArgs.prompts.flatMap((prompt) =>
          detectPromptRisk(prompt)
        );
        const uniqueWarnings = [...new Set(batchWarnings)];
        if (uniqueWarnings.length && CONFIG.RISK_BLOCKING) {
          return createErrorResponse(
            'HYDRA_RISK_BLOCKED',
            uniqueWarnings.join(' '),
            name
          );
        }
        if (uniqueWarnings.length) {
          logger.warn('Potential prompt risk detected', { tool: name });
        }

        // Optimize prompts if requested
        if (safeArgs.optimize) {
          prompts = prompts.map((p) => getBetterPrompt(p, model));
        }

        // Process in batch via Queue
        const ids = enqueueBatch(prompts, {
          model: model,
          priority: Priority.LOW
        });

        // Wait for all results
        const queueResults = await Promise.all(
          ids.map((id) =>
            getQueue()
              .waitFor(id, CONFIG.QUEUE_TIMEOUT_MS * 2 || 120000)
              .catch((e) => ({ error: e.message }))
          )
        );

        result = {
          results: queueResults.map((item, i) => ({
            prompt: prompts[i].substring(0, 50) + '...',
            response: item.result || null,
            error: item.error || null
          })),
          total: prompts.length,
          successful: queueResults.filter((r) => r.result).length,
          optimized: safeArgs.optimize || false,
          fallbackUsed: resolved.fallbackUsed,
          securityWarnings: uniqueWarnings
        };
        break;
      }

      case 'ollama_status': {
        const health = await checkHealth();
        const models = health.available ? await listModels() : [];
        const cacheStats = getCacheStats();

        result = {
          ollama: health,
          models: models,
          cache: cacheStats,
          config: {
            defaultModel: CONFIG.DEFAULT_MODEL,
            fastModel: CONFIG.FAST_MODEL,
            coderModel: CONFIG.CODER_MODEL
          },
          features: {
            promptOptimizer: true,
            speculativeDecoding: true,
            selfCorrection: true,
            caching: true,
            batchProcessing: true,
            swarm: true,
            riskBlocking: CONFIG.RISK_BLOCKING
          },
          apiVersion: CONFIG.API_VERSION,
          serverVersion: SERVER_VERSION
        };
        break;
      }

      case 'ollama_pull': {
        const success = await pullModel(safeArgs.model);
        result = { model: safeArgs.model, pulled: success };
        break;
      }

      case 'ollama_cache_clear': {
        const { readdirSync, unlinkSync, statSync } = await import('fs');
        const { join } = await import('path');
        const cacheDir = CONFIG.CACHE_DIR || './cache';
        const olderThan = safeArgs.olderThan ? safeArgs.olderThan * 1000 : 0;
        const now = Date.now();
        let cleared = 0;

        try {
          const files = readdirSync(cacheDir).filter((f) =>
            f.endsWith('.json')
          );
          for (const file of files) {
            const path = join(cacheDir, file);
            const stat = statSync(path);
            if (olderThan === 0 || now - stat.mtimeMs > olderThan) {
              unlinkSync(path);
              cleared++;
            }
          }
        } catch {}

        result = { cleared, cacheDir };
        break;
      }

      // === GEMINI MODELS TOOLS ===
      case 'gemini_models': {
        result = await getGeminiModels(
          safeArgs.forceRefresh || false,
          safeArgs.apiKey
        );
        break;
      }

      case 'gemini_model_details': {
        result = await getModelDetails(safeArgs.model, safeArgs.apiKey);
        break;
      }

      case 'gemini_models_summary': {
        const modelsResult = await getGeminiModels(
          safeArgs.forceRefresh || false
        );
        if (modelsResult.success) {
          result = {
            source: modelsResult.source,
            summary: getModelsSummary(modelsResult.models)
          };
        } else {
          result = modelsResult;
        }
        break;
      }

      case 'gemini_models_recommend': {
        const modelsResult = await getGeminiModels(
          safeArgs.forceRefresh || false
        );
        if (modelsResult.success) {
          result = {
            source: modelsResult.source,
            recommendations: getRecommendedModels(modelsResult.models)
          };
        } else {
          result = modelsResult;
        }
        break;
      }

      case 'gemini_models_filter': {
        const modelsResult = await getGeminiModels(
          safeArgs.forceRefresh || false
        );
        if (modelsResult.success) {
          const filtered = filterModelsByCapability(
            modelsResult.models,
            safeArgs.capability
          );
          result = {
            capability: safeArgs.capability,
            count: filtered.length,
            models: filtered.map((m) => ({
              name: m.name,
              displayName: m.displayName,
              inputTokenLimit: m.inputTokenLimit,
              outputTokenLimit: m.outputTokenLimit
            }))
          };
        } else {
          result = modelsResult;
        }
        break;
      }

      // === QUEUE MANAGEMENT TOOLS ===
      case 'queue_enqueue': {
        const priorityMap = {
          urgent: Priority.URGENT,
          high: Priority.HIGH,
          normal: Priority.NORMAL,
          low: Priority.LOW,
          background: Priority.BACKGROUND
        };
        const id = enqueue(safeArgs.prompt, {
          model: safeArgs.model || CONFIG.DEFAULT_MODEL,
          priority: priorityMap[safeArgs.priority] ?? Priority.NORMAL,
          metadata: safeArgs.metadata || {}
        });
        result = {
          id,
          status: 'queued',
          priority: safeArgs.priority || 'normal',
          model: safeArgs.model || CONFIG.DEFAULT_MODEL
        };
        break;
      }

      case 'queue_batch': {
        const priorityMap = {
          urgent: Priority.URGENT,
          high: Priority.HIGH,
          normal: Priority.NORMAL,
          low: Priority.LOW,
          background: Priority.BACKGROUND
        };
        const ids = enqueueBatch(safeArgs.prompts, {
          model: safeArgs.model || CONFIG.DEFAULT_MODEL,
          priority: priorityMap[safeArgs.priority] ?? Priority.NORMAL
        });
        result = {
          ids,
          count: ids.length,
          priority: safeArgs.priority || 'normal',
          model: safeArgs.model || CONFIG.DEFAULT_MODEL
        };
        break;
      }

      case 'queue_status': {
        result = getQueueStatus();
        break;
      }

      case 'queue_item': {
        const item = getQueue().getItem(safeArgs.id);
        if (item) {
          result = {
            id: item.id,
            status: item.status,
            priority: item.priority,
            attempts: item.attempts,
            prompt:
              item.prompt.substring(0, 100) +
              (item.prompt.length > 100 ? '...' : ''),
            result: item.result,
            error: item.error,
            createdAt: item.createdAt
              ? new Date(item.createdAt).toISOString()
              : null,
            startedAt: item.startedAt
              ? new Date(item.startedAt).toISOString()
              : null,
            completedAt: item.completedAt
              ? new Date(item.completedAt).toISOString()
              : null
          };
        } else {
          result = { error: `Nie znaleziono elementu ${safeArgs.id}` };
        }
        break;
      }

      case 'queue_cancel': {
        const cancelled = cancelItem(safeArgs.id);
        result = { id: safeArgs.id, cancelled };
        break;
      }

      case 'queue_cancel_all': {
        const cancelled = getQueue().cancelAll();
        result = { cancelled: cancelled.length, ids: cancelled };
        break;
      }

      case 'queue_pause': {
        pauseQueue();
        result = { paused: true };
        break;
      }

      case 'queue_resume': {
        resumeQueue();
        result = { resumed: true };
        break;
      }

      case 'queue_wait': {
        try {
          const item = await getQueue().waitFor(
            safeArgs.id,
            safeArgs.timeout || CONFIG.QUEUE_TIMEOUT_MS
          );
          result = {
            id: item.id,
            status: item.status,
            result: item.result,
            error: item.error,
            duration: item.completedAt
              ? item.completedAt - item.startedAt
              : null
          };
        } catch (e) {
          result = { error: `Nie udało się pobrać wyniku: ${e.message}` };
        }
        break;
      }

      case 'hydra_swarm': {
        const risk = evaluatePromptRisk(safeArgs.prompt);
        if (risk.blocked) {
          return createErrorResponse(
            'HYDRA_RISK_BLOCKED',
            risk.warnings.join(' '),
            name
          );
        }
        result = await runSwarm({
          prompt: safeArgs.prompt,
          title: safeArgs.title,
          agents: safeArgs.agents,
          includeTranscript: safeArgs.includeTranscript || false,
          saveMemory: safeArgs.saveMemory !== false,
          logger
        });
        result.securityWarnings = risk.warnings;
        break;
      }

      case 'hydra_health': {
        const health = await checkHealth();
        const cacheStats = getCacheStats();
        const queueStatus = getQueueStatus();
        const nodeEngines = resolveNodeEngines();
        result = {
          status: health.available ? 'ok' : 'degraded',
          ollama: health,
          queue: queueStatus,
          cache: cacheStats,
          version: SERVER_VERSION,
          apiVersion: CONFIG.API_VERSION,
          runtime: {
            yoloMode: CONFIG.YOLO_MODE,
            riskBlocking: CONFIG.RISK_BLOCKING
          },
          node: {
            runtime: process.versions.node,
            engines: nodeEngines
          }
        };
        break;
      }

      case 'hydra_config': {
        result = {
          apiVersion: CONFIG.API_VERSION,
          defaults: {
            defaultModel: CONFIG.DEFAULT_MODEL,
            fastModel: CONFIG.FAST_MODEL,
            coderModel: CONFIG.CODER_MODEL
          },
          runtime: {
            yoloMode: CONFIG.YOLO_MODE,
            riskBlocking: CONFIG.RISK_BLOCKING
          },
          cache: {
            enabled: CONFIG.CACHE_ENABLED,
            ttlMs: CONFIG.CACHE_TTL_MS,
            dir: CONFIG.CACHE_DIR,
            encrypted: Boolean(CONFIG.CACHE_ENCRYPTION_KEY)
          },
          queue: {
            maxConcurrent: CONFIG.QUEUE_MAX_CONCURRENT,
            maxRetries: CONFIG.QUEUE_MAX_RETRIES,
            retryDelayBase: CONFIG.QUEUE_RETRY_DELAY_BASE,
            timeoutMs: CONFIG.QUEUE_TIMEOUT_MS,
            rateLimit: {
              tokens: CONFIG.QUEUE_RATE_LIMIT_TOKENS,
              refillRate: CONFIG.QUEUE_RATE_LIMIT_REFILL
            }
          }
        };
        break;
      }

      default:
        return createErrorResponse(
          'HYDRA_TOOL_UNKNOWN',
          'Nieznane narzędzie.',
          name
        );
    }

    logger.info('Tool executed', {
      tool: name,
      durationMs: Date.now() - startedAt
    });

    return {
      content: [
        {
          type: 'text',
          text:
            typeof result === 'string'
              ? result
              : JSON.stringify(result, null, 2)
        }
      ]
    };
  } catch (error) {
    logger.error('Tool execution failed', { tool: name, error: error.message });
    return {
      content: [
        {
          type: 'text',
          text: JSON.stringify({
            error: `Wystąpił błąd podczas przetwarzania: ${error.message}`,
            tool: name
          })
        }
      ],
      isError: true
    };
  }
});

// Start server
async function main() {
  const transport = new StdioServerTransport();

  const cacheStats = getCacheStats();
  logger.info('Cache warmup completed', {
    totalEntries: cacheStats.totalEntries,
    validEntries: cacheStats.validEntries
  });

  // Initialize Gemini models at startup (from cache or API)
  const engines = resolveNodeEngines();
  const runtimeMajor = getMajorVersion(process.versions.node);
  const engineMajor = engines ? getMajorVersion(engines) : null;
  if (engineMajor && runtimeMajor && runtimeMajor < engineMajor) {
    logger.warn('Node runtime may not satisfy engines requirement', {
      runtime: process.versions.node,
      engines
    });
  }

  const modelsInit = await initializeModels();
  if (modelsInit.success) {
    logger.info('Gemini models ready', { count: modelsInit.count });
  }

  // Initialize prompt queue with Ollama handler
  const queue = getQueue({
    maxConcurrent: CONFIG.QUEUE_MAX_CONCURRENT,
    maxRetries: CONFIG.QUEUE_MAX_RETRIES,
    retryDelayBase: CONFIG.QUEUE_RETRY_DELAY_BASE,
    timeout: CONFIG.QUEUE_TIMEOUT_MS,
    rateLimit: {
      maxTokens: CONFIG.QUEUE_RATE_LIMIT_TOKENS,
      refillRate: CONFIG.QUEUE_RATE_LIMIT_REFILL
    }
  });

  // Set default handler for prompt processing
  queue.setHandler(async (prompt, model, metadata) => {
    const response = await generate(model || CONFIG.DEFAULT_MODEL, prompt, {
      temperature: metadata?.temperature || 0.3,
      maxTokens: metadata?.maxTokens || 2048
    });
    return response.response;
  });

  // Log queue events
  queue.on('completed', ({ id, duration }) => {
    logger.info('Queue item completed', { id, duration });
  });
  queue.on('failed', ({ id, error }) => {
    logger.error('Queue item failed', { id, error });
  });
  queue.on('retrying', ({ id, attempt, delay }) => {
    logger.warn('Queue item retrying', { id, attempt, delay });
  });

  logger.info('Prompt queue initialized', {
    maxConcurrent: CONFIG.QUEUE_MAX_CONCURRENT,
    retries: CONFIG.QUEUE_MAX_RETRIES
  });

  await server.connect(transport);
  logger.info('HYDRA Ollama MCP Server running on stdio', {
    version: SERVER_VERSION
  });
}

main().catch((error) => {
  logger.error('Server failed to start', { error: error.message });
});
</file>

<file path="_launcher.ps1">
# ======================================================================
# GEMINI CLI - HYDRA LAUNCHER
# Enhanced terminal experience with Ollama integration
# ======================================================================

param(
    [switch]$Yolo
)

# Set UTF-8 encoding for Windows console (Output AND Input)
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8
[Console]::InputEncoding = [System.Text.Encoding]::UTF8
$OutputEncoding = [System.Text.Encoding]::UTF8
chcp 65001 | Out-Null

$script:YoloEnabled = $false

# === FUNCTION: Get API Key with fallback chain ===
function Get-APIKey {
    param([string]$KeyName)

    # 1. Check .env file first
    $envFile = Join-Path $PSScriptRoot '.env'
    if (Test-Path $envFile) {
        $match = Get-Content $envFile | Where-Object { $_ -match "^$KeyName=" }
        if ($match) {
            $value = ($match -split '=', 2)[1].Trim().Trim([char]34, [char]39)
            if ($value) { return @{ Value = $value; Source = '.env' } }
        }
    }

    # 2. Check Process scope (current session)
    $processVal = [Environment]::GetEnvironmentVariable($KeyName, 'Process')
    if ($processVal) { return @{ Value = $processVal; Source = 'Process' } }

    # 3. Check User scope
    $userVal = [Environment]::GetEnvironmentVariable($KeyName, 'User')
    if ($userVal) { return @{ Value = $userVal; Source = 'User' } }

    # 4. Check Machine scope
    $machineVal = [Environment]::GetEnvironmentVariable($KeyName, 'Machine')
    if ($machineVal) { return @{ Value = $machineVal; Source = 'Machine' } }

    return $null
}

# === LOAD ENVIRONMENT ===
function Initialize-Environment
     {
    $envFile = Join-Path $PSScriptRoot '.env'
    if (Test-Path $envFile) {
        Get-Content $envFile | ForEach-Object {
            if ($_ -match '^([^#=]+)=(.*)$') {
                $name = $matches[1].Trim()
                $value = $matches[2].Trim().Trim([char]34, [char]39)
                [Environment]::SetEnvironmentVariable($name, $value, 'Process')
            }
        }
    }
}

function Get-OllamaHost {
    $ollamaHost = $env:OLLAMA_HOST
    if (-not $ollamaHost) { $ollamaHost = 'http://localhost:11434' }
    return $ollamaHost.TrimEnd('/')
}

function Test-LocalOllamaHost {
    param([string]$HostUrl)
    try {
        $uri = [Uri]$HostUrl
    } catch {
        return $false
    }
    $uriHost = $uri.Host.ToLowerInvariant()
    return $uriHost -in @('localhost', '127.0.0.1', '::1')
}

function Test-OllamaReady {
    param([string]$HostUrl)
    try {
        Invoke-RestMethod -Uri "$HostUrl/api/tags" -TimeoutSec 2 | Out-Null
        return $true
    } catch {
        return $false
    }
}

# Ensure Gemini API Key is available
function Ensure-GeminiApiKey {
    $apiKeyInfo = Get-APIKey -KeyName 'GEMINI_API_KEY'
    if (-not $apiKeyInfo -or -not $apiKeyInfo.Value) {
        Write-Host "`n[API] No GEMINI_API_KEY found. Gemini CLI requires it." -ForegroundColor $colors.Warning
        Write-Host "  Get free key: https://aistudio.google.com/app/apikey" -ForegroundColor $colors.Secondary
        $key = Read-Host "Enter your GEMINI_API_KEY (paste and Enter)"
        if ($key -and $key.Trim()) {
            [Environment]::SetEnvironmentVariable('GEMINI_API_KEY', $key.Trim(), 'Process')
            $envFile = Join-Path $PSScriptRoot '.env'
            $envLine = "GEMINI_API_KEY=$($key.Trim())"
            if (-not (Select-String -Path $envFile -Pattern '^GEMINI_API_KEY=')) {
                Add-Content -Path $envFile -Value $envLine -Encoding UTF8
            }
            Write-Host "  âś“ API Key set for session and .env updated." -ForegroundColor $colors.Success
            # Refresh
            $apiKeyInfo = @{ Value = $key.Trim(); Source = 'Prompt' }
        } else {
            Write-Host "  âš  No key entered. Some features may not work." -ForegroundColor $colors.Warning
        }
        Write-Host ""
    }
}

function Start-OllamaIfNeeded {
    $ollamaHost = Get-OllamaHost
    if (-not (Test-LocalOllamaHost -HostUrl $ollamaHost)) {
        return
    }

    if (Test-OllamaReady -HostUrl $ollamaHost) {
        return
    }

    $ollamaCmd = Get-Command ollama -ErrorAction SilentlyContinue
    if (-not $ollamaCmd) {
        $script:MockOllama = Start-MockOllama
        return
    }

    $ollamaRunning = Get-Process -Name 'ollama' -ErrorAction SilentlyContinue
    if (-not $ollamaRunning) {
        Start-Process -FilePath 'ollama' -ArgumentList 'serve' -WindowStyle Hidden
        Start-Sleep -Milliseconds 800

        # Wait for Ollama to be ready (max 15s)
        $ollamaHost = Get-OllamaHost  # refresh
        $startTime = Get-Date
        $timeoutSec = 15
        while (-not (Test-OllamaReady -HostUrl $ollamaHost) -and ((Get-Date) - $startTime).TotalSeconds -lt $timeoutSec) {
            Start-Sleep -Milliseconds 500
        }
        if (-not (Test-OllamaReady -HostUrl $ollamaHost)) {
            Write-Host "  âš  Ollama not ready after ${timeoutSec}s (but process started). Local models may be slow." -ForegroundColor $colors.Warning
        } else {
            Write-Host "  âś“ Ollama ready!" -ForegroundColor $colors.Success
        }
    }
}

# === MOCK OLLAMA ===
function Start-MockOllama {
    $listener = New-Object System.Net.HttpListener
    $listener.Prefixes.Add("http://localhost:11434/")
    $listener.Start()
    Write-Host "Started mock Ollama server on localhost:11434" -ForegroundColor Green

    $job = Start-Job -ScriptBlock {
        param($listener)
        try {
            while ($listener.IsListening) {
                $context = $listener.GetContext()
                $reqPath = $context.Request.Url.AbsolutePath
                $response = $context.Response
                $response.ContentType = 'application/json'
                if ($reqPath -eq '/api/tags') {
                    $models = @(
                        @{
                            id = "mock:llama3"
                            object = "model"
                            created_at = (Get-Date).ToString("yyyy-MM-ddTHH:mm:ssZ")
                            last_modified = (Get-Date).ToString("yyyy-MM-ddTHH:mm:ssZ")
                            message = $null
                            name = "mock:llama3"
                            size = 8000000000
                            digest = "sha256:mock"
                            details = @{
                                format = "gguf"
                                family = "llama"
                                families = @("llama")
                                parameter_size = "8B"
                                quantization_level = "Q4_0"
                            }
                        }
                    )
                    $body = @{ models = $models } | ConvertTo-Json -Depth 10
                } else {
                    $body = @{ } | ConvertTo-Json
                }
                $buffer = [System.Text.Encoding]::UTF8.GetBytes($body)
                $response.ContentLength64 = $buffer.Length
                $response.OutputStream.Write($buffer, 0, $buffer.Length)
                $response.Close()
            }
        } catch {
            Write-Error $_.Exception.Message
        } finally {
            $listener.Stop()
        }
    } -ArgumentList $listener

    Start-Sleep -Milliseconds 500
    return @{ Listener = $listener; Job = $job }
}

function Stop-MockOllama {
    param($MockData)
    if ($MockData) {
        if ($MockData.Job -and $MockData.Job.State -ne 'Completed') {
            Stop-Job $MockData.Job -PassThru | Remove-Job -Force
        }
        if ($MockData.Listener) {
            $MockData.Listener.Stop()
        }
    }
}

function Enable-YoloMode {
    $script:YoloEnabled = $true
    $env:HYDRA_YOLO = 'true'
    $env:QUEUE_MAX_CONCURRENT = '10'
    $env:QUEUE_MAX_RETRIES = '1'
    $env:QUEUE_TIMEOUT_MS = '15000'
    $env:HYDRA_RISK_BLOCKING = 'false'
}

function Resolve-StatusShell {
    $pwsh = Get-Command pwsh -ErrorAction SilentlyContinue
    if ($pwsh) { return $pwsh.Source }
    $powershell = Get-Command powershell -ErrorAction SilentlyContinue
    if ($powershell) { return $powershell.Source }
    return $null
}

function Start-StatusMonitor {
    param(
        [string]$OllamaHost,
        [string]$ApiKeyMask
    )

    $shell = Resolve-StatusShell
    if (-not $shell) {
        return $null
    }

    if (-not $OllamaHost) { $OllamaHost = 'http://localhost:11434' }
    if (-not $ApiKeyMask) { $ApiKeyMask = 'missing' }

    $env:HYDRA_STATUS_HOST = $OllamaHost
    $env:HYDRA_STATUS_API_MASK = $ApiKeyMask

    $statusScript = @'
$ErrorActionPreference = 'SilentlyContinue'
$Host.UI.RawUI.WindowTitle = 'HYDRA Status'
$ollamaHost = $env:HYDRA_STATUS_HOST
$apiMask = $env:HYDRA_STATUS_API_MASK
while ($true) {
    $models = 0
    try {
        $resp = Invoke-RestMethod -Uri "$ollamaHost/api/tags" -TimeoutSec 2
        if ($resp.models) { $models = $resp.models.Count }
    } catch {
        $models = 0
    }
    $time = (Get-Date).ToString('HH:mm:ss')
    if ($models -gt 0) { $ollamaText = "$models models" } else { $ollamaText = 'down' }
    $line = "[{0}] Ollama: {1} | API: {2}" -f $time, $ollamaText, $apiMask
    if ($line.Length -lt 80) { $line = $line.PadRight(80) }
    Write-Host -NoNewline "`r$line"
    Start-Sleep -Seconds 5
}
'@

    return Start-Process -FilePath $shell -ArgumentList @(
        '-NoProfile',
        '-ExecutionPolicy',
        'Bypass',
        '-Command',
        $statusScript
    ) -PassThru
}

function Stop-StatusMonitor {
    param([System.Diagnostics.Process]$Process)
    if ($Process -and -not $Process.HasExited) {
        Stop-Process -Id $Process.Id -Force -ErrorAction SilentlyContinue
    }
}

function Invoke-GeminiCli {
    param([string[]]$Arguments)

    try {
        npm exec --yes --package @google/gemini-cli --package @google/gemini-cli-core -- gemini @Arguments
        $script:GeminiExitCode = $LASTEXITCODE
    } catch {
        $script:GeminiExitCode = 1
    }
}

function Start-GeminiCli {
    param([string[]]$Arguments)

    $attempt = 0
    $maxRestarts = if ($script:YoloEnabled) { 5 } else { 3 }
    $delaySeconds = if ($script:YoloEnabled) { 1 } else { 3 }



    while ($true) {
        $attempt++
        Write-Host "  Starting Gemini CLI (attempt $attempt/$maxRestarts)..." -ForegroundColor $colors.Secondary
        Invoke-GeminiCli -Arguments $Arguments | Out-Host
        $exitCode = $script:GeminiExitCode
        if ($exitCode -eq 0 -or $exitCode -eq 42 -or $exitCode -eq 130) {
            return $exitCode
        }
        if ($attempt -ge $maxRestarts) {
            return $exitCode
        }
        Write-Host "  Gemini CLI exited with code $exitCode. Auto-resume in $delaySeconds s..." -ForegroundColor $colors.Warning
        Start-Sleep -Seconds $delaySeconds
    }
}

# === COLORS ===
$colors = @{
    Primary   = 'Blue'
    Secondary = 'Cyan'
    Accent    = 'Yellow'
    Success   = 'Green'
    Warning   = 'DarkYellow'
    Error     = 'Red'
    Muted     = 'DarkGray'
    Text      = 'White'
}

# === SPLASH SCREEN ===
function Show-SplashScreen {
    Clear-Host
    $title = "GEMINI CLI (HYDRA) v2.2.0"
    $subtitle = "Ollama + Prompt Optimizer | Speculative Decoding"
    
    if ($script:YoloEnabled) {
        Write-Host $title -ForegroundColor $colors.Error
        Write-Host "YOLO MODE ACTIVE - SAFETY DISABLED" -ForegroundColor $colors.Error
    } else {
        Write-Host $title -ForegroundColor $colors.Secondary
        Write-Host $subtitle -ForegroundColor $colors.Muted
    }
    Write-Host ""
}

# === STATUS BAR ===
function Show-StatusBar {
    # Working Directory
    $dir = (Get-Location).Path
    if ($dir.Length -gt 45) { $dir = "..." + $dir.Substring($dir.Length - 42) }
    
    # Ollama Status
    $ollamaHost = Get-OllamaHost
    $ollamaStatus = try {
        $response = Invoke-RestMethod -Uri "$ollamaHost/api/tags" -TimeoutSec 2
        $response.models.Count
    } catch { 0 }

    if ($ollamaStatus -gt 0) {
        $ollamaText = "$ollamaStatus models"
        $ollamaColor = $colors.Success
    } else {
        $ollamaText = "Offline"
        $ollamaColor = $colors.Error
    }

    # API Key Status
    $apiKey = Get-APIKey -KeyName 'GEMINI_API_KEY'
    if ($apiKey) {
        $apiText = "Active"
        $apiColor = $colors.Success
    } else {
        $apiText = "Missing"
        $apiColor = $colors.Warning
    }

    # Mode
    $modeText = if ($script:YoloEnabled) { 'YOLO' } else { 'STD' }

    # Render Minimal Status Bar
    Write-Host " [" -NoNewline -ForegroundColor $colors.Muted
    Write-Host "DIR" -NoNewline -ForegroundColor $colors.Secondary
    Write-Host "] $dir " -NoNewline -ForegroundColor $colors.Text
    
    Write-Host "[" -NoNewline -ForegroundColor $colors.Muted
    Write-Host "OLLAMA" -NoNewline -ForegroundColor $colors.Secondary
    Write-Host "] " -NoNewline -ForegroundColor $colors.Text
    Write-Host $ollamaText -NoNewline -ForegroundColor $ollamaColor
    
    Write-Host " [" -NoNewline -ForegroundColor $colors.Muted
    Write-Host "API" -NoNewline -ForegroundColor $colors.Secondary
    Write-Host "] " -NoNewline -ForegroundColor $colors.Text
    Write-Host $apiText -NoNewline -ForegroundColor $apiColor

    Write-Host " [" -NoNewline -ForegroundColor $colors.Muted
    Write-Host "MODE" -NoNewline -ForegroundColor $colors.Secondary
    Write-Host "] $modeText" -ForegroundColor $colors.Text
    Write-Host ""
}

# === TIPS ===
function Show-Tips {
    $tips = @(
        "TIP: Use /help to see available commands",
        "TIP: Type /ollama to switch to local Ollama models",
        "TIP: Use /gemini:models to list available Gemini models",
        "TIP: Try /queue:status to check prompt queue",
        "TIP: Press Ctrl+C to cancel current generation"
    )
    $tip = $tips | Get-Random
    Write-Host " $tip" -ForegroundColor $colors.Muted
    Write-Host " TIP: Use Right-Click or Ctrl+Shift+V to paste multiline prompts safely." -ForegroundColor $colors.Muted
    Write-Host ""
}

# === MAIN ===
Initialize-Environment
if ($Yolo) { Enable-YoloMode }
Start-OllamaIfNeeded

Set-Location $PSScriptRoot
$Host.UI.RawUI.WindowTitle = 'Gemini CLI (HYDRA)'

Show-SplashScreen
Show-StatusBar
Show-Tips

$apiKey = Get-APIKey -KeyName 'GEMINI_API_KEY'
$apiMask = if ($apiKey) { $apiKey.Value.Substring(0, [Math]::Min(12, $apiKey.Value.Length)) + "..." } else { 'missing' }
$statusProcess = Start-StatusMonitor -OllamaHost (Get-OllamaHost) -ApiKeyMask $apiMask

Write-Host "  Starting Gemini CLI..." -ForegroundColor $colors.Secondary
Write-Host ""

$exitCode = Start-GeminiCli -Arguments $args

Stop-MockOllama -MockData $script:MockOllama
Stop-StatusMonitor -Process $statusProcess

Write-Host ""
Write-Host "  Terminated." -ForegroundColor $colors.Muted
Write-Host ""
</file>

</files>
